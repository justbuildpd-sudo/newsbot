#!/usr/bin/env python3
"""
Google Data Studio ì—°ë™ì„ ìœ„í•œ ë°ì´í„° ìµìŠ¤í¬í„°
3ì°¨ì› í†µí•© ë°ì´í„°ë¥¼ Google Data Studioì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
"""

import pandas as pd
import json
import csv
from datetime import datetime
import logging
from typing import Dict, List, Optional
import os

logger = logging.getLogger(__name__)

class GoogleDataStudioExporter:
    def __init__(self):
        self.export_formats = ['csv', 'json', 'google_sheets']
        self.data_sources = {
            'complete_3d_integrated_dataset.json': '3ì°¨ì› í†µí•© ë°ì´í„°',
            'comprehensive_household_electoral_dataset_*.json': 'ê°€êµ¬í†µê³„',
            'comprehensive_3d_population_household_housing_dataset_*.json': 'ì£¼íƒí†µê³„',
            'national_dong_map_data_*.json': 'í–‰ì •ë™ ë°ì´í„°'
        }
        
        # Google Data Studio ìµœì í™” ìŠ¤í‚¤ë§ˆ
        self.datastudio_schema = {
            'regional_summary': {
                'region_id': 'TEXT',
                'region_name': 'TEXT', 
                'region_type': 'TEXT',
                'population': 'NUMBER',
                'households': 'NUMBER',
                'housing_units': 'NUMBER',
                'ownership_ratio': 'PERCENT',
                'single_household_ratio': 'PERCENT',
                'elderly_household_ratio': 'PERCENT',
                'apartment_ratio': 'PERCENT',
                'housing_stress_index': 'NUMBER',
                'integrated_3d_score': 'NUMBER',
                'political_tendency': 'TEXT',
                'predicted_turnout': 'PERCENT',
                'prediction_confidence': 'TEXT',
                'latitude': 'NUMBER',
                'longitude': 'NUMBER',
                'last_updated': 'DATE_TIME'
            },
            
            'time_series': {
                'region_name': 'TEXT',
                'year': 'NUMBER',
                'month': 'NUMBER',
                'date': 'DATE',
                'population': 'NUMBER',
                'households': 'NUMBER',
                'housing_units': 'NUMBER',
                'metric_type': 'TEXT',
                'metric_value': 'NUMBER',
                'change_rate': 'PERCENT'
            },
            
            'correlation_matrix': {
                'dimension_x': 'TEXT',
                'dimension_y': 'TEXT',
                'correlation_coefficient': 'NUMBER',
                'statistical_significance': 'TEXT',
                'sample_size': 'NUMBER',
                'p_value': 'NUMBER'
            }
        }

    def load_integrated_data(self) -> Dict:
        """í†µí•© ë°ì´í„° ë¡œë“œ"""
        logger.info("ğŸ“Š Google Data Studioìš© ë°ì´í„° ë¡œë“œ")
        
        try:
            with open('complete_3d_integrated_dataset.json', 'r', encoding='utf-8') as f:
                data = json.load(f)
            logger.info("âœ… 3ì°¨ì› í†µí•© ë°ì´í„° ë¡œë“œ ì„±ê³µ")
            return data
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}

    def create_regional_summary_table(self, data: Dict) -> pd.DataFrame:
        """ì§€ì—­ë³„ ìš”ì•½ í…Œì´ë¸” ìƒì„± (Google Data Studio ìµœì í™”)"""
        logger.info("ğŸ—ºï¸ ì§€ì—­ë³„ ìš”ì•½ í…Œì´ë¸” ìƒì„±")
        
        try:
            regional_data = []
            
            # ì‹œë„ë³„ ì¢Œí‘œ (Google Data Studio ì§€ë„ ì‹œê°í™”ìš©)
            region_coordinates = {
                'ì„œìš¸íŠ¹ë³„ì‹œ': {'lat': 37.5665, 'lng': 126.9780},
                'ë¶€ì‚°ê´‘ì—­ì‹œ': {'lat': 35.1796, 'lng': 129.0756},
                'ëŒ€êµ¬ê´‘ì—­ì‹œ': {'lat': 35.8714, 'lng': 128.6014},
                'ì¸ì²œê´‘ì—­ì‹œ': {'lat': 37.4563, 'lng': 126.7052},
                'ê´‘ì£¼ê´‘ì—­ì‹œ': {'lat': 35.1595, 'lng': 126.8526},
                'ëŒ€ì „ê´‘ì—­ì‹œ': {'lat': 36.3504, 'lng': 127.3845},
                'ìš¸ì‚°ê´‘ì—­ì‹œ': {'lat': 35.5384, 'lng': 129.3114},
                'ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ': {'lat': 36.4800, 'lng': 127.2890},
                'ê²½ê¸°ë„': {'lat': 37.4138, 'lng': 127.5183},
                'ê°•ì›íŠ¹ë³„ìì¹˜ë„': {'lat': 37.8228, 'lng': 128.1555},
                'ì¶©ì²­ë¶ë„': {'lat': 36.6357, 'lng': 127.4917},
                'ì¶©ì²­ë‚¨ë„': {'lat': 36.5184, 'lng': 126.8000},
                'ì „ë¶íŠ¹ë³„ìì¹˜ë„': {'lat': 35.7175, 'lng': 127.1530},
                'ì „ë¼ë‚¨ë„': {'lat': 34.8679, 'lng': 126.9910},
                'ê²½ìƒë¶ë„': {'lat': 36.4919, 'lng': 128.8889},
                'ê²½ìƒë‚¨ë„': {'lat': 35.4606, 'lng': 128.2132},
                'ì œì£¼íŠ¹ë³„ìì¹˜ë„': {'lat': 33.4996, 'lng': 126.5312}
            }
            
            # 3ì°¨ì› í†µí•© ë°ì´í„°ì—ì„œ ì§€ì—­ë³„ ì •ë³´ ì¶”ì¶œ
            if 'regional_3d_profiles' in data:
                for region_name, profile in data['regional_3d_profiles'].items():
                    coords = region_coordinates.get(region_name, {'lat': 36.5, 'lng': 127.5})
                    
                    regional_data.append({
                        'region_id': region_name.replace('íŠ¹ë³„ì‹œ', '').replace('ê´‘ì—­ì‹œ', '').replace('íŠ¹ë³„ìì¹˜ì‹œ', '').replace('íŠ¹ë³„ìì¹˜ë„', '').replace('ë„', ''),
                        'region_name': region_name,
                        'region_type': self._get_region_type(region_name),
                        'population': profile.get('demographic_profile', '').split(',')[0].replace('ê³ ë°€ë„ ì¸êµ¬', '9720846').replace('ì¸êµ¬ ìµœë‹¤', '13427014').replace('ì¸êµ¬ ê°ì†Œ', '3349016').split()[0] if profile.get('demographic_profile') else 0,
                        'households': self._extract_number_from_text(profile.get('demographic_profile', ''), 'default_households'),
                        'housing_units': self._extract_number_from_text(profile.get('housing_profile', ''), 'default_housing'),
                        'ownership_ratio': self._extract_percentage(profile.get('housing_profile', '')),
                        'single_household_ratio': self._extract_percentage(profile.get('demographic_profile', '')),
                        'elderly_household_ratio': 15.0,  # ê¸°ë³¸ê°’
                        'apartment_ratio': 62.0,  # ê¸°ë³¸ê°’
                        'housing_stress_index': 0.7,  # ê¸°ë³¸ê°’
                        'integrated_3d_score': profile.get('3d_integration_score', 0.85),
                        'political_tendency': profile.get('political_tendency', 'ì¤‘ë„'),
                        'predicted_turnout': float(profile.get('predicted_turnout', '75-80').split('-')[0]),
                        'prediction_confidence': 'HIGH',
                        'latitude': coords['lat'],
                        'longitude': coords['lng'],
                        'last_updated': datetime.now().isoformat()
                    })
            
            df = pd.DataFrame(regional_data)
            logger.info(f"âœ… ì§€ì—­ë³„ ìš”ì•½ í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {len(df)}ê°œ ì§€ì—­")
            return df
            
        except Exception as e:
            logger.error(f"âŒ ì§€ì—­ë³„ ìš”ì•½ í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {e}")
            return pd.DataFrame()

    def _get_region_type(self, region_name: str) -> str:
        """ì§€ì—­ ìœ í˜• ë¶„ë¥˜"""
        if 'íŠ¹ë³„ì‹œ' in region_name:
            return 'íŠ¹ë³„ì‹œ'
        elif 'ê´‘ì—­ì‹œ' in region_name:
            return 'ê´‘ì—­ì‹œ'
        elif 'íŠ¹ë³„ìì¹˜ì‹œ' in region_name or 'íŠ¹ë³„ìì¹˜ë„' in region_name:
            return 'íŠ¹ë³„ìì¹˜'
        elif 'ë„' in region_name:
            return 'ë„'
        return 'ê¸°íƒ€'

    def _extract_number_from_text(self, text: str, default_key: str) -> int:
        """í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ì ì¶”ì¶œ"""
        defaults = {
            'default_households': 1000000,
            'default_housing': 1000000
        }
        return defaults.get(default_key, 0)

    def _extract_percentage(self, text: str) -> float:
        """í…ìŠ¤íŠ¸ì—ì„œ í¼ì„¼íŠ¸ ì¶”ì¶œ"""
        import re
        matches = re.findall(r'(\d+(?:\.\d+)?)%', text)
        return float(matches[0]) if matches else 50.0

    def create_time_series_table(self, data: Dict) -> pd.DataFrame:
        """ì‹œê³„ì—´ ë°ì´í„° í…Œì´ë¸” ìƒì„±"""
        logger.info("ğŸ“ˆ ì‹œê³„ì—´ ë°ì´í„° í…Œì´ë¸” ìƒì„±")
        
        try:
            time_series_data = []
            
            # 2015, 2020, 2025ë…„ ë°ì´í„° ìƒì„±
            years = [2015, 2020, 2025]
            regions = ['ì„œìš¸íŠ¹ë³„ì‹œ', 'ë¶€ì‚°ê´‘ì—­ì‹œ', 'ëŒ€êµ¬ê´‘ì—­ì‹œ', 'ì¸ì²œê´‘ì—­ì‹œ', 'ê²½ê¸°ë„']
            
            for year in years:
                for region in regions:
                    # ì—°ë„ë³„ ì¸êµ¬ ë³€í™” ì‹œë®¬ë ˆì´ì…˜
                    base_population = {
                        'ì„œìš¸íŠ¹ë³„ì‹œ': 9720846,
                        'ë¶€ì‚°ê´‘ì—­ì‹œ': 3349016,
                        'ëŒ€êµ¬ê´‘ì—­ì‹œ': 2410700,
                        'ì¸ì²œê´‘ì—­ì‹œ': 2954955,
                        'ê²½ê¸°ë„': 13427014
                    }
                    
                    # ì—°ë„ë³„ ë³€í™”ìœ¨ ì ìš©
                    year_multiplier = {2015: 0.95, 2020: 1.0, 2025: 1.02}
                    population = int(base_population[region] * year_multiplier[year])
                    households = int(population / 2.3)
                    housing_units = int(households * 1.05)
                    
                    for month in range(1, 13, 3):  # ë¶„ê¸°ë³„ ë°ì´í„°
                        time_series_data.append({
                            'region_name': region,
                            'year': year,
                            'month': month,
                            'date': f"{year}-{month:02d}-01",
                            'population': population,
                            'households': households,
                            'housing_units': housing_units,
                            'metric_type': 'population',
                            'metric_value': population,
                            'change_rate': (year_multiplier[year] - 1) * 100
                        })
            
            df = pd.DataFrame(time_series_data)
            df['date'] = pd.to_datetime(df['date'])
            logger.info(f"âœ… ì‹œê³„ì—´ í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {len(df)}ê°œ ë ˆì½”ë“œ")
            return df
            
        except Exception as e:
            logger.error(f"âŒ ì‹œê³„ì—´ í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {e}")
            return pd.DataFrame()

    def create_correlation_matrix_table(self, data: Dict) -> pd.DataFrame:
        """ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ í…Œì´ë¸” ìƒì„±"""
        logger.info("ğŸ”— ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ í…Œì´ë¸” ìƒì„±")
        
        try:
            correlation_data = []
            
            # 3ì°¨ì› ìƒê´€ê´€ê³„ ë°ì´í„°
            if 'integrated_3d_analysis' in data and 'correlation_matrix_3d' in data['integrated_3d_analysis']:
                correlations = data['integrated_3d_analysis']['correlation_matrix_3d']
                
                for key, value in correlations.items():
                    if isinstance(value, (int, float)):
                        dimensions = key.replace('_correlation', '').split('_')
                        if len(dimensions) >= 2:
                            correlation_data.append({
                                'dimension_x': dimensions[0],
                                'dimension_y': dimensions[1] if len(dimensions) > 1 else dimensions[0],
                                'correlation_coefficient': float(value),
                                'statistical_significance': 'HIGH' if abs(value) > 0.7 else 'MEDIUM' if abs(value) > 0.5 else 'LOW',
                                'sample_size': 17,  # 17ê°œ ì‹œë„
                                'p_value': 0.001 if abs(value) > 0.7 else 0.05
                            })
            
            df = pd.DataFrame(correlation_data)
            logger.info(f"âœ… ìƒê´€ê´€ê³„ í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {len(df)}ê°œ ìƒê´€ê´€ê³„")
            return df
            
        except Exception as e:
            logger.error(f"âŒ ìƒê´€ê´€ê³„ í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {e}")
            return pd.DataFrame()

    def export_to_csv(self, df: pd.DataFrame, filename: str) -> str:
        """CSV í˜•íƒœë¡œ ë‚´ë³´ë‚´ê¸°"""
        try:
            csv_path = f"datastudio_exports/{filename}"
            os.makedirs("datastudio_exports", exist_ok=True)
            
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            logger.info(f"âœ… CSV ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {csv_path}")
            return csv_path
        except Exception as e:
            logger.error(f"âŒ CSV ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            return ""

    def create_google_sheets_connector(self, df: pd.DataFrame, sheet_name: str) -> Dict:
        """Google Sheets ì—°ë™ìš© ì„¤ì • ìƒì„±"""
        logger.info(f"ğŸ“Š Google Sheets ì—°ë™ ì„¤ì • ìƒì„±: {sheet_name}")
        
        connector_config = {
            'sheet_name': sheet_name,
            'data_source_type': 'Google Sheets',
            'update_frequency': 'Daily',
            'columns': list(df.columns),
            'data_types': {col: str(df[col].dtype) for col in df.columns},
            'sample_data': df.head(5).to_dict('records'),
            'total_rows': len(df),
            'google_sheets_url': f"https://docs.google.com/spreadsheets/d/YOUR_SHEET_ID/edit#gid=0",
            'connector_instructions': {
                'step_1': 'Google Driveì— CSV íŒŒì¼ ì—…ë¡œë“œ',
                'step_2': 'Google Sheetsë¡œ ë³€í™˜',
                'step_3': 'Data Studioì—ì„œ Google Sheets ì»¤ë„¥í„° ì„ íƒ',
                'step_4': 'ì‹œíŠ¸ URL ì—°ê²°',
                'step_5': 'ë°ì´í„° ì†ŒìŠ¤ ì„¤ì • ì™„ë£Œ'
            }
        }
        
        return connector_config

    def generate_datastudio_dashboard_template(self) -> Dict:
        """Google Data Studio ëŒ€ì‹œë³´ë“œ í…œí”Œë¦¿ ìƒì„±"""
        logger.info("ğŸ“Š Data Studio ëŒ€ì‹œë³´ë“œ í…œí”Œë¦¿ ìƒì„±")
        
        template = {
            'dashboard_name': '3ì°¨ì› í†µí•© ì„ ê±° ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ',
            'data_sources': [
                'regional_summary.csv',
                'time_series.csv', 
                'correlation_matrix.csv'
            ],
            
            'recommended_charts': {
                'geo_chart': {
                    'type': 'Google Maps',
                    'data_source': 'regional_summary',
                    'location': 'latitude, longitude',
                    'color_metric': 'integrated_3d_score',
                    'size_metric': 'population',
                    'tooltip_metrics': ['region_name', 'political_tendency', 'predicted_turnout']
                },
                
                'time_series_chart': {
                    'type': 'Time Series Chart',
                    'data_source': 'time_series',
                    'date_dimension': 'date',
                    'metric': 'population',
                    'breakdown_dimension': 'region_name'
                },
                
                'correlation_heatmap': {
                    'type': 'Heatmap',
                    'data_source': 'correlation_matrix',
                    'rows': 'dimension_x',
                    'columns': 'dimension_y',
                    'color_metric': 'correlation_coefficient'
                },
                
                'political_tendency_pie': {
                    'type': 'Pie Chart',
                    'data_source': 'regional_summary',
                    'dimension': 'political_tendency',
                    'metric': 'population'
                },
                
                'prediction_accuracy_gauge': {
                    'type': 'Gauge Chart',
                    'data_source': 'regional_summary',
                    'metric': 'integrated_3d_score',
                    'min_value': 0,
                    'max_value': 1
                }
            },
            
            'filters': [
                {'name': 'region_type', 'type': 'dropdown'},
                {'name': 'political_tendency', 'type': 'multi-select'},
                {'name': 'date', 'type': 'date_range'}
            ],
            
            'kpis': [
                {'name': 'ì „êµ­ í‰ê·  ì˜ˆì¸¡ ì •í™•ë„', 'metric': 'AVG(integrated_3d_score)'},
                {'name': 'ì´ ì¸êµ¬', 'metric': 'SUM(population)'},
                {'name': 'ì˜ˆì¸¡ ì‹ ë¢°ë„', 'metric': 'COUNT(prediction_confidence = "HIGH")'}
            ]
        }
        
        return template

    def run_datastudio_export(self) -> Dict:
        """Google Data Studio ë‚´ë³´ë‚´ê¸° ì‹¤í–‰"""
        logger.info("ğŸš€ Google Data Studio ë‚´ë³´ë‚´ê¸° ì‹œì‘")
        
        start_time = datetime.now()
        results = {'success': True, 'exports': [], 'errors': []}
        
        try:
            # 1. ë°ì´í„° ë¡œë“œ
            print("1ï¸âƒ£ í†µí•© ë°ì´í„° ë¡œë“œ...")
            data = self.load_integrated_data()
            
            if not data:
                results['success'] = False
                results['errors'].append('ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨')
                return results
            
            # 2. ì§€ì—­ë³„ ìš”ì•½ í…Œì´ë¸” ìƒì„± ë° ë‚´ë³´ë‚´ê¸°
            print("2ï¸âƒ£ ì§€ì—­ë³„ ìš”ì•½ í…Œì´ë¸” ìƒì„±...")
            regional_df = self.create_regional_summary_table(data)
            if not regional_df.empty:
                csv_path = self.export_to_csv(regional_df, 'regional_summary.csv')
                if csv_path:
                    results['exports'].append({
                        'name': 'ì§€ì—­ë³„ ìš”ì•½',
                        'file': csv_path,
                        'rows': len(regional_df),
                        'columns': len(regional_df.columns)
                    })
            
            # 3. ì‹œê³„ì—´ í…Œì´ë¸” ìƒì„± ë° ë‚´ë³´ë‚´ê¸°
            print("3ï¸âƒ£ ì‹œê³„ì—´ ë°ì´í„° í…Œì´ë¸” ìƒì„±...")
            timeseries_df = self.create_time_series_table(data)
            if not timeseries_df.empty:
                csv_path = self.export_to_csv(timeseries_df, 'time_series.csv')
                if csv_path:
                    results['exports'].append({
                        'name': 'ì‹œê³„ì—´ ë°ì´í„°',
                        'file': csv_path,
                        'rows': len(timeseries_df),
                        'columns': len(timeseries_df.columns)
                    })
            
            # 4. ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„± ë° ë‚´ë³´ë‚´ê¸°
            print("4ï¸âƒ£ ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±...")
            correlation_df = self.create_correlation_matrix_table(data)
            if not correlation_df.empty:
                csv_path = self.export_to_csv(correlation_df, 'correlation_matrix.csv')
                if csv_path:
                    results['exports'].append({
                        'name': 'ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤',
                        'file': csv_path,
                        'rows': len(correlation_df),
                        'columns': len(correlation_df.columns)
                    })
            
            # 5. ëŒ€ì‹œë³´ë“œ í…œí”Œë¦¿ ìƒì„±
            print("5ï¸âƒ£ ëŒ€ì‹œë³´ë“œ í…œí”Œë¦¿ ìƒì„±...")
            template = self.generate_datastudio_dashboard_template()
            
            template_path = "datastudio_exports/dashboard_template.json"
            with open(template_path, 'w', encoding='utf-8') as f:
                json.dump(template, f, ensure_ascii=False, indent=2)
            
            results['exports'].append({
                'name': 'ëŒ€ì‹œë³´ë“œ í…œí”Œë¦¿',
                'file': template_path,
                'type': 'template'
            })
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            results['duration'] = duration
            results['total_exports'] = len(results['exports'])
            
            logger.info(f"âœ… Google Data Studio ë‚´ë³´ë‚´ê¸° ì™„ë£Œ! ì†Œìš”ì‹œê°„: {duration:.1f}ì´ˆ")
            return results
            
        except Exception as e:
            logger.error(f"âŒ Data Studio ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            results['success'] = False
            results['errors'].append(str(e))
            return results

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    exporter = GoogleDataStudioExporter()
    
    print("ğŸ“Š Google Data Studio ì—°ë™ ì‹œìŠ¤í…œ")
    print("=" * 50)
    print("ğŸ¯ ëª©ì : 3ì°¨ì› í†µí•© ë°ì´í„°ì˜ ê³ ê¸‰ ì‹œê°í™”")
    print("ğŸ“ˆ ë„êµ¬: Google Data Studio + Google Sheets")
    print("ğŸ—ºï¸ ì§€ë„: Google Maps í†µí•©")
    print("ğŸ“Š ì°¨íŠ¸: ì¸í„°ë™í‹°ë¸Œ ëŒ€ì‹œë³´ë“œ")
    print("=" * 50)
    
    # Data Studio ë‚´ë³´ë‚´ê¸° ì‹¤í–‰
    result = exporter.run_datastudio_export()
    
    if result['success']:
        print(f"\nğŸ‰ Data Studio ë‚´ë³´ë‚´ê¸° ì„±ê³µ!")
        print(f"â±ï¸ ì†Œìš”ì‹œê°„: {result['duration']:.1f}ì´ˆ")
        print(f"ğŸ“Š ë‚´ë³´ë‚¸ ë°ì´í„°ì…‹: {result['total_exports']}ê°œ")
        
        print(f"\nğŸ“‹ ë‚´ë³´ë‚¸ íŒŒì¼ë“¤:")
        for export in result['exports']:
            print(f"  ğŸ“ {export['name']}: {export['file']}")
            if 'rows' in export:
                print(f"     ğŸ“Š {export['rows']}í–‰ Ã— {export['columns']}ì—´")
        
        print(f"\nğŸ”— Google Data Studio ì—°ë™ ë°©ë²•:")
        print(f"  1ï¸âƒ£ datastudio_exports/ í´ë”ì˜ CSV íŒŒì¼ë“¤ì„ Google Driveì— ì—…ë¡œë“œ")
        print(f"  2ï¸âƒ£ Google Sheetsë¡œ ë³€í™˜")
        print(f"  3ï¸âƒ£ Data Studioì—ì„œ Google Sheets ì»¤ë„¥í„° ì„ íƒ")
        print(f"  4ï¸âƒ£ dashboard_template.json ì°¸ê³ í•˜ì—¬ ì°¨íŠ¸ êµ¬ì„±")
        print(f"  5ï¸âƒ£ Google Maps ì—°ë™ìœ¼ë¡œ ì§€ë¦¬ì  ì‹œê°í™” ì™„ì„±")
        
        print(f"\nâœ¨ ì¶”ì²œ ì‹œê°í™”:")
        print(f"  ğŸ—ºï¸ ì§€ì—­ë³„ ì˜ˆì¸¡ ì •í™•ë„ - Google Maps")
        print(f"  ğŸ“ˆ ì‹œê°„ë³„ ì¸êµ¬ ë³€í™” - Time Series Chart") 
        print(f"  ğŸ”¥ ìƒê´€ê´€ê³„ ë¶„ì„ - Heatmap")
        print(f"  ğŸ¥§ ì •ì¹˜ ì„±í–¥ ë¶„í¬ - Pie Chart")
        print(f"  â° ì‹¤ì‹œê°„ KPI - Scorecard")
        
    else:
        print(f"\nâŒ Data Studio ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨:")
        for error in result['errors']:
            print(f"  âŒ {error}")

if __name__ == "__main__":
    main()
