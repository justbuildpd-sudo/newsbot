#!/usr/bin/env python3
"""
ê°œì„ ëœ ì •ì±…ì„ ê±°ë¬¸í™” í…ìŠ¤íŠ¸ë§ˆì´ë‹ ì‹œìŠ¤í…œ
konlpy ì—†ì´ë„ ì‘ë™í•˜ëŠ” ê¸°ë³¸ í…ìŠ¤íŠ¸ ë¶„ì„ + ì§€ì—­ë³„ ë¯¸ìƒí† í”½ ì¶”ì¶œ
"""

import os
import json
import logging
import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
import re
import fitz  # PyMuPDF for PDF processing
from collections import defaultdict, Counter
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

logger = logging.getLogger(__name__)

class EnhancedPolicyTextMiningSystem:
    """ê°œì„ ëœ ì •ì±… í…ìŠ¤íŠ¸ë§ˆì´ë‹ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.base_dir = "/Users/hopidaay/newsbot-kr"
        self.backend_dir = "/Users/hopidaay/newsbot-kr/backend"
        self.pdf_file = "/Users/hopidaay/Desktop/231215_ì •ì±…ì„ ê±°ë¬¸í™”_í™•ì‚°ì„_ìœ„í•œ_ì–¸ë¡ ê¸°ì‚¬_ë¹…ë°ì´í„°_ë¶„ì„.pdf"
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        self.output_dir = os.path.join(self.backend_dir, "enhanced_policy_analysis")
        os.makedirs(self.output_dir, exist_ok=True)
        
        # ë¯¸ìƒí† í”½ ì¹´í…Œê³ ë¦¬ (í™•ì¥)
        self.misaeng_topics = {
            'ê²½ì œì •ì±…': {
                'keywords': ['ì¼ìë¦¬', 'ì·¨ì—…', 'ì°½ì—…', 'ê²½ì œì„±ì¥', 'ì†Œë“', 'ì„ê¸ˆ', 'ê³ ìš©', 'ì‹¤ì—…', 'ê²½ì œí™œë™', 'ì‚¬ì—…', 'íˆ¬ì', 'ê¸ˆìœµ'],
                'description': 'ì¼ìë¦¬ ì°½ì¶œ, ê²½ì œ í™œì„±í™”, ì†Œë“ ì¦ëŒ€'
            },
            'ì£¼ê±°ì •ì±…': {
                'keywords': ['ì£¼íƒ', 'ë¶€ë™ì‚°', 'ì„ëŒ€', 'ì „ì„¸', 'ì›”ì„¸', 'ì•„íŒŒíŠ¸', 'ì§‘ê°’', 'ì£¼ê±°', 'ë¶„ì–‘', 'ë§¤ë§¤', 'ì£¼íƒê³µê¸‰'],
                'description': 'ì£¼íƒ ê³µê¸‰, ë¶€ë™ì‚° ì•ˆì •í™”, ì£¼ê±° ë³µì§€'
            },
            'êµìœ¡ì •ì±…': {
                'keywords': ['êµìœ¡', 'í•™êµ', 'ëŒ€í•™', 'ì…ì‹œ', 'ì‚¬êµìœ¡', 'êµìœ¡ë¹„', 'í•™ìŠµ', 'í•™ìƒ', 'êµì‚¬', 'êµìœ¡ê³¼ì •', 'í•™ì›'],
                'description': 'êµìœ¡ í™˜ê²½ ê°œì„ , ì‚¬êµìœ¡ ë¶€ë‹´ í•´ì†Œ'
            },
            'ë³µì§€ì •ì±…': {
                'keywords': ['ë³µì§€', 'ì˜ë£Œ', 'ê±´ê°•ë³´í—˜', 'ì—°ê¸ˆ', 'ìœ¡ì•„', 'ë³´ìœ¡', 'ì‚¬íšŒë³´ì¥', 'ë³µì§€í˜œíƒ', 'ì§€ì›ê¸ˆ', 'ëŒë´„'],
                'description': 'ì‚¬íšŒë³µì§€ í™•ì¶©, ì˜ë£Œ ì„œë¹„ìŠ¤ ê°œì„ '
            },
            'í™˜ê²½ì •ì±…': {
                'keywords': ['í™˜ê²½', 'ê¸°í›„', 'ì—ë„ˆì§€', 'ë¯¸ì„¸ë¨¼ì§€', 'ì¬ìƒì—ë„ˆì§€', 'ì¹œí™˜ê²½', 'íƒ„ì†Œ', 'ì˜¤ì—¼', 'ë…¹ìƒ‰', 'ì§€ì†ê°€ëŠ¥'],
                'description': 'í™˜ê²½ ë³´í˜¸, ì§€ì†ê°€ëŠ¥í•œ ë°œì „'
            },
            'êµí†µì •ì±…': {
                'keywords': ['êµí†µ', 'ëŒ€ì¤‘êµí†µ', 'ì§€í•˜ì² ', 'ë²„ìŠ¤', 'ë„ë¡œ', 'ì£¼ì°¨', 'êµí†µì²´ì¦', 'ì´ë™', 'ì ‘ê·¼ì„±', 'ì¸í”„ë¼'],
                'description': 'êµí†µ ì¸í”„ë¼ ê°œì„ , ëŒ€ì¤‘êµí†µ í™•ì¶©'
            },
            'ë¬¸í™”ì •ì±…': {
                'keywords': ['ë¬¸í™”', 'ì˜ˆìˆ ', 'ì²´ìœ¡', 'ê´€ê´‘', 'ì¶•ì œ', 'ê³µì—°', 'ë¬¸í™”ì‹œì„¤', 'ì—¬ê°€', 'ìŠ¤í¬ì¸ ', 'ì½˜í…ì¸ '],
                'description': 'ë¬¸í™” ì‹œì„¤ í™•ì¶©, ê´€ê´‘ ì‚°ì—… ë°œì „'
            },
            'ì•ˆì „ì •ì±…': {
                'keywords': ['ì•ˆì „', 'ì¹˜ì•ˆ', 'ì¬í•´', 'ë°©ë²”', 'ì†Œë°©', 'ì‘ê¸‰', 'ë²”ì£„', 'ì‚¬ê³ ', 'ìœ„í—˜', 'ë³´ì•ˆ'],
                'description': 'ì•ˆì „í•œ ìƒí™œ í™˜ê²½ ì¡°ì„±'
            }
        }
        
        # ì§€ì—­ ê³„ì¸µ êµ¬ì¡°
        self.region_hierarchy = {
            'ê´‘ì—­ì‹œë„': {
                'ì„œìš¸': 'ì„œìš¸íŠ¹ë³„ì‹œ',
                'ë¶€ì‚°': 'ë¶€ì‚°ê´‘ì—­ì‹œ',
                'ëŒ€êµ¬': 'ëŒ€êµ¬ê´‘ì—­ì‹œ',
                'ì¸ì²œ': 'ì¸ì²œê´‘ì—­ì‹œ',
                'ê´‘ì£¼': 'ê´‘ì£¼ê´‘ì—­ì‹œ',
                'ëŒ€ì „': 'ëŒ€ì „ê´‘ì—­ì‹œ',
                'ìš¸ì‚°': 'ìš¸ì‚°ê´‘ì—­ì‹œ',
                'ì„¸ì¢…': 'ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ',
                'ê²½ê¸°': 'ê²½ê¸°ë„',
                'ê°•ì›': 'ê°•ì›ë„',
                'ì¶©ë¶': 'ì¶©ì²­ë¶ë„',
                'ì¶©ë‚¨': 'ì¶©ì²­ë‚¨ë„',
                'ì „ë¶': 'ì „ë¼ë¶ë„',
                'ì „ë‚¨': 'ì „ë¼ë‚¨ë„',
                'ê²½ë¶': 'ê²½ìƒë¶ë„',
                'ê²½ë‚¨': 'ê²½ìƒë‚¨ë„',
                'ì œì£¼': 'ì œì£¼íŠ¹ë³„ìì¹˜ë„'
            }
        }
        
        # ë¶„ì„ ê²°ê³¼
        self.analysis_results = {
            'document_info': {},
            'regional_misaeng_topics': {},
            'topic_interpretations': {},
            'policy_promises': {},
            'hierarchical_data': {},
            'visualization_ready_data': {}
        }

    def extract_pdf_content(self) -> str:
        """PDF ë‚´ìš© ì¶”ì¶œ"""
        try:
            print("ğŸ“„ PDF íŒŒì¼ ë‚´ìš© ì¶”ì¶œ ì¤‘...")
            
            if not os.path.exists(self.pdf_file):
                raise FileNotFoundError(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.pdf_file}")
            
            doc = fitz.open(self.pdf_file)
            full_text = ""
            total_pages = len(doc)
            
            print(f"ğŸ“Š ì´ {total_pages} í˜ì´ì§€ ì²˜ë¦¬ ì¤‘...")
            
            for page_num in range(total_pages):
                page = doc[page_num]
                page_text = page.get_text()
                full_text += page_text + '\n'
                
                if page_num % 50 == 0:
                    print(f"  ğŸ“„ í˜ì´ì§€ {page_num + 1}/{total_pages} ì²˜ë¦¬ ì™„ë£Œ")
            
            doc.close()
            
            self.analysis_results['document_info'] = {
                'file_path': self.pdf_file,
                'total_pages': total_pages,
                'total_text_length': len(full_text),
                'extraction_date': datetime.now().isoformat()
            }
            
            print(f"âœ… PDF ì¶”ì¶œ ì™„ë£Œ: {len(full_text):,} ë¬¸ì")
            return full_text
            
        except Exception as e:
            logger.error(f"âŒ PDF ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return ""

    def basic_korean_tokenizer(self, text: str) -> List[str]:
        """ê¸°ë³¸ í•œêµ­ì–´ í† í°í™” (konlpy ì—†ì´)"""
        try:
            # í…ìŠ¤íŠ¸ ì •ì œ
            text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
            text = re.sub(r'\s+', ' ', text)
            text = text.strip()
            
            # ê¸°ë³¸ í† í°í™” (ê³µë°± ê¸°ì¤€)
            tokens = text.split()
            
            # í•œê¸€ í† í°ë§Œ í•„í„°ë§ (2ê¸€ì ì´ìƒ)
            korean_tokens = []
            for token in tokens:
                if re.search(r'[ê°€-í£]{2,}', token):
                    korean_tokens.append(token)
            
            # ë¶ˆìš©ì–´ ì œê±°
            stopwords = {'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë”°ë¼ì„œ', 'ì´ë¥¼', 'ì´ì—', 'ëŒ€í•œ', 'ìœ„í•œ', 'í†µí•´', 'ìˆë‹¤', 'ì—†ë‹¤', 'ëœë‹¤', 'í•œë‹¤', 'ì´ë‹¤', 'ê²ƒì´ë‹¤', 'ìˆ˜ìˆë‹¤'}
            filtered_tokens = [token for token in korean_tokens if token not in stopwords]
            
            return filtered_tokens[:100]  # ìƒìœ„ 100ê°œ í† í°ë§Œ
            
        except Exception as e:
            logger.error(f"âŒ í† í°í™” ì‹¤íŒ¨: {e}")
            return []

    def extract_regional_misaeng_topics(self, full_text: str) -> Dict[str, Any]:
        """ì§€ì—­ë³„ ë¯¸ìƒí† í”½ ì¶”ì¶œ"""
        try:
            print("ğŸ—ºï¸ ì§€ì—­ë³„ ë¯¸ìƒí† í”½ ì¶”ì¶œ ì¤‘...")
            
            regional_topics = {}
            
            # 1. ê´‘ì—­ì‹œë„ë³„ ë¶„ì„
            print("  ğŸ“ ê´‘ì—­ì‹œë„ ë¶„ì„ ì¤‘...")
            for region_short, region_full in self.region_hierarchy['ê´‘ì—­ì‹œë„'].items():
                region_data = self._analyze_region_topics(full_text, region_short, region_full, 'ê´‘ì—­ì‹œë„')
                if region_data['mention_count'] > 0:
                    regional_topics[region_short] = region_data
            
            # 2. ì£¼ìš” ì‹œêµ°êµ¬ ë¶„ì„ (ìƒìœ„ ì–¸ê¸‰)
            print("  ğŸ“ ì£¼ìš” ì‹œêµ°êµ¬ ë¶„ì„ ì¤‘...")
            sigungu_pattern = r'(\w{1,4}[ì‹œêµ°êµ¬])'
            sigungu_matches = re.findall(sigungu_pattern, full_text)
            sigungu_counter = Counter(sigungu_matches)
            
            for sigungu, count in sigungu_counter.most_common(20):  # ìƒìœ„ 20ê°œ
                if count > 5:  # 5íšŒ ì´ìƒ ì–¸ê¸‰ëœ ê³³ë§Œ
                    region_data = self._analyze_region_topics(full_text, sigungu, sigungu, 'ì‹œêµ°êµ¬')
                    regional_topics[f"{sigungu}"] = region_data
            
            # 3. ì£¼ìš” ìë©´ë™ ë¶„ì„
            print("  ğŸ“ ì£¼ìš” ìë©´ë™ ë¶„ì„ ì¤‘...")
            emd_pattern = r'(\w{1,4}[ìë©´ë™])'
            emd_matches = re.findall(emd_pattern, full_text)
            emd_counter = Counter(emd_matches)
            
            for emd, count in emd_counter.most_common(30):  # ìƒìœ„ 30ê°œ
                if count > 3:  # 3íšŒ ì´ìƒ ì–¸ê¸‰ëœ ê³³ë§Œ
                    region_data = self._analyze_region_topics(full_text, emd, emd, 'ìë©´ë™')
                    regional_topics[f"{emd}"] = region_data
            
            self.analysis_results['regional_misaeng_topics'] = regional_topics
            
            print(f"âœ… ì§€ì—­ë³„ ë¯¸ìƒí† í”½ ì¶”ì¶œ ì™„ë£Œ: {len(regional_topics)}ê°œ ì§€ì—­")
            return regional_topics
            
        except Exception as e:
            logger.error(f"âŒ ì§€ì—­ë³„ í† í”½ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}

    def _analyze_region_topics(self, full_text: str, region_key: str, region_name: str, level: str) -> Dict[str, Any]:
        """ê°œë³„ ì§€ì—­ì˜ í† í”½ ë¶„ì„"""
        try:
            # í•´ë‹¹ ì§€ì—­ ê´€ë ¨ ë¬¸ì¥ ì¶”ì¶œ
            region_sentences = []
            for sentence in full_text.split('.'):
                if region_key in sentence or region_name in sentence:
                    clean_sentence = sentence.strip()
                    if len(clean_sentence) > 10:
                        region_sentences.append(clean_sentence)
            
            if not region_sentences:
                return {
                    'region': region_name,
                    'level': level,
                    'mention_count': 0,
                    'dominant_topics': [],
                    'topic_scores': {},
                    'sentences': [],
                    'tokens': [],
                    'promises': []
                }
            
            # ì§€ì—­ ê´€ë ¨ í…ìŠ¤íŠ¸ ê²°í•©
            region_text = ' '.join(region_sentences)
            
            # í† í°í™”
            tokens = self.basic_korean_tokenizer(region_text)
            
            # ë¯¸ìƒí† í”½ë³„ ì ìˆ˜ ê³„ì‚°
            topic_scores = {}
            for topic, topic_info in self.misaeng_topics.items():
                score = 0
                keywords = topic_info['keywords']
                for token in tokens:
                    for keyword in keywords:
                        if keyword in token or token in keyword:
                            score += 1
                
                if score > 0:
                    topic_scores[topic] = score
            
            # ìƒìœ„ í† í”½ ì •ë ¬
            sorted_topics = sorted(topic_scores.items(), key=lambda x: x[1], reverse=True)
            dominant_topics = [topic for topic, score in sorted_topics[:3]]  # ìƒìœ„ 3ê°œ
            
            # ê³µì•½ì„± í‘œí˜„ ì¶”ì¶œ
            promises = self._extract_promises_from_sentences(region_sentences)
            
            return {
                'region': region_name,
                'level': level,
                'mention_count': len(region_sentences),
                'dominant_topics': dominant_topics,
                'topic_scores': topic_scores,
                'sentences': region_sentences[:5],  # ìƒìœ„ 5ê°œ ë¬¸ì¥
                'tokens': tokens[:20],  # ìƒìœ„ 20ê°œ í† í°
                'promises': promises
            }
            
        except Exception as e:
            logger.error(f"âŒ ì§€ì—­ {region_name} ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}

    def _extract_promises_from_sentences(self, sentences: List[str]) -> List[str]:
        """ë¬¸ì¥ì—ì„œ ê³µì•½ì„± í‘œí˜„ ì¶”ì¶œ"""
        promises = []
        
        # ê³µì•½ ê´€ë ¨ íŒ¨í„´
        promise_patterns = [
            r'(\w+ì„/ë¥¼?\s*(?:í™•ëŒ€|ê°•í™”|ê°œì„ |ì§€ì›|ì¶”ì§„|ë„ì…|ì‹¤ì‹œ|ì‹œí–‰))',
            r'(\w+\s*(?:ì •ì±…|ì‚¬ì—…|í”„ë¡œê·¸ë¨|ì œë„|ê³„íš)ì„/ë¥¼?\s*(?:ë§ˆë ¨|ìˆ˜ë¦½|ì¶”ì§„|ì‹¤í–‰))',
            r'(\w+ì„/ë¥¼?\s*ìœ„í•œ\s*\w+)',
            r'(\w+\s*(?:ì¡°ì„±|ê±´ì„¤|ì„¤ì¹˜|êµ¬ì¶•|í™•ì¶©))',
            r'(\w+ì—\s*ëŒ€í•œ\s*\w+\s*(?:ê°•í™”|ê°œì„ |í™•ëŒ€))'
        ]
        
        for sentence in sentences:
            for pattern in promise_patterns:
                matches = re.findall(pattern, sentence)
                for match in matches:
                    clean_match = match.strip()
                    if len(clean_match) > 3 and clean_match not in promises:
                        promises.append(clean_match)
        
        return promises[:5]  # ìƒìœ„ 5ê°œ

    def interpret_regional_topics(self, regional_topics: Dict[str, Any]) -> Dict[str, Any]:
        """ì§€ì—­ë³„ í† í”½ í•´ì„"""
        try:
            print("ğŸ¯ ì§€ì—­ë³„ í† í”½ í•´ì„ ì¤‘...")
            
            interpretations = {}
            
            for region, data in regional_topics.items():
                if data['mention_count'] > 0:
                    interpretation = self._generate_region_interpretation(region, data)
                    interpretations[region] = interpretation
            
            self.analysis_results['topic_interpretations'] = interpretations
            
            print(f"âœ… í† í”½ í•´ì„ ì™„ë£Œ: {len(interpretations)}ê°œ ì§€ì—­")
            return interpretations
            
        except Exception as e:
            logger.error(f"âŒ í† í”½ í•´ì„ ì‹¤íŒ¨: {e}")
            return {}

    def _generate_region_interpretation(self, region: str, data: Dict[str, Any]) -> Dict[str, Any]:
        """ê°œë³„ ì§€ì—­ í•´ì„ ìƒì„±"""
        try:
            dominant_topics = data['dominant_topics']
            topic_scores = data['topic_scores']
            level = data['level']
            mention_count = data['mention_count']
            promises = data['promises']
            
            # ê¸°ë³¸ í•´ì„ ìƒì„±
            if dominant_topics:
                main_topic = dominant_topics[0]
                topic_description = self.misaeng_topics[main_topic]['description']
                
                interpretation = f"{region}ì€ {main_topic} ë¶„ì•¼ê°€ ì£¼ìš” ê´€ì‹¬ì‚¬ë¡œ, {topic_description}ì— ëŒ€í•œ ë…¼ì˜ê°€ í™œë°œí•¨"
                
                # ì¶”ê°€ í† í”½ë“¤
                if len(dominant_topics) > 1:
                    additional_topics = ', '.join(dominant_topics[1:])
                    interpretation += f". ë˜í•œ {additional_topics} ë¶„ì•¼ë„ í•¨ê»˜ ì£¼ëª©ë°›ê³  ìˆìŒ"
            else:
                interpretation = f"{region}ì— ëŒ€í•œ ì •ì±…ì  ë…¼ì˜ê°€ ì§„í–‰ë˜ê³  ìˆìœ¼ë‚˜ íŠ¹ì • ë¶„ì•¼ì— ì§‘ì¤‘ë˜ì§€ëŠ” ì•ŠìŒ"
            
            # ê³µì•½ ìš”ì•½
            promise_summary = ""
            if promises:
                promise_summary = f"ì£¼ìš” ê³µì•½: {', '.join(promises)}"
            
            return {
                'region': region,
                'level': level,
                'main_interpretation': interpretation,
                'dominant_topics': dominant_topics,
                'topic_scores': topic_scores,
                'mention_count': mention_count,
                'promise_summary': promise_summary,
                'promises': promises,
                'confidence_score': sum(topic_scores.values()) if topic_scores else 0
            }
            
        except Exception as e:
            logger.error(f"âŒ ì§€ì—­ {region} í•´ì„ ìƒì„± ì‹¤íŒ¨: {e}")
            return {}

    def create_hierarchical_data(self, regional_topics: Dict[str, Any]) -> Dict[str, Any]:
        """ê³„ì¸µì  ì§€ì—­ ë°ì´í„° ìƒì„±"""
        try:
            print("ğŸ—ï¸ ê³„ì¸µì  ì§€ì—­ ë°ì´í„° ìƒì„± ì¤‘...")
            
            hierarchical_data = {
                'ê´‘ì—­ì‹œë„': [],
                'ì‹œêµ°êµ¬': [],
                'ìë©´ë™': []
            }
            
            for region, data in regional_topics.items():
                level = data.get('level', 'ê¸°íƒ€')
                
                if level in hierarchical_data:
                    hierarchical_data[level].append({
                        'region': region,
                        'mention_count': data['mention_count'],
                        'dominant_topics': data['dominant_topics'][:3],
                        'confidence_score': sum(data['topic_scores'].values()) if data['topic_scores'] else 0
                    })
            
            # ê° ë ˆë²¨ë³„ë¡œ ì–¸ê¸‰ íšŸìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            for level in hierarchical_data:
                hierarchical_data[level].sort(key=lambda x: x['mention_count'], reverse=True)
            
            self.analysis_results['hierarchical_data'] = hierarchical_data
            
            print(f"âœ… ê³„ì¸µì  ë°ì´í„° ìƒì„± ì™„ë£Œ")
            return hierarchical_data
            
        except Exception as e:
            logger.error(f"âŒ ê³„ì¸µì  ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")
            return {}

    def create_visualization_data(self, regional_topics: Dict[str, Any], interpretations: Dict[str, Any]) -> Dict[str, Any]:
        """ì‹œê°í™”ìš© ë°ì´í„° ìƒì„±"""
        try:
            print("ğŸ“Š ì‹œê°í™” ë°ì´í„° ìƒì„± ì¤‘...")
            
            viz_data = {
                'regional_topic_distribution': self._create_topic_distribution_data(regional_topics),
                'top_regions_by_topic': self._create_top_regions_by_topic(regional_topics),
                'promise_analysis': self._create_promise_analysis_data(regional_topics),
                'regional_network': self._create_regional_network_data(regional_topics),
                'topic_hierarchy_map': self._create_topic_hierarchy_map(regional_topics)
            }
            
            self.analysis_results['visualization_ready_data'] = viz_data
            
            print(f"âœ… ì‹œê°í™” ë°ì´í„° ìƒì„± ì™„ë£Œ")
            return viz_data
            
        except Exception as e:
            logger.error(f"âŒ ì‹œê°í™” ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")
            return {}

    def _create_topic_distribution_data(self, regional_topics: Dict[str, Any]) -> Dict[str, Any]:
        """í† í”½ ë¶„í¬ ë°ì´í„° ìƒì„±"""
        topic_distribution = defaultdict(int)
        level_distribution = defaultdict(int)
        
        for region, data in regional_topics.items():
            level = data.get('level', 'ê¸°íƒ€')
            level_distribution[level] += 1
            
            for topic in data.get('dominant_topics', []):
                topic_distribution[topic] += 1
        
        return {
            'topic_counts': dict(topic_distribution),
            'level_counts': dict(level_distribution),
            'total_regions': len(regional_topics)
        }

    def _create_top_regions_by_topic(self, regional_topics: Dict[str, Any]) -> Dict[str, List]:
        """í† í”½ë³„ ìƒìœ„ ì§€ì—­ ë°ì´í„°"""
        topic_regions = defaultdict(list)
        
        for region, data in regional_topics.items():
            for topic in data.get('dominant_topics', []):
                topic_score = data.get('topic_scores', {}).get(topic, 0)
                topic_regions[topic].append({
                    'region': region,
                    'score': topic_score,
                    'mention_count': data['mention_count']
                })
        
        # ê° í† í”½ë³„ë¡œ ì ìˆ˜ ìˆœ ì •ë ¬
        for topic in topic_regions:
            topic_regions[topic].sort(key=lambda x: x['score'], reverse=True)
            topic_regions[topic] = topic_regions[topic][:10]  # ìƒìœ„ 10ê°œë§Œ
        
        return dict(topic_regions)

    def _create_promise_analysis_data(self, regional_topics: Dict[str, Any]) -> Dict[str, Any]:
        """ê³µì•½ ë¶„ì„ ë°ì´í„°"""
        all_promises = []
        promise_by_topic = defaultdict(list)
        
        for region, data in regional_topics.items():
            promises = data.get('promises', [])
            all_promises.extend(promises)
            
            for topic in data.get('dominant_topics', []):
                promise_by_topic[topic].extend(promises)
        
        return {
            'total_promises': len(all_promises),
            'unique_promises': len(set(all_promises)),
            'promise_by_topic': {k: len(set(v)) for k, v in promise_by_topic.items()},
            'common_promises': [item for item, count in Counter(all_promises).most_common(10)]
        }

    def _create_regional_network_data(self, regional_topics: Dict[str, Any]) -> Dict[str, Any]:
        """ì§€ì—­ ë„¤íŠ¸ì›Œí¬ ë°ì´í„°"""
        nodes = []
        edges = []
        
        # ë…¸ë“œ ìƒì„± (ì§€ì—­)
        for region, data in regional_topics.items():
            nodes.append({
                'id': region,
                'label': region,
                'level': data.get('level', 'ê¸°íƒ€'),
                'size': data['mention_count'],
                'topics': data.get('dominant_topics', [])
            })
        
        # ì—£ì§€ ìƒì„± (ê³µí†µ í† í”½ì„ ê°€ì§„ ì§€ì—­ë“¤ ì—°ê²°)
        regions_list = list(regional_topics.items())
        for i, (region1, data1) in enumerate(regions_list):
            for region2, data2 in regions_list[i+1:]:
                common_topics = set(data1.get('dominant_topics', [])) & set(data2.get('dominant_topics', []))
                if common_topics:
                    edges.append({
                        'source': region1,
                        'target': region2,
                        'weight': len(common_topics),
                        'common_topics': list(common_topics)
                    })
        
        return {'nodes': nodes, 'edges': edges}

    def _create_topic_hierarchy_map(self, regional_topics: Dict[str, Any]) -> Dict[str, Any]:
        """í† í”½ ê³„ì¸µ ë§µ ë°ì´í„°"""
        hierarchy_map = {}
        
        for level in ['ê´‘ì—­ì‹œë„', 'ì‹œêµ°êµ¬', 'ìë©´ë™']:
            level_data = []
            for region, data in regional_topics.items():
                if data.get('level') == level:
                    level_data.append({
                        'region': region,
                        'topics': data.get('dominant_topics', []),
                        'mention_count': data['mention_count'],
                        'promises': len(data.get('promises', []))
                    })
            
            hierarchy_map[level] = sorted(level_data, key=lambda x: x['mention_count'], reverse=True)
        
        return hierarchy_map

    def save_results(self) -> str:
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        try:
            print("ğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘...")
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            results_file = os.path.join(self.output_dir, f"enhanced_policy_analysis_{timestamp}.json")
            
            # ì „ì²´ ê²°ê³¼ ì €ì¥
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(self.analysis_results, f, ensure_ascii=False, indent=2)
            
            # ì§€ì—­ë³„ ê²°ê³¼ì°½ìš© ë°ì´í„° ë³„ë„ ì €ì¥
            frontend_data = self._prepare_frontend_data()
            frontend_file = os.path.join(self.output_dir, f"regional_misaeng_topics_frontend_{timestamp}.json")
            
            with open(frontend_file, 'w', encoding='utf-8') as f:
                json.dump(frontend_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
            print(f"  ğŸ“„ ì „ì²´ ê²°ê³¼: {results_file}")
            print(f"  ğŸ–¥ï¸ í”„ë¡ íŠ¸ì—”ë“œìš©: {frontend_file}")
            
            return results_file
            
        except Exception as e:
            logger.error(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return None

    def _prepare_frontend_data(self) -> Dict[str, Any]:
        """í”„ë¡ íŠ¸ì—”ë“œìš© ë°ì´í„° ì¤€ë¹„"""
        frontend_data = {
            'last_updated': datetime.now().isoformat(),
            'total_regions': len(self.analysis_results.get('regional_misaeng_topics', {})),
            'misaeng_topic_categories': self.misaeng_topics,
            'regional_data': {}
        }
        
        # ì§€ì—­ë³„ ë°ì´í„° êµ¬ì¡°í™”
        for region, data in self.analysis_results.get('regional_misaeng_topics', {}).items():
            if data['mention_count'] > 0:
                frontend_data['regional_data'][region] = {
                    'region_name': data['region'],
                    'level': data['level'],
                    'mention_count': data['mention_count'],
                    'dominant_topics': data['dominant_topics'],
                    'topic_scores': data['topic_scores'],
                    'interpretation': self.analysis_results.get('topic_interpretations', {}).get(region, {}).get('main_interpretation', ''),
                    'promises': data['promises'],
                    'confidence_score': sum(data['topic_scores'].values()) if data['topic_scores'] else 0,
                    'sample_sentences': data['sentences'][:3]  # ìƒìœ„ 3ê°œ ë¬¸ì¥
                }
        
        return frontend_data

    def run_complete_analysis(self) -> Dict[str, Any]:
        """ì „ì²´ ë¶„ì„ ì‹¤í–‰"""
        print("ğŸš€ ê°œì„ ëœ ì •ì±…ì„ ê±°ë¬¸í™” í…ìŠ¤íŠ¸ë§ˆì´ë‹ ì‹œì‘")
        print("=" * 80)
        
        try:
            # 1. PDF ë‚´ìš© ì¶”ì¶œ
            print("\nğŸ“„ 1ë‹¨ê³„: PDF ë‚´ìš© ì¶”ì¶œ")
            full_text = self.extract_pdf_content()
            if not full_text:
                raise Exception("PDF ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨")
            
            # 2. ì§€ì—­ë³„ ë¯¸ìƒí† í”½ ì¶”ì¶œ
            print("\nğŸ—ºï¸ 2ë‹¨ê³„: ì§€ì—­ë³„ ë¯¸ìƒí† í”½ ì¶”ì¶œ")
            regional_topics = self.extract_regional_misaeng_topics(full_text)
            
            # 3. í† í”½ í•´ì„
            print("\nğŸ¯ 3ë‹¨ê³„: ì§€ì—­ë³„ í† í”½ í•´ì„")
            interpretations = self.interpret_regional_topics(regional_topics)
            
            # 4. ê³„ì¸µì  ë°ì´í„° ìƒì„±
            print("\nğŸ—ï¸ 4ë‹¨ê³„: ê³„ì¸µì  ì§€ì—­ ë°ì´í„° ìƒì„±")
            hierarchical_data = self.create_hierarchical_data(regional_topics)
            
            # 5. ì‹œê°í™” ë°ì´í„° ìƒì„±
            print("\nğŸ“Š 5ë‹¨ê³„: ì‹œê°í™” ë°ì´í„° ìƒì„±")
            viz_data = self.create_visualization_data(regional_topics, interpretations)
            
            # 6. ê²°ê³¼ ì €ì¥
            print("\nğŸ’¾ 6ë‹¨ê³„: ê²°ê³¼ ì €ì¥")
            results_file = self.save_results()
            
            print("\nğŸ‰ ê°œì„ ëœ ì •ì±…ì„ ê±°ë¬¸í™” ë¶„ì„ ì™„ë£Œ!")
            print("=" * 80)
            
            # ìš”ì•½ í†µê³„
            summary = self._generate_summary()
            
            return {
                'success': True,
                'results_file': results_file,
                'summary': summary,
                'completion_time': datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"\nâŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e),
                'completion_time': datetime.now().isoformat()
            }

    def _generate_summary(self) -> Dict[str, Any]:
        """ë¶„ì„ ìš”ì•½ ìƒì„±"""
        regional_topics = self.analysis_results.get('regional_misaeng_topics', {})
        
        # ë ˆë²¨ë³„ í†µê³„
        level_stats = defaultdict(int)
        topic_stats = defaultdict(int)
        
        for region, data in regional_topics.items():
            level_stats[data.get('level', 'ê¸°íƒ€')] += 1
            for topic in data.get('dominant_topics', []):
                topic_stats[topic] += 1
        
        return {
            'total_regions_analyzed': len(regional_topics),
            'regions_by_level': dict(level_stats),
            'top_topics': dict(Counter(topic_stats).most_common(5)),
            'total_promises_extracted': sum(len(data.get('promises', [])) for data in regional_topics.values()),
            'document_pages': self.analysis_results.get('document_info', {}).get('total_pages', 0),
            'text_length': self.analysis_results.get('document_info', {}).get('total_text_length', 0)
        }

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    analyzer = EnhancedPolicyTextMiningSystem()
    results = analyzer.run_complete_analysis()
    
    if results['success']:
        print(f"\nâœ… ë¶„ì„ ì„±ê³µ!")
        print(f"ğŸ“„ ê²°ê³¼ íŒŒì¼: {results['results_file']}")
        
        summary = results['summary']
        print(f"\nğŸ“Š ë¶„ì„ ìš”ì•½:")
        print(f"  ğŸ—ºï¸ ë¶„ì„ ì§€ì—­: {summary['total_regions_analyzed']}ê°œ")
        print(f"  ğŸ“„ ë¬¸ì„œ í˜ì´ì§€: {summary['document_pages']}í˜ì´ì§€")
        print(f"  ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´: {summary['text_length']:,}ì")
        print(f"  ğŸ¯ ì¶”ì¶œ ê³µì•½: {summary['total_promises_extracted']}ê°œ")
        
        print(f"\nğŸ›ï¸ ë ˆë²¨ë³„ ì§€ì—­:")
        for level, count in summary['regions_by_level'].items():
            print(f"    {level}: {count}ê°œ")
        
        print(f"\nğŸ”¥ ì£¼ìš” í† í”½:")
        for topic, count in summary['top_topics'].items():
            print(f"    {topic}: {count}ê°œ ì§€ì—­")
        
    else:
        print(f"\nâŒ ë¶„ì„ ì‹¤íŒ¨: {results['error']}")

if __name__ == "__main__":
    main()
