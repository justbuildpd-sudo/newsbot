import requests
import json
import time
import hashlib
from datetime import datetime, timedelta
from typing import List, Dict, Set
import re
from difflib import SequenceMatcher

class NewsService:
    def __init__(self):
        self.client_id = "kXwlSsFmb055ku9rWyx1"
        self.client_secret = "JZqw_LTiq_"
        self.base_url = "https://openapi.naver.com/v1/search/news.json"
        self.news_cache = {}  # ë‰´ìŠ¤ ìºì‹œ
        self.last_cleanup = datetime.now()
        self.cleanup_interval = timedelta(hours=4, minutes=15)  # 4ì‹œê°„ 15ë¶„
        self.last_news_fetch = None  # ë§ˆì§€ë§‰ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œê°„
        self.news_fetch_interval = timedelta(minutes=30)  # 30ë¶„ë§ˆë‹¤ ìƒˆë¡œ ìˆ˜ì§‘
        
        # ì •ì¹˜ ê´€ë ¨ í‚¤ì›Œë“œ
        self.political_keywords = [
            "ëŒ€í†µë ¹", "êµ­íšŒ", "ì˜ì›", "ë¯¼ì£¼ë‹¹", "êµ­ë¯¼ì˜í˜", 
            "íŠ¸ëŸ¼í”„", "ì´ë¦¬", "ì •ì¹˜", "ì •ë¶€", "ì—¬ë‹¹", "ì•¼ë‹¹"
        ]
        
        # ê²€ìƒ‰ í‚¤ì›Œë“œ (ë‹¨ë…, ì†ë³´)
        self.search_keywords = ["ë‹¨ë…", "ì†ë³´"]
    
    def get_news_from_naver(self, query: str, display: int = 100) -> List[Dict]:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ APIì—ì„œ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        headers = {
            'X-Naver-Client-Id': self.client_id,
            'X-Naver-Client-Secret': self.client_secret
        }
        
        params = {
            'query': query,
            'display': display,
            'sort': 'sim'  # ì •í™•ë„ìˆœ
        }
        
        try:
            response = requests.get(self.base_url, headers=headers, params=params)
            response.raise_for_status()
            data = response.json()
            return data.get('items', [])
        except Exception as e:
            print(f"ë„¤ì´ë²„ API ì˜¤ë¥˜: {e}")
            return []
    
    def get_news_content(self, news_url: str) -> Dict:
        """ë‰´ìŠ¤ ê¸°ì‚¬ ì „ë¬¸ ê°€ì ¸ì˜¤ê¸° (ì›¹ ìŠ¤í¬ë˜í•‘)"""
        try:
            import requests
            # BeautifulSoup ì œê±°ë¨
            import re
            import time
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ URL í™•ì¸
            if 'news.naver.com' not in news_url:
                return {
                    'title': '',
                    'content': 'ë„¤ì´ë²„ ë‰´ìŠ¤ê°€ ì•„ë‹™ë‹ˆë‹¤.',
                    'images': [],
                    'pub_date': '',
                    'url': news_url
                }
            
            # User-Agent ì„¤ì • (ë„¤ì´ë²„ ë´‡ ì°¨ë‹¨ ìš°íšŒ)
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Cache-Control': 'max-age=0'
            }
            
            # ìš”ì²­ ê°„ê²© ì¡°ì ˆ (DDoS ë°©ì§€)
            time.sleep(1)
            
            response = requests.get(news_url, headers=headers, timeout=15)
            response.raise_for_status()
            
            # BeautifulSoup ì œê±°ë¨ - ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ íŠ¹í™” ì…€ë ‰í„°
            title = ""
            title_selectors = [
                '.media_end_head_headline',  # ë„¤ì´ë²„ ë‰´ìŠ¤ ì œëª©
                'h1', 
                '.article_info h3', 
                '.news_title', 
                '.headline', 
                'title'
            ]
            
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    title = title_elem.get_text().strip()
                    # HTML íƒœê·¸ ì œê±°
                    title = re.sub(r'<[^>]+>', '', title)
                    break
            
            # ë³¸ë¬¸ ì¶”ì¶œ (ë„¤ì´ë²„ ë‰´ìŠ¤ íŠ¹í™”)
            content = ""
            content_selectors = [
                '#newsct_article',  # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸
                '.go_article',      # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸
                '.article_body', 
                '.news_body', 
                '.article_view', 
                '.article_content', 
                '.news_content', 
                '.content',
                '[id*="article"]', 
                '[class*="article"]', 
                '[class*="content"]'
            ]
            
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼, ê´‘ê³  íƒœê·¸ ì œê±°
                    for unwanted in content_elem(["script", "style", ".ad", ".advertisement", ".banner"]):
                        unwanted.decompose()
                    
                    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    content = content_elem.get_text().strip()
                    
                    # HTML íƒœê·¸ ì œê±°
                    content = re.sub(r'<[^>]+>', '', content)
                    
                    # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
                    content = re.sub(r'\s+', ' ', content)
                    
                    if len(content) > 100:  # ì¶©ë¶„í•œ ê¸¸ì´ì˜ ë‚´ìš©ì´ ìˆëŠ”ì§€ í™•ì¸
                        break
            
            # ì´ë¯¸ì§€ ì¶”ì¶œ (ë„¤ì´ë²„ ë‰´ìŠ¤ íŠ¹í™”)
            images = []
            img_selectors = [
                '#newsct_article img',  # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ ì´ë¯¸ì§€
                '.go_article img',      # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ ì´ë¯¸ì§€
                'img[src*="http"]', 
                '.article_body img', 
                '.news_body img',
                '.article_view img', 
                '.content img'
            ]
            
            for selector in img_selectors:
                img_elements = soup.select(selector)
                for img in img_elements:
                    src = img.get('src') or img.get('data-src') or img.get('data-original')
                    if src and src.startswith('http'):
                        # ë„¤ì´ë²„ ì´ë¯¸ì§€ URL ì •ë¦¬
                        if 'news.naver.com' in src:
                            # ë„¤ì´ë²„ ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (ë” í° ì´ë¯¸ì§€)
                            src = src.replace('type=f120_80', 'type=f640_480')
                        
                        alt = img.get('alt', '')
                        # ë¹ˆ alt í…ìŠ¤íŠ¸ ì²˜ë¦¬
                        if not alt:
                            alt = 'ë‰´ìŠ¤ ì´ë¯¸ì§€'
                        
                        images.append({
                            'src': src,
                            'alt': alt
                        })
            
            # ë°œí–‰ì¼ ì¶”ì¶œ
            pub_date = ""
            date_selectors = [
                '.article_info .t11', '.news_date', '.article_date',
                '.publish_date', '[class*="date"]', '[class*="time"]'
            ]
            
            for selector in date_selectors:
                date_elem = soup.select_one(selector)
                if date_elem:
                    pub_date = date_elem.get_text().strip()
                    break
            
            return {
                'title': title,
                'content': content,
                'images': images[:5],  # ìµœëŒ€ 5ê°œ ì´ë¯¸ì§€
                'pub_date': pub_date,
                'url': news_url
            }
            
        except Exception as e:
            print(f"ë‰´ìŠ¤ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
            return {
                'title': '',
                'content': 'ê¸°ì‚¬ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                'images': [],
                'pub_date': '',
                'url': news_url
            }
    
    def clean_html(self, text: str) -> str:
        """HTML íƒœê·¸ ì œê±°"""
        clean = re.compile('<.*?>')
        return re.sub(clean, '', text)
    
    def calculate_similarity(self, title1: str, title2: str) -> float:
        """ì œëª© ìœ ì‚¬ë„ ê³„ì‚° (0-1)"""
        return SequenceMatcher(None, title1, title2).ratio()
    
    def is_duplicate_news(self, new_title: str, existing_news: List[Dict], threshold: float = 0.6) -> bool:
        """ì¤‘ë³µ ë‰´ìŠ¤ í™•ì¸ (60% ì´ìƒ ìœ ì‚¬ë„)"""
        for news in existing_news:
            if self.calculate_similarity(new_title, news['title']) >= threshold:
                return True
        return False
    
    def is_recent_news(self, pub_date: str, hours: int = 4) -> bool:
        """ìµœê·¼ ë‰´ìŠ¤ì¸ì§€ í™•ì¸ (4ì‹œê°„ ì´ë‚´)"""
        try:
            # ë„¤ì´ë²„ API ë‚ ì§œ í˜•ì‹: "Mon, 14 Sep 2025 12:00:00 +0900"
            news_time = datetime.strptime(pub_date, "%a, %d %b %Y %H:%M:%S %z")
            cutoff_time = datetime.now(news_time.tzinfo) - timedelta(hours=hours)
            return news_time > cutoff_time
        except:
            return False
    
    def contains_political_keywords(self, title: str, description: str = "") -> bool:
        """ì •ì¹˜ ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸"""
        text = f"{title} {description}".lower()
        for keyword in self.political_keywords:
            if keyword in text:
                return True
        return False
    
    def filter_news(self, news_list: List[Dict]) -> List[Dict]:
        """ë‰´ìŠ¤ í•„í„°ë§ (ì¤‘ë³µ ì œê±°, ì‹œê°„ í•„í„°ë§, ì •ì¹˜ í‚¤ì›Œë“œ í•„í„°ë§)"""
        filtered_news = []
        
        for news in news_list:
            # HTML íƒœê·¸ ì œê±°
            title = self.clean_html(news.get('title', ''))
            description = self.clean_html(news.get('description', ''))
            pub_date = news.get('pubDate', '')
            
            # 1. ìµœê·¼ ë‰´ìŠ¤ì¸ì§€ í™•ì¸ (4ì‹œê°„ ì´ë‚´)
            if not self.is_recent_news(pub_date):
                continue
            
            # 2. ì •ì¹˜ ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸
            if not self.contains_political_keywords(title, description):
                continue
            
            # 3. ì¤‘ë³µ ë‰´ìŠ¤ í™•ì¸ (60% ì´ìƒ ìœ ì‚¬ë„)
            if self.is_duplicate_news(title, filtered_news):
                continue
            
            # í•„í„°ë§ í†µê³¼í•œ ë‰´ìŠ¤ ì¶”ê°€
            filtered_news.append({
                'title': title,
                'description': description,
                'link': news.get('link', ''),
                'pubDate': pub_date,
                'source': news.get('originallink', ''),
                'timestamp': datetime.now().isoformat()
            })
        
        return filtered_news
    
    def get_political_news(self) -> List[Dict]:
        """ì •ì¹˜ ê´€ë ¨ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        all_news = []
        
        # ë‹¨ë…, ì†ë³´ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
        for keyword in self.search_keywords:
            news_list = self.get_news_from_naver(keyword, display=50)
            all_news.extend(news_list)
        
        # í•„í„°ë§ ì ìš©
        filtered_news = self.filter_news(all_news)
        
        # ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬
        filtered_news.sort(key=lambda x: x['pubDate'], reverse=True)
        
        return filtered_news[:20]  # ìƒìœ„ 20ê°œë§Œ ë°˜í™˜
    
    def cleanup_old_news(self):
        """ì˜¤ë˜ëœ ë‰´ìŠ¤ ì •ë¦¬ (4ì‹œê°„ 15ë¶„ ì£¼ê¸°)"""
        current_time = datetime.now()
        if current_time - self.last_cleanup >= self.cleanup_interval:
            # ìºì‹œëœ ë‰´ìŠ¤ ì¤‘ ì˜¤ë˜ëœ ê²ƒë“¤ ì œê±°
            cutoff_time = current_time - timedelta(hours=4, minutes=15)
            
            # íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€ìœ¼ë¡œ ì˜¤ë˜ëœ ë‰´ìŠ¤ ì œê±°
            self.news_cache = {
                k: v for k, v in self.news_cache.items()
                if datetime.fromisoformat(v['timestamp']) > cutoff_time
            }
            
            self.last_cleanup = current_time
            print(f"ë‰´ìŠ¤ ì •ë¦¬ ì™„ë£Œ: {len(self.news_cache)}ê°œ ë‰´ìŠ¤ ë‚¨ìŒ")
    
    def get_cached_news(self) -> List[Dict]:
        """ìºì‹œëœ ë‰´ìŠ¤ ë°˜í™˜ (íƒ€ì´ë° ì¡°ì ˆ)"""
        self.cleanup_old_news()
        
        current_time = datetime.now()
        
        # ìºì‹œê°€ ë¹„ì–´ìˆê±°ë‚˜ ì§€ì •ëœ ì‹œê°„ì´ ì§€ë‚¬ìœ¼ë©´ ìƒˆë¡œ ìˆ˜ì§‘
        if not self.news_cache or (self.last_news_fetch is None or 
                                 current_time - self.last_news_fetch > self.news_fetch_interval):
            print(f"ğŸ“° ë‰´ìŠ¤ ìƒˆë¡œ ìˆ˜ì§‘ ì¤‘... (ë§ˆì§€ë§‰ ìˆ˜ì§‘: {self.last_news_fetch})")
            new_news = self.get_political_news()
            self.news_cache = {}
            for news in new_news:
                news_id = hashlib.md5(news['title'].encode()).hexdigest()
                self.news_cache[news_id] = news
            self.last_news_fetch = current_time
            print(f"âœ… ë‰´ìŠ¤ {len(new_news)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        else:
            print(f"ğŸ“‹ ìºì‹œëœ ë‰´ìŠ¤ ì‚¬ìš© ì¤‘... (ë‚¨ì€ ì‹œê°„: {self.news_fetch_interval - (current_time - self.last_news_fetch)})")
        
        return list(self.news_cache.values())
    
    def get_news_stats(self) -> Dict:
        """ë‰´ìŠ¤ í†µê³„ ì •ë³´"""
        return {
            'total_cached': len(self.news_cache),
            'last_cleanup': self.last_cleanup.isoformat(),
            'cache_size': len(self.news_cache)
        }
    

# í…ŒìŠ¤íŠ¸ìš©
if __name__ == "__main__":
    news_service = NewsService()
    news = news_service.get_political_news()
    
    print(f"ê°€ì ¸ì˜¨ ë‰´ìŠ¤ ìˆ˜: {len(news)}")
    for i, article in enumerate(news[:5], 1):
        print(f"{i}. {article['title']}")
        print(f"   ì¶œì²˜: {article['source']}")
        print(f"   ì‹œê°„: {article['pubDate']}")
        print()
