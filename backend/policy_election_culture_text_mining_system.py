#!/usr/bin/env python3
"""
ì •ì±…ì„ ê±°ë¬¸í™” í™•ì‚°ì„ ìœ„í•œ ì–¸ë¡ ê¸°ì‚¬ ë¹…ë°ì´í„° ë¶„ì„ ì‹œìŠ¤í…œ
ì§€ì—­ë³„ ë¯¸ìƒí† í”½ ë¶„ì„ ë° í…ìŠ¤íŠ¸ë§ˆì´ë‹ì„ í†µí•œ í† í”½-í•´ì„-ê³µì•½ ë°ì´í„°í™”

íŒŒì¼: 231215_ì •ì±…ì„ ê±°ë¬¸í™”_í™•ì‚°ì„_ìœ„í•œ_ì–¸ë¡ ê¸°ì‚¬_ë¹…ë°ì´í„°_ë¶„ì„.pdf
ëª©í‘œ: ê´‘ì—­ì—ì„œ ë™ë‹¨ìœ„ê¹Œì§€ ì§€ì—­ë³„ ê²°ê³¼ì°½ì— ë¯¸ìƒí† í”½ í‘œì‹œ
"""

import os
import json
import logging
import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
import re
import fitz  # PyMuPDF for PDF processing
from collections import defaultdict, Counter
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import matplotlib.font_manager as fm

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

# í…ìŠ¤íŠ¸ ë§ˆì´ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from konlpy.tag import Okt, Mecab, Komoran
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.cluster import KMeans
    from sklearn.decomposition import LatentDirichletAllocation
    from sklearn.manifold import TSNE
    import networkx as nx
    # textrankëŠ” ì„ íƒì  ë¼ì´ë¸ŒëŸ¬ë¦¬
    try:
        from textrank import TextRank
        TEXTRANK_AVAILABLE = True
    except ImportError:
        TEXTRANK_AVAILABLE = False
    NLP_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ í…ìŠ¤íŠ¸ë§ˆì´ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í•„ìš”: {e}")
    NLP_AVAILABLE = False
    TEXTRANK_AVAILABLE = False

logger = logging.getLogger(__name__)

class PolicyElectionCultureAnalyzer:
    """ì •ì±…ì„ ê±°ë¬¸í™” ë¹…ë°ì´í„° ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.base_dir = "/Users/hopidaay/newsbot-kr"
        self.backend_dir = "/Users/hopidaay/newsbot-kr/backend"
        self.pdf_file = "/Users/hopidaay/Desktop/231215_ì •ì±…ì„ ê±°ë¬¸í™”_í™•ì‚°ì„_ìœ„í•œ_ì–¸ë¡ ê¸°ì‚¬_ë¹…ë°ì´í„°_ë¶„ì„.pdf"
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        self.output_dir = os.path.join(self.backend_dir, "policy_election_culture_analysis")
        os.makedirs(self.output_dir, exist_ok=True)
        
        # í…ìŠ¤íŠ¸ ë§ˆì´ë‹ ë„êµ¬ ì´ˆê¸°í™”
        if NLP_AVAILABLE:
            self.okt = Okt()
            self.tfidf = TfidfVectorizer(max_features=1000, stop_words=None)
            self.lda_model = None
            self.kmeans_model = None
        
        # ì§€ì—­ ë§¤í•‘ ì •ë³´
        self.region_hierarchy = {
            'ê´‘ì—­ì‹œë„': ['ì„œìš¸', 'ë¶€ì‚°', 'ëŒ€êµ¬', 'ì¸ì²œ', 'ê´‘ì£¼', 'ëŒ€ì „', 'ìš¸ì‚°', 'ì„¸ì¢…', 'ê²½ê¸°', 'ê°•ì›', 'ì¶©ë¶', 'ì¶©ë‚¨', 'ì „ë¶', 'ì „ë‚¨', 'ê²½ë¶', 'ê²½ë‚¨', 'ì œì£¼'],
            'ì‹œêµ°êµ¬': [],  # ë™ì ìœ¼ë¡œ ë¡œë“œ
            'ìë©´ë™': []   # ë™ì ìœ¼ë¡œ ë¡œë“œ
        }
        
        # ë¯¸ìƒí† í”½ ì¹´í…Œê³ ë¦¬
        self.misaeng_topics = {
            'ê²½ì œì •ì±…': ['ì¼ìë¦¬', 'ì·¨ì—…', 'ì°½ì—…', 'ê²½ì œì„±ì¥', 'ì†Œë“', 'ì„ê¸ˆ'],
            'ì£¼ê±°ì •ì±…': ['ì£¼íƒ', 'ë¶€ë™ì‚°', 'ì„ëŒ€', 'ì „ì„¸', 'ì›”ì„¸', 'ì•„íŒŒíŠ¸'],
            'êµìœ¡ì •ì±…': ['êµìœ¡', 'í•™êµ', 'ëŒ€í•™', 'ì…ì‹œ', 'ì‚¬êµìœ¡', 'êµìœ¡ë¹„'],
            'ë³µì§€ì •ì±…': ['ë³µì§€', 'ì˜ë£Œ', 'ê±´ê°•ë³´í—˜', 'ì—°ê¸ˆ', 'ìœ¡ì•„', 'ë³´ìœ¡'],
            'í™˜ê²½ì •ì±…': ['í™˜ê²½', 'ê¸°í›„', 'ì—ë„ˆì§€', 'ë¯¸ì„¸ë¨¼ì§€', 'ì¬ìƒì—ë„ˆì§€'],
            'êµí†µì •ì±…': ['êµí†µ', 'ëŒ€ì¤‘êµí†µ', 'ì§€í•˜ì² ', 'ë²„ìŠ¤', 'ë„ë¡œ', 'ì£¼ì°¨'],
            'ë¬¸í™”ì •ì±…': ['ë¬¸í™”', 'ì˜ˆìˆ ', 'ì²´ìœ¡', 'ê´€ê´‘', 'ì¶•ì œ', 'ê³µì—°'],
            'ì•ˆì „ì •ì±…': ['ì•ˆì „', 'ì¹˜ì•ˆ', 'ì¬í•´', 'ë°©ë²”', 'ì†Œë°©', 'ì‘ê¸‰']
        }
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        self.analysis_results = {
            'document_info': {},
            'regional_topics': {},
            'topic_interpretation': {},
            'policy_promises': {},
            'text_mining_results': {},
            'visualization_data': {}
        }

    def extract_pdf_content(self) -> Dict[str, Any]:
        """PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ"""
        try:
            print("ğŸ“„ PDF íŒŒì¼ ë‚´ìš© ì¶”ì¶œ ì¤‘...")
            
            if not os.path.exists(self.pdf_file):
                raise FileNotFoundError(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.pdf_file}")
            
            # PDF ë¬¸ì„œ ì—´ê¸°
            doc = fitz.open(self.pdf_file)
            
            extracted_content = {
                'total_pages': len(doc),
                'pages': [],
                'full_text': '',
                'metadata': doc.metadata,
                'creation_date': datetime.now().isoformat()
            }
            
            print(f"ğŸ“Š ì´ {len(doc)} í˜ì´ì§€ PDF ë¬¸ì„œ ë¶„ì„ ì‹œì‘")
            
            # ê° í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            for page_num in range(len(doc)):
                page = doc[page_num]
                page_text = page.get_text()
                
                page_info = {
                    'page_number': page_num + 1,
                    'text': page_text,
                    'text_length': len(page_text),
                    'has_content': len(page_text.strip()) > 0
                }
                
                extracted_content['pages'].append(page_info)
                extracted_content['full_text'] += page_text + '\n'
                
                if page_num % 10 == 0:
                    print(f"  ğŸ“„ í˜ì´ì§€ {page_num + 1}/{len(doc)} ì²˜ë¦¬ ì™„ë£Œ")
            
            doc.close()
            
            # ë¬¸ì„œ ì •ë³´ ì €ì¥
            self.analysis_results['document_info'] = {
                'file_path': self.pdf_file,
                'total_pages': extracted_content['total_pages'],
                'total_text_length': len(extracted_content['full_text']),
                'metadata': extracted_content['metadata'],
                'extraction_date': extracted_content['creation_date']
            }
            
            print(f"âœ… PDF ì¶”ì¶œ ì™„ë£Œ: {len(extracted_content['full_text'])} ë¬¸ì")
            return extracted_content
            
        except Exception as e:
            logger.error(f"âŒ PDF ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def preprocess_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° í† í°í™”"""
        if not NLP_AVAILABLE:
            print("âš ï¸ í…ìŠ¤íŠ¸ë§ˆì´ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
            return []
        
        try:
            # í…ìŠ¤íŠ¸ ì •ì œ
            text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
            text = re.sub(r'\s+', ' ', text)
            text = text.strip()
            
            # í˜•íƒœì†Œ ë¶„ì„ ë° ëª…ì‚¬ ì¶”ì¶œ
            tokens = self.okt.nouns(text)
            
            # ë¶ˆìš©ì–´ ì œê±° ë° ê¸¸ì´ í•„í„°ë§
            stopwords = {'ê²ƒ', 'ë“±', 'ë°', 'ë˜í•œ', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë”°ë¼ì„œ', 'ì´ë¥¼', 'ì´ì—', 'ëŒ€í•œ', 'ìœ„í•œ', 'í†µí•´', 'ìˆë‹¤', 'ì—†ë‹¤', 'ëœë‹¤', 'í•œë‹¤', 'ì´ë‹¤'}
            filtered_tokens = [token for token in tokens if len(token) > 1 and token not in stopwords]
            
            return filtered_tokens
            
        except Exception as e:
            logger.error(f"âŒ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return []

    def extract_regional_topics(self, content: Dict[str, Any]) -> Dict[str, Any]:
        """ì§€ì—­ë³„ í† í”½ ì¶”ì¶œ"""
        try:
            print("ğŸ—ºï¸ ì§€ì—­ë³„ í† í”½ ì¶”ì¶œ ì¤‘...")
            
            full_text = content['full_text']
            regional_topics = defaultdict(list)
            
            # ì§€ì—­ëª… íŒ¨í„´ ì •ì˜
            region_patterns = {
                'ê´‘ì—­ì‹œë„': r'(ì„œìš¸|ë¶€ì‚°|ëŒ€êµ¬|ì¸ì²œ|ê´‘ì£¼|ëŒ€ì „|ìš¸ì‚°|ì„¸ì¢…|ê²½ê¸°|ê°•ì›|ì¶©ë¶|ì¶©ë‚¨|ì „ë¶|ì „ë‚¨|ê²½ë¶|ê²½ë‚¨|ì œì£¼)(?:íŠ¹ë³„ì‹œ|ê´‘ì—­ì‹œ|íŠ¹ë³„ìì¹˜ì‹œ|ë„|íŠ¹ë³„ìì¹˜ë„)?',
                'ì‹œêµ°êµ¬': r'(\w+)(?:ì‹œ|êµ°|êµ¬)',
                'ìë©´ë™': r'(\w+)(?:ì|ë©´|ë™)'
            }
            
            # ê° ì§€ì—­ ë ˆë²¨ë³„ í† í”½ ì¶”ì¶œ
            for level, pattern in region_patterns.items():
                matches = re.findall(pattern, full_text)
                region_counter = Counter(matches)
                
                print(f"  ğŸ“ {level}: {len(region_counter)}ê°œ ì§€ì—­ ë°œê²¬")
                
                for region, count in region_counter.most_common(50):  # ìƒìœ„ 50ê°œ ì§€ì—­
                    # í•´ë‹¹ ì§€ì—­ ê´€ë ¨ ë¬¸ì¥ ì¶”ì¶œ
                    region_sentences = []
                    for sentence in full_text.split('.'):
                        if region in sentence:
                            region_sentences.append(sentence.strip())
                    
                    if region_sentences:
                        # í† í”½ ë¶„ì„
                        region_text = ' '.join(region_sentences)
                        tokens = self.preprocess_text(region_text)
                        
                        if tokens:
                            # í† í”½ ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ë§¤ì¹­
                            topic_scores = {}
                            for topic, keywords in self.misaeng_topics.items():
                                score = sum(1 for token in tokens if any(keyword in token for keyword in keywords))
                                if score > 0:
                                    topic_scores[topic] = score
                            
                            regional_topics[level].append({
                                'region': region,
                                'mention_count': count,
                                'sentences': region_sentences[:5],  # ìƒìœ„ 5ê°œ ë¬¸ì¥
                                'tokens': tokens[:20],  # ìƒìœ„ 20ê°œ í† í°
                                'topic_scores': topic_scores,
                                'dominant_topic': max(topic_scores.items(), key=lambda x: x[1])[0] if topic_scores else 'ê¸°íƒ€'
                            })
            
            self.analysis_results['regional_topics'] = dict(regional_topics)
            
            print(f"âœ… ì§€ì—­ë³„ í† í”½ ì¶”ì¶œ ì™„ë£Œ")
            return dict(regional_topics)
            
        except Exception as e:
            logger.error(f"âŒ ì§€ì—­ë³„ í† í”½ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}

    def perform_topic_modeling(self, content: Dict[str, Any]) -> Dict[str, Any]:
        """í† í”½ ëª¨ë¸ë§ ìˆ˜í–‰"""
        if not NLP_AVAILABLE:
            print("âš ï¸ í† í”½ ëª¨ë¸ë§ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
            return {}
        
        try:
            print("ğŸ§  í† í”½ ëª¨ë¸ë§ ìˆ˜í–‰ ì¤‘...")
            
            # ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
            sentences = [s.strip() for s in content['full_text'].split('.') if len(s.strip()) > 20]
            print(f"  ğŸ“ ì´ {len(sentences)}ê°œ ë¬¸ì¥ ë¶„ì„")
            
            # ê° ë¬¸ì¥ì„ í† í°í™”
            tokenized_sentences = []
            for sentence in sentences[:1000]:  # ì²˜ë¦¬ ì†ë„ë¥¼ ìœ„í•´ ìƒìœ„ 1000ê°œ ë¬¸ì¥ë§Œ
                tokens = self.preprocess_text(sentence)
                if len(tokens) > 3:  # ìµœì†Œ 3ê°œ ì´ìƒì˜ í† í°ì´ ìˆëŠ” ë¬¸ì¥ë§Œ
                    tokenized_sentences.append(' '.join(tokens))
            
            if not tokenized_sentences:
                print("âš ï¸ í† í°í™”ëœ ë¬¸ì¥ì´ ì—†ìŒ")
                return {}
            
            print(f"  ğŸ”¤ {len(tokenized_sentences)}ê°œ ë¬¸ì¥ í† í°í™” ì™„ë£Œ")
            
            # TF-IDF ë²¡í„°í™”
            tfidf_matrix = self.tfidf.fit_transform(tokenized_sentences)
            feature_names = self.tfidf.get_feature_names_out()
            
            # LDA í† í”½ ëª¨ë¸ë§
            n_topics = 8  # ë¯¸ìƒí† í”½ ì¹´í…Œê³ ë¦¬ ìˆ˜
            self.lda_model = LatentDirichletAllocation(
                n_components=n_topics, 
                random_state=42, 
                max_iter=100
            )
            lda_result = self.lda_model.fit_transform(tfidf_matrix)
            
            # í† í”½ë³„ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
            topics_info = []
            for topic_idx, topic in enumerate(self.lda_model.components_):
                top_words_idx = topic.argsort()[-10:][::-1]  # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ
                top_words = [feature_names[i] for i in top_words_idx]
                top_weights = [topic[i] for i in top_words_idx]
                
                # í† í”½ ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜
                topic_category = self._classify_topic_category(top_words)
                
                topics_info.append({
                    'topic_id': topic_idx,
                    'category': topic_category,
                    'keywords': top_words,
                    'weights': top_weights.tolist(),
                    'keyword_weight_pairs': list(zip(top_words, top_weights.tolist()))
                })
            
            # K-means í´ëŸ¬ìŠ¤í„°ë§
            n_clusters = 5
            self.kmeans_model = KMeans(n_clusters=n_clusters, random_state=42)
            cluster_labels = self.kmeans_model.fit_predict(tfidf_matrix)
            
            # í´ëŸ¬ìŠ¤í„°ë³„ ëŒ€í‘œ í‚¤ì›Œë“œ
            clusters_info = []
            for cluster_id in range(n_clusters):
                cluster_sentences_idx = np.where(cluster_labels == cluster_id)[0]
                cluster_tfidf = tfidf_matrix[cluster_sentences_idx].mean(axis=0).A1
                top_features_idx = cluster_tfidf.argsort()[-10:][::-1]
                cluster_keywords = [feature_names[i] for i in top_features_idx]
                
                clusters_info.append({
                    'cluster_id': cluster_id,
                    'size': len(cluster_sentences_idx),
                    'keywords': cluster_keywords,
                    'representative_sentences': [sentences[i] for i in cluster_sentences_idx[:3]]
                })
            
            topic_modeling_results = {
                'lda_topics': topics_info,
                'kmeans_clusters': clusters_info,
                'total_sentences': len(sentences),
                'processed_sentences': len(tokenized_sentences),
                'vocabulary_size': len(feature_names),
                'modeling_date': datetime.now().isoformat()
            }
            
            self.analysis_results['text_mining_results'] = topic_modeling_results
            
            print(f"âœ… í† í”½ ëª¨ë¸ë§ ì™„ë£Œ: {n_topics}ê°œ í† í”½, {n_clusters}ê°œ í´ëŸ¬ìŠ¤í„°")
            return topic_modeling_results
            
        except Exception as e:
            logger.error(f"âŒ í† í”½ ëª¨ë¸ë§ ì‹¤íŒ¨: {e}")
            return {}

    def _classify_topic_category(self, keywords: List[str]) -> str:
        """í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í† í”½ ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜"""
        category_scores = {}
        
        for category, category_keywords in self.misaeng_topics.items():
            score = 0
            for keyword in keywords:
                for cat_keyword in category_keywords:
                    if cat_keyword in keyword or keyword in cat_keyword:
                        score += 1
            category_scores[category] = score
        
        if category_scores:
            return max(category_scores.items(), key=lambda x: x[1])[0]
        else:
            return 'ê¸°íƒ€ì •ì±…'

    def interpret_topics_and_extract_promises(self, regional_topics: Dict[str, Any], topic_modeling: Dict[str, Any]) -> Dict[str, Any]:
        """í† í”½ í•´ì„ ë° ê³µì•½ ì¶”ì¶œ"""
        try:
            print("ğŸ¯ í† í”½ í•´ì„ ë° ê³µì•½ ì¶”ì¶œ ì¤‘...")
            
            interpretations = {}
            policy_promises = {}
            
            # ì§€ì—­ë³„ í† í”½ í•´ì„
            for level, regions in regional_topics.items():
                interpretations[level] = []
                policy_promises[level] = []
                
                for region_info in regions:
                    region = region_info['region']
                    dominant_topic = region_info['dominant_topic']
                    topic_scores = region_info['topic_scores']
                    sentences = region_info['sentences']
                    
                    # í† í”½ í•´ì„ ìƒì„±
                    interpretation = self._generate_topic_interpretation(
                        region, dominant_topic, topic_scores, sentences
                    )
                    
                    # ê³µì•½ ì¶”ì¶œ
                    promises = self._extract_policy_promises(sentences, dominant_topic)
                    
                    interpretations[level].append({
                        'region': region,
                        'dominant_topic': dominant_topic,
                        'interpretation': interpretation,
                        'confidence_score': max(topic_scores.values()) if topic_scores else 0
                    })
                    
                    policy_promises[level].append({
                        'region': region,
                        'topic': dominant_topic,
                        'promises': promises,
                        'promise_count': len(promises)
                    })
            
            # LDA í† í”½ë³„ í•´ì„
            lda_interpretations = []
            if 'lda_topics' in topic_modeling:
                for topic_info in topic_modeling['lda_topics']:
                    topic_interpretation = self._generate_lda_topic_interpretation(topic_info)
                    lda_interpretations.append(topic_interpretation)
            
            results = {
                'regional_interpretations': interpretations,
                'regional_policy_promises': policy_promises,
                'lda_topic_interpretations': lda_interpretations,
                'interpretation_date': datetime.now().isoformat()
            }
            
            self.analysis_results['topic_interpretation'] = interpretations
            self.analysis_results['policy_promises'] = policy_promises
            
            print(f"âœ… í† í”½ í•´ì„ ë° ê³µì•½ ì¶”ì¶œ ì™„ë£Œ")
            return results
            
        except Exception as e:
            logger.error(f"âŒ í† í”½ í•´ì„ ë° ê³µì•½ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}

    def _generate_topic_interpretation(self, region: str, topic: str, scores: Dict[str, int], sentences: List[str]) -> str:
        """ì§€ì—­ë³„ í† í”½ í•´ì„ ìƒì„±"""
        try:
            # í† í”½ë³„ í•´ì„ í…œí”Œë¦¿
            topic_templates = {
                'ê²½ì œì •ì±…': f"{region} ì§€ì—­ì€ ì¼ìë¦¬ ì°½ì¶œê³¼ ê²½ì œ í™œì„±í™”ê°€ ì£¼ìš” ê´€ì‹¬ì‚¬ë¡œ ë‚˜íƒ€ë‚¨",
                'ì£¼ê±°ì •ì±…': f"{region} ì§€ì—­ì€ ì£¼íƒ ê³µê¸‰ê³¼ ë¶€ë™ì‚° ì•ˆì •í™”ê°€ í•µì‹¬ ì´ìŠˆì„",
                'êµìœ¡ì •ì±…': f"{region} ì§€ì—­ì€ êµìœ¡ í™˜ê²½ ê°œì„ ê³¼ ì‚¬êµìœ¡ ë¶€ë‹´ í•´ì†Œê°€ ì¤‘ìš”í•¨",
                'ë³µì§€ì •ì±…': f"{region} ì§€ì—­ì€ ì‚¬íšŒë³µì§€ í™•ì¶©ê³¼ ì˜ë£Œ ì„œë¹„ìŠ¤ ê°œì„ ì´ í•„ìš”í•¨",
                'í™˜ê²½ì •ì±…': f"{region} ì§€ì—­ì€ í™˜ê²½ ë³´í˜¸ì™€ ì§€ì†ê°€ëŠ¥í•œ ë°œì „ì´ ê´€ì‹¬ì‚¬ì„",
                'êµí†µì •ì±…': f"{region} ì§€ì—­ì€ êµí†µ ì¸í”„ë¼ ê°œì„ ê³¼ ëŒ€ì¤‘êµí†µ í™•ì¶©ì´ ì¤‘ìš”í•¨",
                'ë¬¸í™”ì •ì±…': f"{region} ì§€ì—­ì€ ë¬¸í™” ì‹œì„¤ í™•ì¶©ê³¼ ê´€ê´‘ ì‚°ì—… ë°œì „ì´ í•„ìš”í•¨",
                'ì•ˆì „ì •ì±…': f"{region} ì§€ì—­ì€ ì•ˆì „í•œ ìƒí™œ í™˜ê²½ ì¡°ì„±ì´ ìš°ì„  ê³¼ì œì„"
            }
            
            base_interpretation = topic_templates.get(topic, f"{region} ì§€ì—­ì˜ {topic} ê´€ë ¨ ì´ìŠˆê°€ ì£¼ëª©ë°›ê³  ìˆìŒ")
            
            # ì ìˆ˜ ê¸°ë°˜ ìƒì„¸ í•´ì„ ì¶”ê°€
            if scores:
                sorted_topics = sorted(scores.items(), key=lambda x: x[1], reverse=True)
                if len(sorted_topics) > 1:
                    secondary_topic = sorted_topics[1][0]
                    base_interpretation += f". ë˜í•œ {secondary_topic} ë¶„ì•¼ë„ í•¨ê»˜ ê´€ì‹¬ì„ ë°›ê³  ìˆìŒ"
            
            return base_interpretation
            
        except Exception as e:
            return f"{region} ì§€ì—­ì˜ {topic} ê´€ë ¨ ë¶„ì„ ê²°ê³¼"

    def _extract_policy_promises(self, sentences: List[str], topic: str) -> List[str]:
        """ë¬¸ì¥ì—ì„œ ì •ì±… ê³µì•½ ì¶”ì¶œ"""
        try:
            promises = []
            
            # ê³µì•½ ê´€ë ¨ í‚¤ì›Œë“œ íŒ¨í„´
            promise_patterns = [
                r'(\w+ì„/ë¥¼\s*(?:ì¶”ì§„|ì‹¤ì‹œ|ì‹œí–‰|ë„ì…|í™•ëŒ€|ê°•í™”|ê°œì„ ))',
                r'(\w+\s*(?:ì •ì±…|ì‚¬ì—…|í”„ë¡œê·¸ë¨|ì œë„)ì„/ë¥¼\s*(?:ë§ˆë ¨|ìˆ˜ë¦½|ì¶”ì§„))',
                r'(\w+ì„/ë¥¼\s*ìœ„í•œ\s*\w+)',
                r'(\w+\s*(?:ì§€ì›|í™•ì¶©|ì¡°ì„±|ê±´ì„¤|ì„¤ì¹˜))'
            ]
            
            for sentence in sentences:
                for pattern in promise_patterns:
                    matches = re.findall(pattern, sentence)
                    for match in matches:
                        if len(match) > 5 and match not in promises:
                            promises.append(match.strip())
            
            # í† í”½ë³„ íŠ¹í™” ê³µì•½ ì¶”ì¶œ
            topic_specific_promises = self._extract_topic_specific_promises(sentences, topic)
            promises.extend(topic_specific_promises)
            
            return list(set(promises))[:10]  # ì¤‘ë³µ ì œê±° ë° ìƒìœ„ 10ê°œ
            
        except Exception as e:
            return []

    def _extract_topic_specific_promises(self, sentences: List[str], topic: str) -> List[str]:
        """í† í”½ë³„ íŠ¹í™” ê³µì•½ ì¶”ì¶œ"""
        topic_keywords = self.misaeng_topics.get(topic, [])
        promises = []
        
        for sentence in sentences:
            for keyword in topic_keywords:
                if keyword in sentence:
                    # í•´ë‹¹ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ì—ì„œ ê³µì•½ì„± í‘œí˜„ ì¶”ì¶œ
                    if any(word in sentence for word in ['í™•ëŒ€', 'ê°•í™”', 'ê°œì„ ', 'ì§€ì›', 'ì¶”ì§„', 'ë„ì…']):
                        clean_sentence = sentence.strip()
                        if len(clean_sentence) > 10 and clean_sentence not in promises:
                            promises.append(clean_sentence)
        
        return promises

    def _generate_lda_topic_interpretation(self, topic_info: Dict[str, Any]) -> Dict[str, Any]:
        """LDA í† í”½ í•´ì„ ìƒì„±"""
        keywords = topic_info['keywords']
        category = topic_info['category']
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ í•´ì„ ìƒì„±
        main_keywords = keywords[:5]
        interpretation = f"{category} ë¶„ì•¼ì—ì„œ {', '.join(main_keywords)} ë“±ì´ ì£¼ìš” ê´€ì‹¬ì‚¬ë¡œ ë‚˜íƒ€ë‚¨"
        
        return {
            'topic_id': topic_info['topic_id'],
            'category': category,
            'main_keywords': main_keywords,
            'interpretation': interpretation,
            'all_keywords': keywords
        }

    def create_visualization_data(self) -> Dict[str, Any]:
        """ì‹œê°í™”ë¥¼ ìœ„í•œ ë°ì´í„° ìƒì„±"""
        try:
            print("ğŸ“Š ì‹œê°í™” ë°ì´í„° ìƒì„± ì¤‘...")
            
            viz_data = {
                'regional_topic_distribution': self._create_regional_topic_distribution(),
                'topic_keyword_networks': self._create_topic_keyword_networks(),
                'regional_hierarchy_data': self._create_regional_hierarchy_data(),
                'topic_timeline_data': self._create_topic_timeline_data(),
                'promise_category_data': self._create_promise_category_data()
            }
            
            self.analysis_results['visualization_data'] = viz_data
            
            print("âœ… ì‹œê°í™” ë°ì´í„° ìƒì„± ì™„ë£Œ")
            return viz_data
            
        except Exception as e:
            logger.error(f"âŒ ì‹œê°í™” ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")
            return {}

    def _create_regional_topic_distribution(self) -> Dict[str, Any]:
        """ì§€ì—­ë³„ í† í”½ ë¶„í¬ ë°ì´í„° ìƒì„±"""
        distribution_data = {}
        
        if 'regional_topics' in self.analysis_results:
            for level, regions in self.analysis_results['regional_topics'].items():
                topic_counts = defaultdict(int)
                for region_info in regions:
                    topic = region_info.get('dominant_topic', 'ê¸°íƒ€')
                    topic_counts[topic] += 1
                
                distribution_data[level] = dict(topic_counts)
        
        return distribution_data

    def _create_topic_keyword_networks(self) -> Dict[str, Any]:
        """í† í”½-í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ë°ì´í„° ìƒì„±"""
        network_data = {'nodes': [], 'edges': []}
        
        if 'text_mining_results' in self.analysis_results and 'lda_topics' in self.analysis_results['text_mining_results']:
            for topic_info in self.analysis_results['text_mining_results']['lda_topics']:
                topic_node = {
                    'id': f"topic_{topic_info['topic_id']}",
                    'label': topic_info['category'],
                    'type': 'topic',
                    'size': 20
                }
                network_data['nodes'].append(topic_node)
                
                for keyword, weight in topic_info['keyword_weight_pairs'][:5]:
                    keyword_node = {
                        'id': f"keyword_{keyword}",
                        'label': keyword,
                        'type': 'keyword',
                        'size': weight * 10
                    }
                    network_data['nodes'].append(keyword_node)
                    
                    edge = {
                        'source': topic_node['id'],
                        'target': keyword_node['id'],
                        'weight': weight
                    }
                    network_data['edges'].append(edge)
        
        return network_data

    def _create_regional_hierarchy_data(self) -> Dict[str, Any]:
        """ì§€ì—­ ê³„ì¸µ êµ¬ì¡° ë°ì´í„° ìƒì„±"""
        hierarchy_data = {
            'levels': ['ê´‘ì—­ì‹œë„', 'ì‹œêµ°êµ¬', 'ìë©´ë™'],
            'regions': {}
        }
        
        if 'regional_topics' in self.analysis_results:
            for level, regions in self.analysis_results['regional_topics'].items():
                hierarchy_data['regions'][level] = [
                    {
                        'name': region_info['region'],
                        'topic': region_info.get('dominant_topic', 'ê¸°íƒ€'),
                        'mention_count': region_info.get('mention_count', 0)
                    }
                    for region_info in regions
                ]
        
        return hierarchy_data

    def _create_topic_timeline_data(self) -> Dict[str, Any]:
        """í† í”½ ì‹œê°„ì„  ë°ì´í„° ìƒì„± (ë¬¸ì„œ ë‚´ ìœ„ì¹˜ ê¸°ë°˜)"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” PDFì˜ í˜ì´ì§€ë‚˜ ì„¹ì…˜ ì •ë³´ë¥¼ í™œìš©
        return {'timeline': 'PDF êµ¬ì¡° ë¶„ì„ í›„ êµ¬í˜„'}

    def _create_promise_category_data(self) -> Dict[str, Any]:
        """ê³µì•½ ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„° ìƒì„±"""
        category_data = defaultdict(int)
        
        if 'policy_promises' in self.analysis_results:
            for level, regions in self.analysis_results['policy_promises'].items():
                for region_info in regions:
                    topic = region_info.get('topic', 'ê¸°íƒ€')
                    promise_count = region_info.get('promise_count', 0)
                    category_data[topic] += promise_count
        
        return dict(category_data)

    def save_analysis_results(self) -> str:
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        try:
            print("ğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘...")
            
            # ê²°ê³¼ íŒŒì¼ ê²½ë¡œ
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            results_file = os.path.join(self.output_dir, f"policy_election_culture_analysis_{timestamp}.json")
            
            # JSONìœ¼ë¡œ ì €ì¥
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(self.analysis_results, f, ensure_ascii=False, indent=2)
            
            # ìš”ì•½ í†µê³„ ìƒì„±
            summary = self._generate_analysis_summary()
            summary_file = os.path.join(self.output_dir, f"analysis_summary_{timestamp}.json")
            
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
            print(f"  ğŸ“„ ìƒì„¸ ê²°ê³¼: {results_file}")
            print(f"  ğŸ“Š ìš”ì•½ í†µê³„: {summary_file}")
            
            return results_file
            
        except Exception as e:
            logger.error(f"âŒ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return None

    def _generate_analysis_summary(self) -> Dict[str, Any]:
        """ë¶„ì„ ìš”ì•½ í†µê³„ ìƒì„±"""
        summary = {
            'analysis_date': datetime.now().isoformat(),
            'document_info': self.analysis_results.get('document_info', {}),
            'regional_statistics': {},
            'topic_statistics': {},
            'promise_statistics': {},
            'overall_insights': []
        }
        
        # ì§€ì—­ë³„ í†µê³„
        if 'regional_topics' in self.analysis_results:
            regional_stats = {}
            for level, regions in self.analysis_results['regional_topics'].items():
                regional_stats[level] = {
                    'total_regions': len(regions),
                    'top_regions': [r['region'] for r in sorted(regions, key=lambda x: x.get('mention_count', 0), reverse=True)[:5]]
                }
            summary['regional_statistics'] = regional_stats
        
        # í† í”½ë³„ í†µê³„
        if 'text_mining_results' in self.analysis_results:
            topic_stats = {
                'total_topics': len(self.analysis_results['text_mining_results'].get('lda_topics', [])),
                'total_clusters': len(self.analysis_results['text_mining_results'].get('kmeans_clusters', [])),
                'vocabulary_size': self.analysis_results['text_mining_results'].get('vocabulary_size', 0)
            }
            summary['topic_statistics'] = topic_stats
        
        # ê³µì•½ë³„ í†µê³„
        if 'policy_promises' in self.analysis_results:
            promise_stats = {}
            total_promises = 0
            for level, regions in self.analysis_results['policy_promises'].items():
                level_promises = sum(r.get('promise_count', 0) for r in regions)
                promise_stats[level] = level_promises
                total_promises += level_promises
            
            promise_stats['total_promises'] = total_promises
            summary['promise_statistics'] = promise_stats
        
        return summary

    def run_complete_analysis(self) -> Dict[str, Any]:
        """ì „ì²´ ë¶„ì„ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("ğŸš€ ì •ì±…ì„ ê±°ë¬¸í™” ë¹…ë°ì´í„° ë¶„ì„ ì‹œì‘")
        print("=" * 80)
        
        try:
            # 1. PDF ë‚´ìš© ì¶”ì¶œ
            print("\nğŸ“„ 1ë‹¨ê³„: PDF ë‚´ìš© ì¶”ì¶œ")
            content = self.extract_pdf_content()
            if not content:
                raise Exception("PDF ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨")
            
            # 2. ì§€ì—­ë³„ í† í”½ ì¶”ì¶œ
            print("\nğŸ—ºï¸ 2ë‹¨ê³„: ì§€ì—­ë³„ í† í”½ ì¶”ì¶œ")
            regional_topics = self.extract_regional_topics(content)
            
            # 3. í† í”½ ëª¨ë¸ë§
            print("\nğŸ§  3ë‹¨ê³„: í† í”½ ëª¨ë¸ë§")
            topic_modeling = self.perform_topic_modeling(content)
            
            # 4. í† í”½ í•´ì„ ë° ê³µì•½ ì¶”ì¶œ
            print("\nğŸ¯ 4ë‹¨ê³„: í† í”½ í•´ì„ ë° ê³µì•½ ì¶”ì¶œ")
            interpretations = self.interpret_topics_and_extract_promises(regional_topics, topic_modeling)
            
            # 5. ì‹œê°í™” ë°ì´í„° ìƒì„±
            print("\nğŸ“Š 5ë‹¨ê³„: ì‹œê°í™” ë°ì´í„° ìƒì„±")
            viz_data = self.create_visualization_data()
            
            # 6. ê²°ê³¼ ì €ì¥
            print("\nğŸ’¾ 6ë‹¨ê³„: ê²°ê³¼ ì €ì¥")
            results_file = self.save_analysis_results()
            
            print("\nğŸ‰ ì •ì±…ì„ ê±°ë¬¸í™” ë¹…ë°ì´í„° ë¶„ì„ ì™„ë£Œ!")
            print("=" * 80)
            
            final_results = {
                'success': True,
                'results_file': results_file,
                'analysis_summary': self._generate_analysis_summary(),
                'completion_time': datetime.now().isoformat()
            }
            
            return final_results
            
        except Exception as e:
            print(f"\nâŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e),
                'completion_time': datetime.now().isoformat()
            }

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    analyzer = PolicyElectionCultureAnalyzer()
    results = analyzer.run_complete_analysis()
    
    if results['success']:
        print(f"\nâœ… ë¶„ì„ ì„±ê³µ!")
        print(f"ğŸ“„ ê²°ê³¼ íŒŒì¼: {results['results_file']}")
        print(f"ğŸ“Š ë¶„ì„ ìš”ì•½:")
        
        summary = results['analysis_summary']
        if 'regional_statistics' in summary:
            for level, stats in summary['regional_statistics'].items():
                print(f"  ğŸ—ºï¸ {level}: {stats['total_regions']}ê°œ ì§€ì—­")
        
        if 'topic_statistics' in summary:
            topic_stats = summary['topic_statistics']
            print(f"  ğŸ§  í† í”½: {topic_stats.get('total_topics', 0)}ê°œ")
            print(f"  ğŸ“š ì–´íœ˜: {topic_stats.get('vocabulary_size', 0)}ê°œ")
        
        if 'promise_statistics' in summary:
            promise_stats = summary['promise_statistics']
            print(f"  ğŸ¯ ê³µì•½: {promise_stats.get('total_promises', 0)}ê°œ")
        
    else:
        print(f"\nâŒ ë¶„ì„ ì‹¤íŒ¨: {results['error']}")

if __name__ == "__main__":
    main()
