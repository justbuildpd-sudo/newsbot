#!/usr/bin/env python3
"""
ë§¤ë‹ˆí˜ìŠ¤í†  í…ìŠ¤íŠ¸ë§ˆì´ë‹ ì‹œìŠ¤í…œ
8íšŒ ì§€ë°©ì„ ê±° ì¶œë§ˆì ê³µì•½ì„ ë¶„ì„í•˜ì—¬ ê²€ì¦ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ ì œê³µ
"""

import os
import json
import re
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from collections import Counter, defaultdict
import numpy as np
from pathlib import Path

# NLP ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from konlpy.tag import Okt, Mecab, Komoran
    import pandas as pd
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.cluster import KMeans
    from sklearn.decomposition import LatentDirichletAllocation
    from sklearn.metrics.pairwise import cosine_similarity
    import matplotlib.pyplot as plt
    import seaborn as sns
    plt.rcParams['font.family'] = ['AppleGothic', 'Malgun Gothic', 'DejaVu Sans']
    NLP_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ NLP ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í•„ìš”: {e}")
    NLP_AVAILABLE = False

logger = logging.getLogger(__name__)

@dataclass
class ManifestoAnalysis:
    """ë§¤ë‹ˆí˜ìŠ¤í†  ë¶„ì„ ê²°ê³¼"""
    candidate_name: str
    party: str
    region: str
    position_type: str
    total_pledges: int
    
    # í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼
    total_words: int
    unique_words: int
    avg_pledge_length: float
    
    # í‚¤ì›Œë“œ ë¶„ì„
    top_keywords: List[Tuple[str, int]]
    category_distribution: Dict[str, int]
    
    # ê°ì • ë¶„ì„
    sentiment_score: float
    confidence_level: str
    
    # êµ¬ì²´ì„± ë¶„ì„
    specificity_score: float
    budget_mentions: int
    timeline_mentions: int
    
    # ìœ ì‚¬ì„± ë¶„ì„
    similar_candidates: List[Tuple[str, float]]

class ManifestoTextMiningSystem:
    """ë§¤ë‹ˆí˜ìŠ¤í†  í…ìŠ¤íŠ¸ë§ˆì´ë‹ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.base_dir = "/Users/hopidaay/newsbot-kr"
        self.data_dir = "/Users/hopidaay/newsbot-kr/backend/election_data"
        self.analysis_dir = "/Users/hopidaay/newsbot-kr/backend/manifesto_analysis"
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        Path(self.analysis_dir).mkdir(parents=True, exist_ok=True)
        
        # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
        self.analyzer = None
        self._initialize_korean_analyzer()
        
        # ì •ì¹˜ ë„ë©”ì¸ íŠ¹í™” ì‚¬ì „
        self.political_keywords = {
            "ì •ì±…ë¶„ì•¼": ["ê²½ì œ", "ì£¼ê±°", "êµìœ¡", "ë³µì§€", "í™˜ê²½", "êµí†µ", "ë¬¸í™”", "ì•ˆì „", "ë³´ê±´", "ì¼ìë¦¬"],
            "ì‹¤í–‰ë™ì‚¬": ["ê³µê¸‰", "ì§€ì›", "í™•ëŒ€", "ê°•í™”", "ê°œì„ ", "ê±´ì„¤", "ì¡°ì„±", "ë„ì…", "ìš´ì˜", "ì œê³µ"],
            "ëŒ€ìƒ": ["ì‹œë¯¼", "ì²­ë…„", "ì–´ë¥´ì‹ ", "í•™ìƒ", "ì£¼ë¯¼", "ê°€ì •", "ê¸°ì—…", "ì†Œìƒê³µì¸"],
            "ê·œëª¨": ["ë§Œí˜¸", "ì–µì›", "ì¡°ì›", "ì²œëª…", "ë§Œëª…", "%", "ë°°", "ë…„", "ê°œì›”"],
            "ê°ì •": {
                "ê¸ì •": ["ë°œì „", "ì„±ì¥", "í–¥ìƒ", "ê°œì„ ", "í˜ì‹ ", "í™œì„±í™”", "ì¦ì§„", "í™•ì¶©"],
                "ë¶€ì •": ["í•´ê²°", "ì™„í™”", "ê²½ê°", "ë°©ì§€", "ê°œì„ ", "ê·¹ë³µ", "í•´ì†Œ"]
            }
        }
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥ì†Œ
        self.analysis_results = {}
        self.candidates_data = {}
        
    def _initialize_korean_analyzer(self):
        """í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        if not NLP_AVAILABLE:
            print("âš ï¸ NLP ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ - ê¸°ë³¸ ë¶„ì„ë§Œ ìˆ˜í–‰")
            return
        
        # Okt > Komoran > Mecab ìˆœìœ¼ë¡œ ì‹œë„
        analyzers = [
            ("Okt", Okt),
            ("Komoran", Komoran),
            ("Mecab", Mecab)
        ]
        
        for name, analyzer_class in analyzers:
            try:
                self.analyzer = analyzer_class()
                print(f"âœ… {name} í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì„±ê³µ")
                return
            except Exception as e:
                print(f"âš ï¸ {name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                continue
        
        print("âš ï¸ ëª¨ë“  í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨ - ê¸°ë³¸ í…ìŠ¤íŠ¸ ë¶„ì„ ì‚¬ìš©")
    
    def load_candidates_data(self, json_file_path: str) -> Dict[str, Any]:
        """ì¶œë§ˆì ë°ì´í„° ë¡œë“œ"""
        try:
            print("ğŸ“‚ ì¶œë§ˆì ë°ì´í„° ë¡œë“œ ì¤‘...")
            
            with open(json_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            self.candidates_data = data
            
            # í†µê³„ ì •ë³´
            total_candidates = data["statistics"]["total_candidates"]
            total_pledges = data["statistics"]["total_pledges"]
            
            print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
            print(f"  ğŸ‘¥ ì¶œë§ˆì: {total_candidates}ëª…")
            print(f"  ğŸ“‹ ê³µì•½: {total_pledges}ê°œ")
            
            return data
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}
    
    def extract_text_features(self, text: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ì§• ì¶”ì¶œ"""
        features = {
            "total_chars": len(text),
            "total_words": len(text.split()),
            "sentences": len(re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text)),
            "numbers": len(re.findall(r'\d+', text)),
            "budget_mentions": len(re.findall(r'\d+[ì¡°ì–µì²œë§Œ]*ì›', text)),
            "timeline_mentions": len(re.findall(r'\d+[ë…„ê°œì›”ì£¼ì¼]', text)),
            "percentage_mentions": len(re.findall(r'\d+%', text))
        }
        
        # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ (ê°€ëŠ¥í•œ ê²½ìš°)
        if self.analyzer:
            try:
                morphs = self.analyzer.morphs(text)
                pos_tags = self.analyzer.pos(text)
                
                features.update({
                    "morphs": morphs,
                    "unique_morphs": len(set(morphs)),
                    "pos_distribution": Counter([pos for word, pos in pos_tags]),
                    "nouns": [word for word, pos in pos_tags if pos.startswith('N')],
                    "verbs": [word for word, pos in pos_tags if pos.startswith('V')]
                })
            except Exception as e:
                logger.warning(f"í˜•íƒœì†Œ ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        return features
    
    def analyze_pledge_specificity(self, pledge: Dict[str, Any]) -> Dict[str, Any]:
        """ê³µì•½ì˜ êµ¬ì²´ì„± ë¶„ì„"""
        content = pledge.get("content", "")
        
        specificity_indicators = {
            "has_budget": bool(re.search(r'\d+[ì¡°ì–µì²œë§Œ]*ì›', content)),
            "has_timeline": bool(re.search(r'\d+[ë…„ê°œì›”ì£¼ì¼]', content)),
            "has_quantity": bool(re.search(r'\d+[ë§Œì²œë°±ì‹­]*[í˜¸ê°œëª…]', content)),
            "has_percentage": bool(re.search(r'\d+%', content)),
            "has_location": bool(pledge.get("target_region") and pledge["target_region"] != "ì „ì²´"),
            "has_method": any(keyword in content for keyword in self.political_keywords["ì‹¤í–‰ë™ì‚¬"])
        }
        
        # êµ¬ì²´ì„± ì ìˆ˜ ê³„ì‚° (0-100)
        specificity_score = sum(specificity_indicators.values()) / len(specificity_indicators) * 100
        
        # ì‹¤í˜„ê°€ëŠ¥ì„± í‰ê°€
        feasibility_factors = {
            "realistic_budget": self._assess_budget_realism(content),
            "reasonable_timeline": self._assess_timeline_realism(content),
            "clear_method": self._assess_method_clarity(content)
        }
        
        feasibility_score = sum(feasibility_factors.values()) / len(feasibility_factors) * 100
        
        return {
            "specificity_indicators": specificity_indicators,
            "specificity_score": specificity_score,
            "feasibility_factors": feasibility_factors,
            "feasibility_score": feasibility_score,
            "overall_quality": (specificity_score + feasibility_score) / 2
        }
    
    def _assess_budget_realism(self, content: str) -> float:
        """ì˜ˆì‚° í˜„ì‹¤ì„± í‰ê°€"""
        budget_matches = re.findall(r'(\d+)([ì¡°ì–µì²œë§Œ]*ì›)', content)
        if not budget_matches:
            return 0.5  # ì˜ˆì‚° ì–¸ê¸‰ ì—†ìŒ
        
        for amount_str, unit in budget_matches:
            try:
                amount = int(amount_str)
                if "ì¡°" in unit and amount > 100:  # 100ì¡°ì› ì´ˆê³¼ì‹œ ë¹„í˜„ì‹¤ì 
                    return 0.2
                elif "ì¡°" in unit and amount > 10:  # 10ì¡°ì› ì´ˆê³¼ì‹œ ì–´ë ¤ì›€
                    return 0.6
                elif "ì–µ" in unit and amount > 10000:  # 1ì¡°ì› ì´ˆê³¼ì‹œ ê²€í†  í•„ìš”
                    return 0.7
                else:
                    return 0.9  # í˜„ì‹¤ì  ë²”ìœ„
            except ValueError:
                continue
        
        return 0.8
    
    def _assess_timeline_realism(self, content: str) -> float:
        """ì¼ì • í˜„ì‹¤ì„± í‰ê°€"""
        timeline_matches = re.findall(r'(\d+)([ë…„ê°œì›”])', content)
        if not timeline_matches:
            return 0.5  # ì¼ì • ì–¸ê¸‰ ì—†ìŒ
        
        for period_str, unit in timeline_matches:
            try:
                period = int(period_str)
                if unit == "ë…„" and period <= 4:  # ì„ê¸° ë‚´
                    return 0.9
                elif unit == "ë…„" and period <= 8:  # 2ì„ê¸° ë‚´
                    return 0.7
                elif unit == "ê°œì›”" and period <= 48:  # 4ë…„ ë‚´
                    return 0.9
                else:
                    return 0.4  # ì¥ê¸°ê°„ ì†Œìš”
            except ValueError:
                continue
        
        return 0.8
    
    def _assess_method_clarity(self, content: str) -> float:
        """ì‹¤í–‰ë°©ë²• ëª…í™•ì„± í‰ê°€"""
        method_keywords = self.political_keywords["ì‹¤í–‰ë™ì‚¬"]
        method_count = sum(1 for keyword in method_keywords if keyword in content)
        
        if method_count == 0:
            return 0.3  # ì‹¤í–‰ë°©ë²• ë¶ˆëª…í™•
        elif method_count <= 2:
            return 0.7  # ì ì ˆí•œ ìˆ˜ì¤€
        else:
            return 0.9  # êµ¬ì²´ì  ë°©ë²• ì œì‹œ
    
    def analyze_candidate_manifesto(self, candidate_data: Dict[str, Any]) -> ManifestoAnalysis:
        """ê°œë³„ í›„ë³´ì ë§¤ë‹ˆí˜ìŠ¤í†  ì¢…í•© ë¶„ì„"""
        print(f"ğŸ” {candidate_data['name']} ë§¤ë‹ˆí˜ìŠ¤í†  ë¶„ì„ ì¤‘...")
        
        pledges = candidate_data.get("pledges", [])
        all_text = " ".join([pledge.get("content", "") for pledge in pledges])
        
        # ê¸°ë³¸ í…ìŠ¤íŠ¸ íŠ¹ì§• ì¶”ì¶œ
        text_features = self.extract_text_features(all_text)
        
        # ê³µì•½ë³„ êµ¬ì²´ì„± ë¶„ì„
        pledge_analyses = []
        total_specificity = 0
        total_feasibility = 0
        
        for pledge in pledges:
            analysis = self.analyze_pledge_specificity(pledge)
            pledge_analyses.append(analysis)
            total_specificity += analysis["specificity_score"]
            total_feasibility += analysis["feasibility_score"]
        
        avg_specificity = total_specificity / len(pledges) if pledges else 0
        avg_feasibility = total_feasibility / len(pledges) if pledges else 0
        
        # í‚¤ì›Œë“œ ë¶„ì„
        top_keywords = self._extract_top_keywords(all_text)
        
        # ì¹´í…Œê³ ë¦¬ ë¶„í¬
        category_dist = Counter([pledge.get("category", "ê¸°íƒ€") for pledge in pledges])
        
        # ê°ì • ë¶„ì„ (ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜)
        sentiment_score = self._analyze_sentiment(all_text)
        
        # ì‹ ë¢°ë„ í‰ê°€
        confidence_level = self._assess_confidence_level(avg_specificity, avg_feasibility)
        
        analysis = ManifestoAnalysis(
            candidate_name=candidate_data["name"],
            party=candidate_data["party"],
            region=candidate_data["region"],
            position_type=candidate_data["position_type"],
            total_pledges=len(pledges),
            
            total_words=text_features["total_words"],
            unique_words=text_features.get("unique_morphs", text_features["total_words"]),
            avg_pledge_length=text_features["total_words"] / len(pledges) if pledges else 0,
            
            top_keywords=top_keywords,
            category_distribution=dict(category_dist),
            
            sentiment_score=sentiment_score,
            confidence_level=confidence_level,
            
            specificity_score=avg_specificity,
            budget_mentions=text_features["budget_mentions"],
            timeline_mentions=text_features["timeline_mentions"],
            
            similar_candidates=[]  # ë‚˜ì¤‘ì— ê³„ì‚°
        )
        
        return analysis
    
    def _extract_top_keywords(self, text: str, top_k: int = 10) -> List[Tuple[str, int]]:
        """ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        if self.analyzer:
            try:
                nouns = [word for word, pos in self.analyzer.pos(text) if pos.startswith('N') and len(word) > 1]
                return Counter(nouns).most_common(top_k)
            except:
                pass
        
        # ê¸°ë³¸ ë‹¨ì–´ ë¶„ë¦¬
        words = [word for word in text.split() if len(word) > 1]
        return Counter(words).most_common(top_k)
    
    def _analyze_sentiment(self, text: str) -> float:
        """ê°ì • ë¶„ì„ (ê·œì¹™ ê¸°ë°˜)"""
        positive_words = self.political_keywords["ê°ì •"]["ê¸ì •"]
        negative_words = self.political_keywords["ê°ì •"]["ë¶€ì •"]
        
        positive_count = sum(1 for word in positive_words if word in text)
        negative_count = sum(1 for word in negative_words if word in text)
        
        total_count = positive_count + negative_count
        if total_count == 0:
            return 0.5  # ì¤‘ë¦½
        
        return positive_count / total_count
    
    def _assess_confidence_level(self, specificity: float, feasibility: float) -> str:
        """ì‹ ë¢°ë„ ìˆ˜ì¤€ í‰ê°€"""
        avg_score = (specificity + feasibility) / 2
        
        if avg_score >= 80:
            return "ë§¤ìš° ë†’ìŒ"
        elif avg_score >= 60:
            return "ë†’ìŒ"
        elif avg_score >= 40:
            return "ë³´í†µ"
        elif avg_score >= 20:
            return "ë‚®ìŒ"
        else:
            return "ë§¤ìš° ë‚®ìŒ"
    
    def calculate_similarity_matrix(self, analyses: List[ManifestoAnalysis]) -> np.ndarray:
        """í›„ë³´ì ê°„ ìœ ì‚¬ì„± ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°"""
        if not NLP_AVAILABLE:
            print("âš ï¸ scikit-learn ì—†ìŒ - ìœ ì‚¬ì„± ë¶„ì„ ê±´ë„ˆëœ€")
            return np.eye(len(analyses))
        
        try:
            # ê° í›„ë³´ìì˜ í‚¤ì›Œë“œë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            texts = []
            for analysis in analyses:
                keywords_text = " ".join([kw for kw, count in analysis.top_keywords])
                texts.append(keywords_text)
            
            # TF-IDF ë²¡í„°í™”
            vectorizer = TfidfVectorizer(max_features=100)
            tfidf_matrix = vectorizer.fit_transform(texts)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarity_matrix = cosine_similarity(tfidf_matrix)
            
            return similarity_matrix
            
        except Exception as e:
            logger.warning(f"ìœ ì‚¬ì„± ê³„ì‚° ì‹¤íŒ¨: {e}")
            return np.eye(len(analyses))
    
    def generate_comprehensive_report(self, analyses: List[ManifestoAnalysis]) -> Dict[str, Any]:
        """ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        print("ğŸ“Š ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        # ì „ì²´ í†µê³„
        total_candidates = len(analyses)
        total_pledges = sum(a.total_pledges for a in analyses)
        
        # ë‹¹ë³„ ë¶„ì„
        party_stats = defaultdict(list)
        for analysis in analyses:
            party_stats[analysis.party].append(analysis)
        
        # ì§€ì—­ë³„ ë¶„ì„
        region_stats = defaultdict(list)
        for analysis in analyses:
            region_stats[analysis.region].append(analysis)
        
        # ì§ì±…ë³„ ë¶„ì„
        position_stats = defaultdict(list)
        for analysis in analyses:
            position_stats[analysis.position_type].append(analysis)
        
        # í’ˆì§ˆ ë¶„ì„
        quality_distribution = {
            "ë§¤ìš° ë†’ìŒ": len([a for a in analyses if a.confidence_level == "ë§¤ìš° ë†’ìŒ"]),
            "ë†’ìŒ": len([a for a in analyses if a.confidence_level == "ë†’ìŒ"]),
            "ë³´í†µ": len([a for a in analyses if a.confidence_level == "ë³´í†µ"]),
            "ë‚®ìŒ": len([a for a in analyses if a.confidence_level == "ë‚®ìŒ"]),
            "ë§¤ìš° ë‚®ìŒ": len([a for a in analyses if a.confidence_level == "ë§¤ìš° ë‚®ìŒ"])
        }
        
        # ì¹´í…Œê³ ë¦¬ë³„ ê³µì•½ ë¶„í¬
        all_categories = defaultdict(int)
        for analysis in analyses:
            for category, count in analysis.category_distribution.items():
                all_categories[category] += count
        
        report = {
            "analysis_date": datetime.now().isoformat(),
            "summary": {
                "total_candidates": total_candidates,
                "total_pledges": total_pledges,
                "avg_pledges_per_candidate": total_pledges / total_candidates if total_candidates > 0 else 0,
                "avg_specificity_score": np.mean([a.specificity_score for a in analyses]),
                "avg_words_per_candidate": np.mean([a.total_words for a in analyses])
            },
            "quality_distribution": quality_distribution,
            "category_distribution": dict(all_categories),
            "party_analysis": {
                party: {
                    "candidates": len(candidates),
                    "avg_specificity": np.mean([c.specificity_score for c in candidates]),
                    "avg_pledges": np.mean([c.total_pledges for c in candidates]),
                    "confidence_levels": Counter([c.confidence_level for c in candidates])
                }
                for party, candidates in party_stats.items()
            },
            "region_analysis": {
                region: {
                    "candidates": len(candidates),
                    "avg_specificity": np.mean([c.specificity_score for c in candidates]),
                    "dominant_categories": Counter([
                        cat for c in candidates 
                        for cat in c.category_distribution.keys()
                    ]).most_common(3)
                }
                for region, candidates in region_stats.items()
            },
            "position_analysis": {
                position: {
                    "candidates": len(candidates),
                    "avg_specificity": np.mean([c.specificity_score for c in candidates]),
                    "avg_budget_mentions": np.mean([c.budget_mentions for c in candidates])
                }
                for position, candidates in position_stats.items()
            },
            "top_performers": {
                "highest_specificity": sorted(analyses, key=lambda x: x.specificity_score, reverse=True)[:3],
                "most_comprehensive": sorted(analyses, key=lambda x: x.total_pledges, reverse=True)[:3],
                "most_detailed": sorted(analyses, key=lambda x: x.total_words, reverse=True)[:3]
            }
        }
        
        return report
    
    def save_analysis_results(self, analyses: List[ManifestoAnalysis], report: Dict[str, Any]) -> Tuple[str, str]:
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ê°œë³„ ë¶„ì„ ê²°ê³¼
        analyses_file = os.path.join(self.analysis_dir, f"manifesto_analyses_{timestamp}.json")
        analyses_data = [
            {
                "candidate_name": a.candidate_name,
                "party": a.party,
                "region": a.region,
                "position_type": a.position_type,
                "total_pledges": a.total_pledges,
                "total_words": a.total_words,
                "unique_words": a.unique_words,
                "avg_pledge_length": a.avg_pledge_length,
                "top_keywords": a.top_keywords,
                "category_distribution": a.category_distribution,
                "sentiment_score": a.sentiment_score,
                "confidence_level": a.confidence_level,
                "specificity_score": a.specificity_score,
                "budget_mentions": a.budget_mentions,
                "timeline_mentions": a.timeline_mentions
            }
            for a in analyses
        ]
        
        with open(analyses_file, 'w', encoding='utf-8') as f:
            json.dump(analyses_data, f, ensure_ascii=False, indent=2)
        
        # ì¢…í•© ë³´ê³ ì„œ
        report_file = os.path.join(self.analysis_dir, f"comprehensive_report_{timestamp}.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"ğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
        print(f"  ğŸ“Š ê°œë³„ ë¶„ì„: {analyses_file}")
        print(f"  ğŸ“‹ ì¢…í•© ë³´ê³ ì„œ: {report_file}")
        
        return analyses_file, report_file

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ” ë§¤ë‹ˆí˜ìŠ¤í†  í…ìŠ¤íŠ¸ë§ˆì´ë‹ ì‹œìŠ¤í…œ")
    print("=" * 60)
    
    mining_system = ManifestoTextMiningSystem()
    
    try:
        # 1. ì¶œë§ˆì ë°ì´í„° ë¡œë“œ
        data_file = "/Users/hopidaay/newsbot-kr/backend/election_data/comprehensive_8th_election_candidates_20250920_124544.json"
        candidates_data = mining_system.load_candidates_data(data_file)
        
        if not candidates_data:
            print("âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
            return
        
        # 2. ê°œë³„ í›„ë³´ì ë¶„ì„
        print("\nğŸ” ê°œë³„ í›„ë³´ì ë§¤ë‹ˆí˜ìŠ¤í†  ë¶„ì„ ì¤‘...")
        analyses = []
        
        for district_id, district_info in candidates_data["districts"].items():
            for candidate_id, candidate_info in district_info["candidates"].items():
                analysis = mining_system.analyze_candidate_manifesto(candidate_info)
                analyses.append(analysis)
        
        # 3. ìœ ì‚¬ì„± ë¶„ì„
        print("\nğŸ“Š í›„ë³´ì ê°„ ìœ ì‚¬ì„± ë¶„ì„ ì¤‘...")
        similarity_matrix = mining_system.calculate_similarity_matrix(analyses)
        
        # 4. ì¢…í•© ë³´ê³ ì„œ ìƒì„±
        report = mining_system.generate_comprehensive_report(analyses)
        
        # 5. ê²°ê³¼ ì €ì¥
        analyses_file, report_file = mining_system.save_analysis_results(analyses, report)
        
        print("\n" + "=" * 60)
        print("ğŸ‰ ë§¤ë‹ˆí˜ìŠ¤í†  í…ìŠ¤íŠ¸ë§ˆì´ë‹ ì™„ë£Œ!")
        
        # ì£¼ìš” ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“Š ë¶„ì„ ê²°ê³¼ ìš”ì•½:")
        print(f"  ë¶„ì„ í›„ë³´ì: {report['summary']['total_candidates']}ëª…")
        print(f"  ì´ ê³µì•½: {report['summary']['total_pledges']}ê°œ")
        print(f"  í‰ê·  êµ¬ì²´ì„±: {report['summary']['avg_specificity_score']:.1f}ì ")
        
        print(f"\nğŸ† í’ˆì§ˆ ë¶„í¬:")
        for level, count in report['quality_distribution'].items():
            print(f"  {level}: {count}ëª…")
        
        print(f"\nğŸ“‹ ì£¼ìš” ê³µì•½ ë¶„ì•¼:")
        for category, count in sorted(report['category_distribution'].items(), key=lambda x: x[1], reverse=True)[:5]:
            print(f"  {category}: {count}ê°œ")
        
        return analyses, report
        
    except Exception as e:
        logger.error(f"í…ìŠ¤íŠ¸ë§ˆì´ë‹ ì‹¤íŒ¨: {e}")
        print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return None, None

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    main()
