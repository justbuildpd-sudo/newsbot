#!/usr/bin/env python3
"""
ì¶œë§ˆì ì „ì› ì´ë¦„ê²€ìƒ‰ ìºì‹± ìš©ëŸ‰ ì˜ˆì¸¡ê¸°
ëª¨ë“  ì¶œë§ˆìì˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì‚¬ì „ ìºì‹±í•  ê²½ìš°ì˜ ìš©ëŸ‰ ê³„ì‚°
"""

import os
import json
import logging
import math
from datetime import datetime
from typing import Dict, List, Optional, Any

logger = logging.getLogger(__name__)

class CandidateCacheCapacityPredictor:
    def __init__(self):
        self.base_dir = "/Users/hopidaay/newsbot-kr"
        self.backend_dir = "/Users/hopidaay/newsbot-kr/backend"
        
        # ì„ ê±° ì§ê¸‰ë³„ ì˜ˆìƒ ì¶œë§ˆì ìˆ˜
        self.candidate_estimates = {
            'êµ­íšŒì˜ì›': {
                'constituencies': 300,  # ì§€ì—­êµ¬ 300ê°œ
                'proportional': 0,      # ë¹„ë¡€ëŒ€í‘œëŠ” ê°œë³„ ìºì‹± ë¶ˆí•„ìš”
                'candidates_per_constituency': 4.5,  # í‰ê·  4.5ëª… ì¶œë§ˆ
                'total_candidates': 1350,  # 300 * 4.5
                'analysis_dimensions': 19,
                'analysis_scope': 'electoral_district_level'
            },
            'ê´‘ì—­ë‹¨ì²´ì¥': {
                'constituencies': 17,   # 17ê°œ ì‹œë„
                'candidates_per_constituency': 3.2,  # í‰ê·  3.2ëª… ì¶œë§ˆ
                'total_candidates': 54,  # 17 * 3.2 (ë°˜ì˜¬ë¦¼)
                'analysis_dimensions': 19,
                'analysis_scope': 'sido_level'
            },
            'ê¸°ì´ˆë‹¨ì²´ì¥': {
                'constituencies': 245,  # 245ê°œ ì‹œêµ°êµ¬
                'candidates_per_constituency': 2.8,  # í‰ê·  2.8ëª… ì¶œë§ˆ
                'total_candidates': 686,  # 245 * 2.8 (ë°˜ì˜¬ë¦¼)
                'analysis_dimensions': 18,
                'analysis_scope': 'sigungu_level'
            },
            'ê´‘ì—­ì˜ì›': {
                'constituencies': 824,  # 17ê°œ ì‹œë„ì˜íšŒ ì´ ì„ ê±°êµ¬
                'candidates_per_constituency': 2.6,  # í‰ê·  2.6ëª… ì¶œë§ˆ
                'total_candidates': 2142,  # 824 * 2.6 (ë°˜ì˜¬ë¦¼)
                'analysis_dimensions': 7,
                'analysis_scope': 'electoral_district_level'
            },
            'ê¸°ì´ˆì˜ì›': {
                'constituencies': 2898,  # 245ê°œ ì‹œêµ°êµ¬ì˜íšŒ ì´ ì„ ê±°êµ¬
                'candidates_per_constituency': 2.3,  # í‰ê·  2.3ëª… ì¶œë§ˆ
                'total_candidates': 6665,  # 2898 * 2.3 (ë°˜ì˜¬ë¦¼)
                'analysis_dimensions': 7,
                'analysis_scope': 'dong_level'
            },
            'êµìœ¡ê°': {
                'constituencies': 17,   # 17ê°œ êµìœ¡ì²­
                'candidates_per_constituency': 2.1,  # í‰ê·  2.1ëª… ì¶œë§ˆ
                'total_candidates': 36,   # 17 * 2.1 (ë°˜ì˜¬ë¦¼)
                'analysis_dimensions': 5,
                'analysis_scope': 'sido_level'
            }
        }
        
        # 96.19% ë‹¤ì–‘ì„± ì‹œìŠ¤í…œ ì°¨ì›ë³„ ë°ì´í„° í¬ê¸° (KB)
        self.dimension_data_sizes = {
            'ì¸êµ¬': 45,      # ì¸êµ¬ í†µê³„ ë°ì´í„°
            'ê°€êµ¬': 38,      # ê°€êµ¬ êµ¬ì„± ë°ì´í„°
            'ì£¼íƒ': 42,      # ì£¼íƒ í†µê³„
            'ì‚¬ì—…ì²´': 52,    # ì‚¬ì—…ì²´ í˜„í™©
            'ë†ê°€': 28,      # ë†ê°€ í†µê³„
            'ì–´ê°€': 25,      # ì–´ê°€ í†µê³„
            'ìƒí™œì—…ì¢…': 35,  # ìƒí™œì—…ì¢… ë¶„ì„
            'ë³µì§€ë¬¸í™”': 48,  # ë³µì§€ë¬¸í™” ì‹œì„¤
            'ë…¸ë™ê²½ì œ': 55,  # ë…¸ë™ê²½ì œ ì§€í‘œ
            'ì¢…êµ': 22,      # ì¢…êµ ë¹„ìœ¨
            'ì‚¬íšŒ': 31,      # ì‚¬íšŒ ì§€í‘œ
            'êµí†µ': 46,      # êµí†µ ì ‘ê·¼ì„±
            'ë„ì‹œí™”': 39,    # ë„ì‹œí™” ë¶„ì„
            'êµìœ¡': 44,      # êµìœ¡ ì‹œì„¤
            'ì˜ë£Œ': 41,      # ì˜ë£Œ ì‹œì„¤
            'ì•ˆì „': 33,      # ì•ˆì „ ì‹œì„¤
            'ë‹¤ë¬¸í™”': 27,    # ë‹¤ë¬¸í™” ê°€ì •
            'ì¬ì •': 36,      # ì¬ì • ìë¦½ë„
            'ì‚°ì—…': 49       # ì‚°ì—… ë‹¨ì§€
        }
        
        # ê¸°ë³¸ ì •ë³´ ë° ë©”íƒ€ë°ì´í„° í¬ê¸° (KB)
        self.base_info_sizes = {
            'basic_profile': 8,           # ê¸°ë³¸ í”„ë¡œí•„
            'electoral_history': 15,      # ì„ ê±° ì´ë ¥
            'performance_metrics': 25,    # ì„±ê³¼ ì§€í‘œ
            'comparative_analysis': 32,   # ë¹„êµ ë¶„ì„
            'jurisdictional_data': 28,    # ê´€í•  ì§€ì—­ ì •ë³´
            'policy_impact': 22,          # ì •ì±… ì˜í–¥
            'future_forecast': 18,        # ë¯¸ë˜ ì „ë§
            'metadata': 5                 # ë©”íƒ€ë°ì´í„°
        }

    def calculate_single_candidate_size(self, position: str) -> float:
        """ë‹¨ì¼ ì¶œë§ˆì ìºì‹œ í¬ê¸° ê³„ì‚° (KB)"""
        
        if position not in self.candidate_estimates:
            return 0
        
        position_info = self.candidate_estimates[position]
        analysis_dimensions = position_info['analysis_dimensions']
        
        # ê¸°ë³¸ ì •ë³´ í¬ê¸°
        base_size = sum(self.base_info_sizes.values())
        
        # ì°¨ì›ë³„ ë°ì´í„° í¬ê¸°
        dimension_size = 0
        dimension_list = list(self.dimension_data_sizes.keys())[:analysis_dimensions]
        
        for dimension in dimension_list:
            dimension_size += self.dimension_data_sizes[dimension]
        
        # ë¶„ì„ ë²”ìœ„ì— ë”°ë¥¸ ì¶”ê°€ ë°ì´í„°
        scope_multiplier = {
            'dong_level': 1.2,           # ë™ ë‹¨ìœ„ ì„¸ë°€ ë¶„ì„
            'sigungu_level': 1.5,        # ì‹œêµ°êµ¬ ë‹¨ìœ„
            'electoral_district_level': 1.3,  # ì„ ê±°êµ¬ ë‹¨ìœ„
            'sido_level': 1.8            # ì‹œë„ ë‹¨ìœ„ ê´‘ë²”ìœ„ ë¶„ì„
        }
        
        analysis_scope = position_info['analysis_scope']
        multiplier = scope_multiplier.get(analysis_scope, 1.0)
        
        # ì´ í¬ê¸° ê³„ì‚°
        total_size = (base_size + dimension_size) * multiplier
        
        return total_size

    def predict_total_cache_capacity(self) -> Dict[str, Any]:
        """ì „ì²´ ìºì‹œ ìš©ëŸ‰ ì˜ˆì¸¡"""
        
        print("ğŸ“Š ì¶œë§ˆì ì „ì› ìºì‹œ ìš©ëŸ‰ ì˜ˆì¸¡ ì‹œì‘...")
        
        cache_prediction = {
            'prediction_timestamp': datetime.now().isoformat(),
            'position_analysis': {},
            'total_summary': {
                'total_candidates': 0,
                'total_size_kb': 0,
                'total_size_mb': 0,
                'within_300mb_limit': False
            },
            'optimization_recommendations': []
        }
        
        total_candidates = 0
        total_size_kb = 0
        
        # ì§ê¸‰ë³„ ìš©ëŸ‰ ê³„ì‚°
        for position, info in self.candidate_estimates.items():
            print(f"  ğŸ” {position} ë¶„ì„...")
            
            # ë‹¨ì¼ ì¶œë§ˆì í¬ê¸°
            single_size_kb = self.calculate_single_candidate_size(position)
            
            # ì´ ì¶œë§ˆì ìˆ˜
            candidates_count = info['total_candidates']
            
            # ì§ê¸‰ë³„ ì´ í¬ê¸°
            position_total_kb = single_size_kb * candidates_count
            position_total_mb = position_total_kb / 1024
            
            cache_prediction['position_analysis'][position] = {
                'candidates_count': candidates_count,
                'single_candidate_size_kb': round(single_size_kb, 2),
                'total_size_kb': round(position_total_kb, 2),
                'total_size_mb': round(position_total_mb, 2),
                'analysis_dimensions': info['analysis_dimensions'],
                'analysis_scope': info['analysis_scope'],
                'constituencies': info['constituencies'],
                'avg_candidates_per_constituency': info['candidates_per_constituency']
            }
            
            total_candidates += candidates_count
            total_size_kb += position_total_kb
            
            print(f"    âœ… {candidates_count}ëª…, {position_total_mb:.1f}MB")
        
        # ì´í•© ê³„ì‚°
        total_size_mb = total_size_kb / 1024
        within_limit = total_size_mb <= 300
        
        cache_prediction['total_summary'] = {
            'total_candidates': total_candidates,
            'total_size_kb': round(total_size_kb, 2),
            'total_size_mb': round(total_size_mb, 2),
            'within_300mb_limit': within_limit,
            'utilization_percentage': round((total_size_mb / 300) * 100, 2)
        }
        
        # ìµœì í™” ê¶Œì¥ì‚¬í•­
        cache_prediction['optimization_recommendations'] = self._generate_optimization_recommendations(
            cache_prediction, within_limit
        )
        
        return cache_prediction

    def _generate_optimization_recommendations(self, prediction: Dict, within_limit: bool) -> List[str]:
        """ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        total_mb = prediction['total_summary']['total_size_mb']
        
        if within_limit:
            recommendations.extend([
                f"âœ… ì´ {total_mb:.1f}MBë¡œ 300MB í•œê³„ ë‚´ ìºì‹± ê°€ëŠ¥",
                "ğŸš€ ì „ì²´ ì¶œë§ˆì ì‚¬ì „ ìºì‹± ì§„í–‰ ê¶Œì¥",
                "ğŸ“Š ì‹¤ì‹œê°„ ê²€ìƒ‰ ì„±ëŠ¥ ëŒ€í­ í–¥ìƒ ê¸°ëŒ€",
                "ğŸ’¾ ë©”ëª¨ë¦¬ ê¸°ë°˜ ê³ ì† ìºì‹œ êµ¬í˜„ ê°€ëŠ¥"
            ])
        else:
            excess_mb = total_mb - 300
            recommendations.extend([
                f"âš ï¸ ì´ {total_mb:.1f}MBë¡œ 300MB í•œê³„ ì´ˆê³¼ ({excess_mb:.1f}MB)",
                "ğŸ”„ ì„ ë³„ì  ìºì‹± ì „ëµ í•„ìš”",
                "ğŸ“Š ì£¼ìš” ì¶œë§ˆì ìš°ì„  ìºì‹±",
                "ğŸ’¿ ë””ìŠ¤í¬ ê¸°ë°˜ ìºì‹œ ê³ ë ¤"
            ])
        
        # ì§ê¸‰ë³„ ìµœì í™” ì œì•ˆ
        position_analysis = prediction['position_analysis']
        largest_position = max(position_analysis.keys(), 
                             key=lambda x: position_analysis[x]['total_size_mb'])
        
        recommendations.append(f"ğŸ“ˆ ìµœëŒ€ ìš©ëŸ‰: {largest_position} ({position_analysis[largest_position]['total_size_mb']:.1f}MB)")
        
        # ì••ì¶• ë° ìµœì í™” ì œì•ˆ
        if total_mb > 200:
            recommendations.extend([
                "ğŸ—œï¸ JSON ì••ì¶• (gzip) ì ìš© ì‹œ 30-40% ìš©ëŸ‰ ì ˆì•½",
                "âš¡ ì§€ì—° ë¡œë”©ìœ¼ë¡œ í•„ìš” ì‹œì  ë°ì´í„° ë¡œë“œ",
                "ğŸ”„ ìºì‹œ ë§Œë£Œ ì •ì±…ìœ¼ë¡œ ë©”ëª¨ë¦¬ ê´€ë¦¬"
            ])
        
        return recommendations

    def analyze_cache_efficiency(self, prediction: Dict) -> Dict[str, Any]:
        """ìºì‹œ íš¨ìœ¨ì„± ë¶„ì„"""
        
        efficiency_analysis = {
            'performance_impact': {
                'search_speed_improvement': '90-95%',  # ê²€ìƒ‰ ì†ë„ ê°œì„ 
                'server_load_reduction': '80-85%',     # ì„œë²„ ë¶€í•˜ ê°ì†Œ
                'user_experience_enhancement': 'EXCELLENT',  # ì‚¬ìš©ì ê²½í—˜
                'concurrent_users_support': '1000+'    # ë™ì‹œ ì‚¬ìš©ì ì§€ì›
            },
            'cost_benefit': {
                'development_cost': 'MEDIUM',           # ê°œë°œ ë¹„ìš©
                'maintenance_cost': 'LOW',              # ìœ ì§€ë³´ìˆ˜ ë¹„ìš©
                'infrastructure_cost': 'LOW',          # ì¸í”„ë¼ ë¹„ìš©
                'roi_timeline': '2-3 months'           # ROI ë‹¬ì„± ê¸°ê°„
            },
            'technical_feasibility': {
                'implementation_complexity': 'MEDIUM', # êµ¬í˜„ ë³µì¡ë„
                'memory_requirements': f"{prediction['total_summary']['total_size_mb']:.0f}MB",
                'update_frequency': 'DAILY',           # ì—…ë°ì´íŠ¸ ì£¼ê¸°
                'cache_invalidation': 'EVENT_DRIVEN'   # ìºì‹œ ë¬´íš¨í™”
            }
        }
        
        return efficiency_analysis

    def generate_implementation_strategy(self, prediction: Dict) -> Dict[str, Any]:
        """êµ¬í˜„ ì „ëµ ìƒì„±"""
        
        total_mb = prediction['total_summary']['total_size_mb']
        within_limit = prediction['total_summary']['within_300mb_limit']
        
        if within_limit:
            strategy = {
                'approach': 'FULL_PRELOAD_CACHE',
                'description': 'ëª¨ë“  ì¶œë§ˆì ì •ë³´ ì‚¬ì „ ìºì‹±',
                'implementation_phases': [
                    {
                        'phase': 1,
                        'title': 'ìºì‹œ êµ¬ì¡° ì„¤ê³„',
                        'duration': '1-2ì¼',
                        'tasks': [
                            'ìºì‹œ ìŠ¤í‚¤ë§ˆ ì •ì˜',
                            'ë©”ëª¨ë¦¬ í• ë‹¹ ê³„íš',
                            'ì••ì¶• ì•Œê³ ë¦¬ì¦˜ ì„ íƒ'
                        ]
                    },
                    {
                        'phase': 2,
                        'title': 'ë°ì´í„° ìƒì„± ë° ê²€ì¦',
                        'duration': '3-4ì¼',
                        'tasks': [
                            'ì „ì²´ ì¶œë§ˆì ë°ì´í„° ìƒì„±',
                            'ë°ì´í„° í’ˆì§ˆ ê²€ì¦',
                            'ì••ì¶• ë° ìµœì í™”'
                        ]
                    },
                    {
                        'phase': 3,
                        'title': 'ìºì‹œ ì‹œìŠ¤í…œ êµ¬í˜„',
                        'duration': '2-3ì¼',
                        'tasks': [
                            'Redis/Memcached ì„¤ì •',
                            'ìºì‹œ ë¡œë”© ë¡œì§ êµ¬í˜„',
                            'ì„±ëŠ¥ í…ŒìŠ¤íŠ¸'
                        ]
                    },
                    {
                        'phase': 4,
                        'title': 'í†µí•© ë° ë°°í¬',
                        'duration': '1-2ì¼',
                        'tasks': [
                            'API í†µí•©',
                            'í”„ë¡œë•ì…˜ ë°°í¬',
                            'ëª¨ë‹ˆí„°ë§ ì„¤ì •'
                        ]
                    }
                ],
                'total_timeline': '7-11ì¼',
                'success_probability': '95%'
            }
        else:
            strategy = {
                'approach': 'SELECTIVE_CACHE',
                'description': 'ì„ ë³„ì  ìºì‹± ì „ëµ',
                'implementation_phases': [
                    {
                        'phase': 1,
                        'title': 'ìš°ì„ ìˆœìœ„ ë¶„ì„',
                        'duration': '1-2ì¼',
                        'tasks': [
                            'ì£¼ìš” ì¶œë§ˆì ì‹ë³„',
                            'ê²€ìƒ‰ ë¹ˆë„ ì˜ˆì¸¡',
                            'ìºì‹œ ìš°ì„ ìˆœìœ„ ì„¤ì •'
                        ]
                    },
                    {
                        'phase': 2,
                        'title': 'í‹°ì–´ë“œ ìºì‹œ êµ¬í˜„',
                        'duration': '4-5ì¼',
                        'tasks': [
                            'HOT ìºì‹œ (ë©”ëª¨ë¦¬)',
                            'WARM ìºì‹œ (SSD)',
                            'COLD ìºì‹œ (ìƒì„±ì‹œì )'
                        ]
                    },
                    {
                        'phase': 3,
                        'title': 'ì§€ëŠ¥í˜• ë¡œë”©',
                        'duration': '3-4ì¼',
                        'tasks': [
                            'ì˜ˆì¸¡ ê¸°ë°˜ í”„ë¦¬ë¡œë”©',
                            'ì‚¬ìš© íŒ¨í„´ í•™ìŠµ',
                            'ë™ì  ìºì‹œ ê´€ë¦¬'
                        ]
                    }
                ],
                'total_timeline': '8-11ì¼',
                'success_probability': '85%'
            }
        
        return strategy

    def export_capacity_prediction(self) -> str:
        """ìš©ëŸ‰ ì˜ˆì¸¡ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°"""
        
        print("ğŸ“Š ì¶œë§ˆì ì „ì› ìºì‹œ ìš©ëŸ‰ ì˜ˆì¸¡ ì‹¤í–‰")
        print("=" * 80)
        
        try:
            # 1. ìš©ëŸ‰ ì˜ˆì¸¡
            prediction = self.predict_total_cache_capacity()
            
            # 2. íš¨ìœ¨ì„± ë¶„ì„
            print("\nğŸ” ìºì‹œ íš¨ìœ¨ì„± ë¶„ì„...")
            efficiency = self.analyze_cache_efficiency(prediction)
            
            # 3. êµ¬í˜„ ì „ëµ
            print("\nğŸ“‹ êµ¬í˜„ ì „ëµ ìˆ˜ë¦½...")
            strategy = self.generate_implementation_strategy(prediction)
            
            # ì¢…í•© ë³´ê³ ì„œ
            comprehensive_report = {
                'metadata': {
                    'title': 'ì¶œë§ˆì ì „ì› ì´ë¦„ê²€ìƒ‰ ìºì‹± ìš©ëŸ‰ ì˜ˆì¸¡',
                    'prediction_date': datetime.now().isoformat(),
                    'target_limit': '300MB',
                    'analysis_scope': '6ê°œ ì§ê¸‰ ì „ì²´ ì¶œë§ˆì'
                },
                'capacity_prediction': prediction,
                'efficiency_analysis': efficiency,
                'implementation_strategy': strategy,
                'final_recommendation': {
                    'proceed': prediction['total_summary']['within_300mb_limit'],
                    'approach': strategy['approach'],
                    'timeline': strategy['total_timeline'],
                    'success_rate': strategy['success_probability']
                }
            }
            
            # ê²°ê³¼ ì €ì¥
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'candidate_cache_capacity_prediction_{timestamp}.json'
            filepath = os.path.join(self.backend_dir, filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(comprehensive_report, f, ensure_ascii=False, indent=2)
            
            print(f"\nâœ… ìš©ëŸ‰ ì˜ˆì¸¡ ì™„ë£Œ: {filename}")
            return filename
            
        except Exception as e:
            print(f"\nâŒ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return ''

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    predictor = CandidateCacheCapacityPredictor()
    
    print('ğŸ“Š ì¶œë§ˆì ì „ì› ì´ë¦„ê²€ìƒ‰ ìºì‹± ìš©ëŸ‰ ì˜ˆì¸¡')
    print('=' * 80)
    print('ğŸ¯ ëª©ì : ëª¨ë“  ì¶œë§ˆì ìºì‹œ ìš©ëŸ‰ ê³„ì‚° ë° 300MB í•œê³„ ê²€ì¦')
    print('ğŸ“Š ê¸°ë°˜: 96.19% ë‹¤ì–‘ì„± ì‹œìŠ¤í…œ + 6ê°œ ì§ê¸‰ë³„ ë¶„ì„')
    print('ğŸ” ë²”ìœ„: êµ­íšŒì˜ì›~ê¸°ì´ˆì˜ì›ê¹Œì§€ ì „ì²´ ì¶œë§ˆì')
    print('=' * 80)
    
    try:
        # ìš©ëŸ‰ ì˜ˆì¸¡ ì‹¤í–‰
        report_file = predictor.export_capacity_prediction()
        
        if report_file:
            print(f'\nğŸ‰ ì¶œë§ˆì ìºì‹œ ìš©ëŸ‰ ì˜ˆì¸¡ ì™„ì„±!')
            print(f'ğŸ“„ ì˜ˆì¸¡ ë³´ê³ ì„œ: {report_file}')
            
            # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
            with open(os.path.join(predictor.backend_dir, report_file), 'r', encoding='utf-8') as f:
                report = json.load(f)
            
            prediction = report['capacity_prediction']['total_summary']
            strategy = report['implementation_strategy']
            
            print(f'\nğŸ“Š ìš©ëŸ‰ ì˜ˆì¸¡ ê²°ê³¼:')
            print(f'  ğŸ‘¥ ì´ ì¶œë§ˆì: {prediction["total_candidates"]:,}ëª…')
            print(f'  ğŸ’¾ ì´ ìš©ëŸ‰: {prediction["total_size_mb"]:.1f}MB')
            print(f'  ğŸ“ 300MB í•œê³„: {"âœ… ê°€ëŠ¥" if prediction["within_300mb_limit"] else "âŒ ì´ˆê³¼"}')
            print(f'  ğŸ“Š ì‚¬ìš©ë¥ : {prediction["utilization_percentage"]:.1f}%')
            
            print(f'\nğŸš€ ê¶Œì¥ ì „ëµ:')
            print(f'  ğŸ“‹ ì ‘ê·¼ë²•: {strategy["approach"]}')
            print(f'  â° êµ¬í˜„ ê¸°ê°„: {strategy["total_timeline"]}')
            print(f'  ğŸ¯ ì„±ê³µ í™•ë¥ : {strategy["success_probability"]}')
            
            # ìµœì¢… ê¶Œì¥ì‚¬í•­
            proceed = report['final_recommendation']['proceed']
            print(f'\nğŸ† ìµœì¢… ê²°ë¡ : {"ğŸš€ ìºì‹± ì§„í–‰ ê¶Œì¥" if proceed else "âš ï¸ ì„ ë³„ì  ìºì‹± ê¶Œì¥"}')
        
        else:
            print('\nâŒ ì˜ˆì¸¡ ì‹¤íŒ¨')
            
    except Exception as e:
        print(f'\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}')

if __name__ == '__main__':
    main()
