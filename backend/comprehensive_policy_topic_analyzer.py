#!/usr/bin/env python3
"""
ì •ì±…ì„ ê±°ë¬¸í™” ë¬¸ì„œ ì¢…í•© í† í”½ ë¶„ì„ê¸°
635í˜ì´ì§€ ë¬¸ì„œë¥¼ ë” ìƒì„¸í•˜ê²Œ ë¶„ì„í•˜ì—¬ ë‹¤ì–‘í•œ í† í”½ ì¶”ì¶œ
"""

import os
import json
import re
import logging
from datetime import datetime
from collections import Counter, defaultdict
from typing import Dict, List, Tuple, Any
import pickle

# í…ìŠ¤íŠ¸ë§ˆì´ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import fitz  # PyMuPDF
    import numpy as np
    from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
    from sklearn.cluster import KMeans, DBSCAN
    from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD
    from sklearn.manifold import TSNE
    from sklearn.metrics.pairwise import cosine_similarity
    import networkx as nx
    
    # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„
    try:
        from konlpy.tag import Okt, Mecab, Komoran
        KOREAN_ANALYZER = "okt"
    except ImportError:
        try:
            import mecab
            KOREAN_ANALYZER = "mecab"
        except ImportError:
            KOREAN_ANALYZER = None
    
    NLP_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ í…ìŠ¤íŠ¸ë§ˆì´ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í•„ìš”: {e}")
    NLP_AVAILABLE = False

logger = logging.getLogger(__name__)

class ComprehensivePolicyTopicAnalyzer:
    """ì •ì±…ì„ ê±°ë¬¸í™” ë¬¸ì„œ ì¢…í•© í† í”½ ë¶„ì„ê¸°"""
    
    def __init__(self, pdf_file_path: str):
        self.pdf_file = pdf_file_path
        self.analysis_results = {}
        
        # í•œêµ­ì–´ ë¶„ì„ê¸° ì´ˆê¸°í™” (Java ì—†ì´ë„ ì‘ë™í•˜ë„ë¡)
        self.analyzer = None
        if KOREAN_ANALYZER == "okt":
            try:
                self.analyzer = Okt()
                print("âœ… Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                print(f"âš ï¸ Okt ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.analyzer = None
        elif KOREAN_ANALYZER == "mecab":
            try:
                self.analyzer = Mecab()
                print("âœ… Mecab í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                print(f"âš ï¸ Mecab ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.analyzer = None
        elif KOREAN_ANALYZER == "komoran":
            try:
                self.analyzer = Komoran()
                print("âœ… Komoran í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                print(f"âš ï¸ Komoran ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.analyzer = None
        else:
            print("âš ï¸ í˜•íƒœì†Œ ë¶„ì„ê¸° ì—†ì´ ì‹¤í–‰í•©ë‹ˆë‹¤. ê¸°ë³¸ ë‹¨ì–´ ë¶„í• ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        # í™•ì¥ëœ í† í”½ ì¹´í…Œê³ ë¦¬ (8ê°œ â†’ 20ê°œ+)
        self.comprehensive_topic_categories = {
            # ê¸°ë³¸ ê²½ì œ/ì‚¬íšŒ í† í”½
            "ê²½ì œì •ì±…": {
                "keywords": ["ì¼ìë¦¬", "ì·¨ì—…", "ì°½ì—…", "ê²½ì œì„±ì¥", "ì†Œë“", "ì„ê¸ˆ", "ê³ ìš©", "ì‹¤ì—…", "ê²½ì œí™œë™", "ì‚¬ì—…", "íˆ¬ì", "ê¸ˆìœµ", "ì¤‘ì†Œê¸°ì—…", "ìŠ¤íƒ€íŠ¸ì—…", "ë²¤ì²˜", "ê²½ì œì§€ì›", "ê²½ì œê°œë°œ", "ì‚°ì—…ìœ¡ì„±", "ìˆ˜ì¶œ", "ìˆ˜ì…", "ë¬´ì—­", "ê²½ì œí˜‘ë ¥"],
                "description": "ì¼ìë¦¬ ì°½ì¶œ, ê²½ì œ í™œì„±í™”, ì†Œë“ ì¦ëŒ€, ì¤‘ì†Œê¸°ì—… ì§€ì›"
            },
            "ì£¼ê±°ì •ì±…": {
                "keywords": ["ì£¼íƒ", "ë¶€ë™ì‚°", "ì„ëŒ€", "ì „ì„¸", "ì›”ì„¸", "ì•„íŒŒíŠ¸", "ì§‘ê°’", "ì£¼ê±°", "ë¶„ì–‘", "ë§¤ë§¤", "ì£¼íƒê³µê¸‰", "ê³µê³µì£¼íƒ", "ì„ëŒ€ì£¼íƒ", "ì²­ë…„ì£¼íƒ", "ì‹ ì¶•", "ì¬ê°œë°œ", "ì¬ê±´ì¶•", "ë„ì‹œê³„íš", "íƒì§€ê°œë°œ"],
                "description": "ì£¼íƒ ê³µê¸‰, ë¶€ë™ì‚° ì•ˆì •í™”, ì£¼ê±° ë³µì§€, ë„ì‹œ ê°œë°œ"
            },
            "êµìœ¡ì •ì±…": {
                "keywords": ["êµìœ¡", "í•™êµ", "ëŒ€í•™", "ì…ì‹œ", "ì‚¬êµìœ¡", "êµìœ¡ë¹„", "í•™ìŠµ", "í•™ìƒ", "êµì‚¬", "êµìœ¡ê³¼ì •", "í•™ì›", "ë³´ìŠµ", "êµìœ¡í˜ì‹ ", "ë””ì§€í„¸êµìœ¡", "ì˜¨ë¼ì¸êµìœ¡", "êµìœ¡ì‹œì„¤", "êµìœ¡í™˜ê²½", "êµìœ¡ì§€ì›", "ì¥í•™ê¸ˆ", "êµìœ¡ë³µì§€"],
                "description": "êµìœ¡ í™˜ê²½ ê°œì„ , ì‚¬êµìœ¡ ë¶€ë‹´ í•´ì†Œ, êµìœ¡ í˜ì‹ "
            },
            "ë³µì§€ì •ì±…": {
                "keywords": ["ë³µì§€", "ì˜ë£Œ", "ê±´ê°•ë³´í—˜", "ì—°ê¸ˆ", "ìœ¡ì•„", "ë³´ìœ¡", "ì‚¬íšŒë³´ì¥", "ë³µì§€í˜œíƒ", "ì§€ì›ê¸ˆ", "ëŒë´„", "ë…¸ì¸ë³µì§€", "ì¥ì• ì¸ë³µì§€", "ê¸°ì´ˆìƒí™œë³´ì¥", "ìƒí™œë³´ì¡°", "ì˜ë£Œë¹„ì§€ì›", "ì¹˜ë£Œë¹„", "ê°„ë³‘", "ìš”ì–‘"],
                "description": "ì‚¬íšŒë³µì§€ í™•ì¶©, ì˜ë£Œ ì„œë¹„ìŠ¤ ê°œì„ , ëŒë´„ ì„œë¹„ìŠ¤"
            },
            "í™˜ê²½ì •ì±…": {
                "keywords": ["í™˜ê²½", "ê¸°í›„", "ì—ë„ˆì§€", "ë¯¸ì„¸ë¨¼ì§€", "ì¬ìƒì—ë„ˆì§€", "ì¹œí™˜ê²½", "íƒ„ì†Œ", "ì˜¤ì—¼", "ë…¹ìƒ‰", "ì§€ì†ê°€ëŠ¥", "ëŒ€ê¸°ì§ˆ", "ìˆ˜ì§ˆ", "í† ì–‘", "íê¸°ë¬¼", "ì¬í™œìš©", "ì—ë„ˆì§€ì ˆì•½", "ì‹ ì¬ìƒì—ë„ˆì§€", "íƒœì–‘ê´‘", "í’ë ¥"],
                "description": "í™˜ê²½ ë³´í˜¸, ì§€ì†ê°€ëŠ¥í•œ ë°œì „, ì²­ì • ì—ë„ˆì§€"
            },
            "êµí†µì •ì±…": {
                "keywords": ["êµí†µ", "ëŒ€ì¤‘êµí†µ", "ì§€í•˜ì² ", "ë²„ìŠ¤", "ë„ë¡œ", "ì£¼ì°¨", "êµí†µì²´ì¦", "ì´ë™", "ì ‘ê·¼ì„±", "ì¸í”„ë¼", "ëŒ€ì¤‘êµí†µë§", "êµí†µë§", "ì§€í•˜ì² ì—°ì¥", "ë²„ìŠ¤ë…¸ì„ ", "ë„ì‹œì² ë„", "ê³ ì†ë„ë¡œ", "êµ­ë„", "ì§€ë°©ë„"],
                "description": "êµí†µ ì¸í”„ë¼ ê°œì„ , ëŒ€ì¤‘êµí†µ í™•ì¶©, êµí†µ ì²´ì¦ í•´ì†Œ"
            },
            "ë¬¸í™”ì •ì±…": {
                "keywords": ["ë¬¸í™”", "ì˜ˆìˆ ", "ê³µì—°", "ì „ì‹œ", "ì¶•ì œ", "ê´€ê´‘", "ì—¬ê°€", "ìŠ¤í¬ì¸ ", "ì²´ìœ¡", "ë„ì„œê´€", "ë°•ë¬¼ê´€", "ë¬¸í™”ì‹œì„¤", "ë¬¸í™”ê³µê°„", "ë¬¸í™”í–‰ì‚¬", "ë¬¸í™”í”„ë¡œê·¸ë¨", "ë¬¸í™”ìœ ì‚°", "ì „í†µë¬¸í™”", "í˜„ëŒ€ë¬¸í™”"],
                "description": "ë¬¸í™” ì˜ˆìˆ  í™œì„±í™”, ì—¬ê°€ ìƒí™œ ì¦ì§„, ê´€ê´‘ ë°œì „"
            },
            "ì•ˆì „ì •ì±…": {
                "keywords": ["ì•ˆì „", "ì¬ë‚œ", "ì¬í•´", "ì†Œë°©", "ì‘ê¸‰", "ë²”ì£„", "ì‚¬ê³ ", "ìœ„í—˜", "ë³´ì•ˆ", "ì¹˜ì•ˆ", "ë°©ë²”", "CCTV", "ì•ˆì „ì‹œì„¤", "ì¬ë‚œëŒ€ì‘", "ì‘ê¸‰ì˜ë£Œ", "êµ¬ì¡°", "êµ¬ê¸‰", "í™”ì¬", "ì§€ì§„", "íƒœí’"],
                "description": "ì•ˆì „í•œ ìƒí™œ í™˜ê²½ ì¡°ì„±, ì¬ë‚œ ëŒ€ì‘ ì²´ê³„ êµ¬ì¶•"
            },
            
            # ì¶”ê°€ ì„¸ë¶„í™” í† í”½ë“¤
            "ë³´ê±´ì˜ë£Œì •ì±…": {
                "keywords": ["ì˜ë£Œ", "ë³‘ì›", "ì˜ë£Œì§„", "ì˜ë£Œê¸°ê´€", "ì§„ë£Œ", "ì¹˜ë£Œ", "ì˜ˆë°©", "ê±´ê°•", "ì§ˆë³‘", "ì˜ë£Œì„œë¹„ìŠ¤", "ì˜ë£Œì‹œì„¤", "ì‘ê¸‰ì‹¤", "ìˆ˜ìˆ ", "ì…ì›", "ì™¸ë˜", "ì˜ë£Œê¸°ê¸°", "ì˜ì•½í’ˆ", "ì˜ë£Œë¹„"],
                "description": "ì˜ë£Œ ì„œë¹„ìŠ¤ í™•ì¶©, ì˜ë£Œ ì¸í”„ë¼ êµ¬ì¶•, ê³µê³µì˜ë£Œ ê°•í™”"
            },
            "ë†ì—…ì •ì±…": {
                "keywords": ["ë†ì—…", "ë†ë¯¼", "ë†ì´Œ", "ë†ì§€", "ë†ì‚°ë¬¼", "ë†ì‘ë¬¼", "ë†ê¸°ê³„", "ë†ì—…ê¸°ìˆ ", "ë†ì—…ì§€ì›", "ë†ì—…ì •ì±…", "ë†ì—…ì¸", "ë†ì—…ì†Œë“", "ë†ì—…ì‹œì„¤", "ë†ì—…ê²½ì˜", "ë†ì—…í˜‘ë™ì¡°í•©", "ë†ì‚°ë¬¼ìœ í†µ"],
                "description": "ë†ì—… ë°œì „, ë†ë¯¼ ì†Œë“ ì¦ëŒ€, ë†ì´Œ í™œì„±í™”"
            },
            "ìˆ˜ì‚°ì—…ì •ì±…": {
                "keywords": ["ìˆ˜ì‚°ì—…", "ì–´ì—…", "ì–‘ì‹", "ì–´ì´Œ", "ì–´ì„ ", "ìˆ˜ì‚°ë¬¼", "ì–´ì—…ì", "ì–´ì—…ì§€ì›", "ìˆ˜ì‚°ì—…ì •ì±…", "ì–‘ì‹ì—…", "ì–´ì—…ê¸°ìˆ ", "ìˆ˜ì‚°ì—…ì‹œì„¤", "ì–´ì´Œê°œë°œ", "ì–´ì—…í˜‘ë™ì¡°í•©", "ìˆ˜ì‚°ë¬¼ìœ í†µ"],
                "description": "ìˆ˜ì‚°ì—… ë°œì „, ì–´ì—…ì¸ ì§€ì›, ì–´ì´Œ í™œì„±í™”"
            },
            "ê´€ê´‘ì •ì±…": {
                "keywords": ["ê´€ê´‘", "ê´€ê´‘ì§€", "ê´€ê´‘ê°", "ê´€ê´‘ì‹œì„¤", "ê´€ê´‘ì‚°ì—…", "ê´€ê´‘ì •ì±…", "ê´€ê´‘ì§€ì—­", "ê´€ê´‘ìì›", "ê´€ê´‘ìƒí’ˆ", "ê´€ê´‘ë§ˆì¼€íŒ…", "ê´€ê´‘ê°ìœ ì¹˜", "ê´€ê´‘ì¸í”„ë¼", "ê´€ê´‘ìˆ™ë°•", "ê´€ê´‘ì‹ë‹¹", "ê´€ê´‘êµí†µ"],
                "description": "ê´€ê´‘ ì‚°ì—… ë°œì „, ê´€ê´‘ì§€ ê°œë°œ, ê´€ê´‘ê° ìœ ì¹˜"
            },
            "ìŠ¤í¬ì¸ ì •ì±…": {
                "keywords": ["ìŠ¤í¬ì¸ ", "ì²´ìœ¡", "ìš´ë™", "ì²´ìœ¡ì‹œì„¤", "ì²´ìœ¡ê´€", "ìš´ë™ì¥", "ìŠ¤í¬ì¸ ì„¼í„°", "ì²´ìœ¡êµìœ¡", "ìŠ¤í¬ì¸ êµìœ¡", "ì²´ìœ¡í”„ë¡œê·¸ë¨", "ìŠ¤í¬ì¸ í”„ë¡œê·¸ë¨", "ì²´ìœ¡ëŒ€íšŒ", "ìŠ¤í¬ì¸ ëŒ€íšŒ", "ì²´ìœ¡ë‹¨ì²´", "ìŠ¤í¬ì¸ ë‹¨ì²´"],
                "description": "ìŠ¤í¬ì¸  ì¸í”„ë¼ êµ¬ì¶•, ì²´ìœ¡ í™œì„±í™”, ê±´ê°•í•œ ìƒí™œ"
            },
            "ì •ë³´í†µì‹ ì •ì±…": {
                "keywords": ["ì •ë³´í†µì‹ ", "IT", "ë””ì§€í„¸", "ì¸í„°ë„·", "í†µì‹ ", "ìŠ¤ë§ˆíŠ¸ì‹œí‹°", "ë¹…ë°ì´í„°", "ì¸ê³µì§€ëŠ¥", "AI", "ë””ì§€í„¸ì •ë¶€", "ì „ìì •ë¶€", "ì •ë³´í™”", "ë””ì§€í„¸í™”", "ìŠ¤ë§ˆíŠ¸ê¸°ê¸°", "ëª¨ë°”ì¼", "ì•±"],
                "description": "ë””ì§€í„¸ ì „í™˜, ìŠ¤ë§ˆíŠ¸ì‹œí‹° êµ¬ì¶•, ì •ë³´í†µì‹  ì¸í”„ë¼"
            },
            "ì—¬ì„±ê°€ì¡±ì •ì±…": {
                "keywords": ["ì—¬ì„±", "ê°€ì¡±", "ìœ¡ì•„", "ì¶œì‚°", "ì„ì‹ ", "ì‚°í›„ì¡°ë¦¬", "ë³´ìœ¡", "ì–´ë¦°ì´ì§‘", "ìœ ì¹˜ì›", "ì•„ë™", "ì²­ì†Œë…„", "ë‹¤ë¬¸í™”", "ê°€ì¡±ì§€ì›", "ì—¬ì„±ì§€ì›", "ì¶œì‚°ì§€ì›", "ìœ¡ì•„ì§€ì›"],
                "description": "ì—¬ì„± ì§€ì›, ê°€ì¡± ì¹œí™” ì •ì±…, ì¶œì‚° ì¥ë ¤"
            },
            "ì²­ë…„ì •ì±…": {
                "keywords": ["ì²­ë…„", "ì²­ì†Œë…„", "ëŒ€í•™ìƒ", "ì·¨ì—…ì¤€ë¹„", "ì²­ë…„ì°½ì—…", "ì²­ë…„ì§€ì›", "ì²­ë…„ì •ì±…", "ì²­ë…„ì¼ìë¦¬", "ì²­ë…„ì£¼íƒ", "ì²­ë…„ë³µì§€", "ì²­ë…„í™œë™", "ì²­ë…„ê¸°ê´€", "ì²­ë…„ì„¼í„°", "ì²­ë…„ê³µê°„"],
                "description": "ì²­ë…„ ì§€ì›, ì²­ë…„ ì¼ìë¦¬ ì°½ì¶œ, ì²­ë…„ ì°½ì—… ì§€ì›"
            },
            "ë…¸ì¸ì •ì±…": {
                "keywords": ["ë…¸ì¸", "ì–´ë¥´ì‹ ", "ê³ ë ¹ì", "ë…¸ì¸ë³µì§€", "ë…¸ì¸ì§€ì›", "ë…¸ì¸ì •ì±…", "ë…¸ì¸ì¼ìë¦¬", "ë…¸ì¸í™œë™", "ë…¸ì¸ì„¼í„°", "ë…¸ì¸ë³µì§€ê´€", "ë…¸ì¸ìš”ì–‘", "ë…¸ì¸ëŒë´„", "ë…¸ì¸ê±´ê°•", "ë…¸ì¸êµìœ¡"],
                "description": "ë…¸ì¸ ë³µì§€, ë…¸ì¸ ì§€ì›, ê³ ë ¹í™” ëŒ€ì‘"
            },
            "ì¥ì• ì¸ì •ì±…": {
                "keywords": ["ì¥ì• ì¸", "ì¥ì• ", "ì¥ì• ì¸ë³µì§€", "ì¥ì• ì¸ì§€ì›", "ì¥ì• ì¸ì •ì±…", "ì¥ì• ì¸ì‹œì„¤", "ì¥ì• ì¸ë³µì§€ê´€", "ì¥ì• ì¸í™œë™", "ì¥ì• ì¸ì¼ìë¦¬", "ì¥ì• ì¸êµìœ¡", "ì¥ì• ì¸ì˜ë£Œ", "ì¥ì• ì¸ëŒë´„"],
                "description": "ì¥ì• ì¸ ë³µì§€, ì¥ì• ì¸ ì§€ì›, ì¥ì• ì¸ ê¶Œë¦¬ ë³´ì¥"
            },
            "í™˜ê²½ë³´ì „ì •ì±…": {
                "keywords": ["í™˜ê²½ë³´ì „", "ìì—°ë³´ì „", "ìƒíƒœë³´ì „", "í™˜ê²½ë³´í˜¸", "ìì—°í™˜ê²½", "ìƒíƒœí™˜ê²½", "í™˜ê²½ë³µì›", "ìì—°ë³µì›", "ìƒíƒœë³µì›", "í™˜ê²½ì¡°ì‚¬", "í™˜ê²½ëª¨ë‹ˆí„°ë§", "í™˜ê²½ì˜í–¥í‰ê°€"],
                "description": "í™˜ê²½ ë³´ì „, ìì—° ìƒíƒœ ë³´í˜¸, í™˜ê²½ ë³µì›"
            },
            "ì¬ì •ì •ì±…": {
                "keywords": ["ì¬ì •", "ì˜ˆì‚°", "ì„¸ê¸ˆ", "ì§€ë°©ì„¸", "êµ­ì„¸", "ì¬ì •ì§€ì›", "ë³´ì¡°ê¸ˆ", "ì§€ì›ê¸ˆ", "ì¬ì •íˆ¬ì", "ì¬ì •ìš´ì˜", "ì¬ì •ê³„íš", "ì¬ì •ì •ì±…", "ì¬ì •ê±´ì „ì„±"],
                "description": "ì¬ì • ìš´ì˜, ì˜ˆì‚° ë°°ë¶„, ì„¸ìˆ˜ í™•ì¶©"
            },
            "í–‰ì •ì •ì±…": {
                "keywords": ["í–‰ì •", "ì •ë¶€", "ì§€ë°©ìì¹˜", "í–‰ì •ì„œë¹„ìŠ¤", "ë¯¼ì›", "í–‰ì •ì²˜ë¦¬", "í–‰ì •ì ˆì°¨", "í–‰ì •ê°œí˜", "í–‰ì •íš¨ìœ¨", "ì „ìì •ë¶€", "í–‰ì •ì •ë³´í™”", "í–‰ì •íˆ¬ëª…ì„±"],
                "description": "í–‰ì • ì„œë¹„ìŠ¤ ê°œì„ , í–‰ì • íš¨ìœ¨ì„±, ì „ìì •ë¶€ êµ¬ì¶•"
            },
            "êµ­ë°©ì•ˆë³´ì •ì±…": {
                "keywords": ["êµ­ë°©", "ì•ˆë³´", "êµ°ì‚¬", "êµ­ë°©ì •ì±…", "ì•ˆë³´ì •ì±…", "êµ­ë°©ë ¥", "êµ°ì‚¬ë ¥", "êµ­ë°©ë¹„", "êµ°ì‚¬ì‹œì„¤", "êµ­ë°©ì‹œì„¤", "ì•ˆë³´ì‹œì„¤", "êµ­ë°©í˜‘ë ¥", "ì•ˆë³´í˜‘ë ¥"],
                "description": "êµ­ë°©ë ¥ ê°•í™”, ì•ˆë³´ ì²´ê³„ êµ¬ì¶•, êµ­ë°© í˜‘ë ¥"
            }
        }
        
        # ì§€ì—­ í‚¤ì›Œë“œ ë§¤í•‘
        self.regional_keywords = self._load_regional_keywords()
    
    def _load_regional_keywords(self) -> Dict[str, List[str]]:
        """ì§€ì—­ë³„ í‚¤ì›Œë“œ ë¡œë“œ"""
        return {
            "ì„œìš¸": ["ì„œìš¸", "ì„œìš¸ì‹œ", "ì„œìš¸íŠ¹ë³„ì‹œ", "ê°•ë‚¨", "ê°•ë¶", "ê°•ë™", "ê°•ì„œ", "ê´€ì•…", "ê´‘ì§„", "êµ¬ë¡œ", "ê¸ˆì²œ", "ë…¸ì›", "ë„ë´‰", "ë™ëŒ€ë¬¸", "ë™ì‘", "ë§ˆí¬", "ì„œëŒ€ë¬¸", "ì„œì´ˆ", "ì„±ë™", "ì„±ë¶", "ì†¡íŒŒ", "ì–‘ì²œ", "ì˜ë“±í¬", "ìš©ì‚°", "ì€í‰", "ì¢…ë¡œ", "ì¤‘êµ¬", "ì¤‘ë‘"],
            "ë¶€ì‚°": ["ë¶€ì‚°", "ë¶€ì‚°ì‹œ", "ë¶€ì‚°ê´‘ì—­ì‹œ", "ê°•ì„œ", "ê¸ˆì •", "ê¸°ì¥", "ë‚¨êµ¬", "ë™êµ¬", "ë™ë˜", "ë¶€ì‚°ì§„", "ë¶êµ¬", "ì‚¬ìƒ", "ì‚¬í•˜", "ì„œêµ¬", "ìˆ˜ì˜", "ì—°ì œ", "ì˜ë„", "ì¤‘êµ¬", "í•´ìš´ëŒ€"],
            "ëŒ€êµ¬": ["ëŒ€êµ¬", "ëŒ€êµ¬ì‹œ", "ëŒ€êµ¬ê´‘ì—­ì‹œ", "êµ°ìœ„", "ë‚¨êµ¬", "ë‹¬ì„œ", "ë‹¬ì„±", "ë™êµ¬", "ë¶êµ¬", "ì„œêµ¬", "ìˆ˜ì„±", "ì¤‘êµ¬"],
            "ì¸ì²œ": ["ì¸ì²œ", "ì¸ì²œì‹œ", "ì¸ì²œê´‘ì—­ì‹œ", "ê°•í™”", "ê³„ì–‘", "ë‚¨ë™", "ë™êµ¬", "ë¯¸ì¶”í™€", "ë¶€í‰", "ì„œêµ¬", "ì—°ìˆ˜", "ì˜¹ì§„", "ì¤‘êµ¬"],
            "ê´‘ì£¼": ["ê´‘ì£¼", "ê´‘ì£¼ì‹œ", "ê´‘ì£¼ê´‘ì—­ì‹œ", "ê´‘ì‚°", "ë‚¨êµ¬", "ë™êµ¬", "ë¶êµ¬", "ì„œêµ¬"],
            "ëŒ€ì „": ["ëŒ€ì „", "ëŒ€ì „ì‹œ", "ëŒ€ì „ê´‘ì—­ì‹œ", "ëŒ€ë•", "ë™êµ¬", "ì„œêµ¬", "ìœ ì„±", "ì¤‘êµ¬"],
            "ìš¸ì‚°": ["ìš¸ì‚°", "ìš¸ì‚°ì‹œ", "ìš¸ì‚°ê´‘ì—­ì‹œ", "ë‚¨êµ¬", "ë™êµ¬", "ë¶êµ¬", "ìš¸ì£¼", "ì¤‘êµ¬"],
            "ì„¸ì¢…": ["ì„¸ì¢…", "ì„¸ì¢…ì‹œ", "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ"],
            "ê²½ê¸°": ["ê²½ê¸°", "ê²½ê¸°ë„", "ê°€í‰", "ê³ ì–‘", "ê³¼ì²œ", "ê´‘ëª…", "ê´‘ì£¼", "êµ¬ë¦¬", "êµ°í¬", "ê¹€í¬", "ë‚¨ì–‘ì£¼", "ë™ë‘ì²œ", "ë¶€ì²œ", "ì„±ë‚¨", "ìˆ˜ì›", "ì‹œí¥", "ì•ˆì‚°", "ì•ˆì„±", "ì•ˆì–‘", "ì–‘ì£¼", "ì–‘í‰", "ì—¬ì£¼", "ì—°ì²œ", "ì˜¤ì‚°", "ìš©ì¸", "ì˜ì™•", "ì˜ì •ë¶€", "ì´ì²œ", "íŒŒì£¼", "í‰íƒ", "í¬ì²œ", "í•˜ë‚¨", "í™”ì„±"],
            "ê°•ì›": ["ê°•ì›", "ê°•ì›ë„", "ê°•ë¦‰", "ê³ ì„±", "ë™í•´", "ì‚¼ì²™", "ì†ì´ˆ", "ì–‘êµ¬", "ì–‘ì–‘", "ì˜ì›”", "ì›ì£¼", "ì¸ì œ", "ì •ì„ ", "ì² ì›", "ì¶˜ì²œ", "íƒœë°±", "í‰ì°½", "í™ì²œ", "í™”ì²œ", "íš¡ì„±"],
            "ì¶©ë¶": ["ì¶©ë¶", "ì¶©ì²­ë¶ë„", "ê´´ì‚°", "ë‹¨ì–‘", "ë³´ì€", "ì˜ë™", "ì˜¥ì²œ", "ìŒì„±", "ì œì²œ", "ì¦í‰", "ì§„ì²œ", "ì²­ì£¼", "ì¶©ì£¼"],
            "ì¶©ë‚¨": ["ì¶©ë‚¨", "ì¶©ì²­ë‚¨ë„", "ê³„ë£¡", "ê³µì£¼", "ê¸ˆì‚°", "ë…¼ì‚°", "ë‹¹ì§„", "ë³´ë ¹", "ë¶€ì—¬", "ì„œì‚°", "ì„œì²œ", "ì•„ì‚°", "ì˜ˆì‚°", "ì²œì•ˆ", "ì²­ì–‘", "íƒœì•ˆ", "í™ì„±"],
            "ì „ë¶": ["ì „ë¶", "ì „ë¼ë¶ë„", "ê³ ì°½", "êµ°ì‚°", "ê¹€ì œ", "ë‚¨ì›", "ë¬´ì£¼", "ë¶€ì•ˆ", "ìˆœì°½", "ì™„ì£¼", "ìµì‚°", "ì„ì‹¤", "ì¥ìˆ˜", "ì „ì£¼", "ì •ì", "ì§„ì•ˆ"],
            "ì „ë‚¨": ["ì „ë‚¨", "ì „ë¼ë‚¨ë„", "ê°•ì§„", "ê³ í¥", "ê³¡ì„±", "ê´‘ì–‘", "êµ¬ë¡€", "ë‚˜ì£¼", "ë‹´ì–‘", "ëª©í¬", "ë¬´ì•ˆ", "ë³´ì„±", "ìˆœì²œ", "ì‹ ì•ˆ", "ì—¬ìˆ˜", "ì˜ê´‘", "ì˜ì•”", "ì™„ë„", "ì¥ì„±", "ì¥í¥", "ì§„ë„", "í•¨í‰", "í•´ë‚¨", "í™”ìˆœ"],
            "ê²½ë¶": ["ê²½ë¶", "ê²½ìƒë¶ë„", "ê²½ì‚°", "ê²½ì£¼", "ê³ ë ¹", "êµ¬ë¯¸", "ê¹€ì²œ", "ë¬¸ê²½", "ë´‰í™”", "ìƒì£¼", "ì„±ì£¼", "ì•ˆë™", "ì˜ë•", "ì˜ì–‘", "ì˜ì£¼", "ì˜ì²œ", "ì˜ˆì²œ", "ìš¸ë¦‰", "ìš¸ì§„", "ì˜ì„±", "ì²­ë„", "ì²­ì†¡", "ì¹ ê³¡", "í¬í•­"],
            "ê²½ë‚¨": ["ê²½ë‚¨", "ê²½ìƒë‚¨ë„", "ê±°ì œ", "ê±°ì°½", "ê³ ì„±", "ê¹€í•´", "ë‚¨í•´", "ë°€ì–‘", "ì‚¬ì²œ", "ì‚°ì²­", "ì–‘ì‚°", "ì˜ë ¹", "ì§„ì£¼", "ì°½ë…•", "ì°½ì›", "í†µì˜", "í•˜ë™", "í•¨ì•ˆ", "í•¨ì–‘", "í•©ì²œ"],
            "ì œì£¼": ["ì œì£¼", "ì œì£¼ë„", "ì œì£¼íŠ¹ë³„ìì¹˜ë„", "ì œì£¼ì‹œ", "ì„œê·€í¬"]
        }
    
    def extract_text_from_pdf(self) -> str:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        print("ğŸ“„ PDF íŒŒì¼ ë‚´ìš© ì¶”ì¶œ ì¤‘...")
        
        if not os.path.exists(self.pdf_file):
            raise FileNotFoundError(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.pdf_file}")
        
        doc = fitz.open(self.pdf_file)
        full_text = ""
        total_pages = len(doc)
        
        print(f"ğŸ“Š ì´ {total_pages} í˜ì´ì§€ ì²˜ë¦¬ ì¤‘...")
        
        for page_num in range(total_pages):
            page = doc[page_num]
            page_text = page.get_text()
            full_text += page_text + '\n'
            
            if page_num % 50 == 0:
                print(f"  ğŸ“„ í˜ì´ì§€ {page_num + 1}/{total_pages} ì²˜ë¦¬ ì™„ë£Œ")
        
        doc.close()
        
        self.analysis_results['document_info'] = {
            'file_path': self.pdf_file,
            'total_pages': total_pages,
            'total_text_length': len(full_text),
            'extraction_date': datetime.now().isoformat()
        }
        
        print(f"âœ… PDF ì¶”ì¶œ ì™„ë£Œ: {len(full_text):,} ë¬¸ì")
        return full_text
    
    def preprocess_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        print("ğŸ”§ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì¤‘...")
        
        # ë¬¸ì¥ êµ¬ë¶„ìë¡œ ë¶„ë¦¬ (ë” ë‹¤ì–‘í•œ êµ¬ë¶„ì ì‚¬ìš©)
        sentence_endings = ['.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ', '\n', '\r\n']
        
        sentences = []
        current_sentence = ""
        
        for char in text:
            current_sentence += char
            
            if char in sentence_endings and len(current_sentence.strip()) > 5:
                cleaned_sentence = current_sentence.strip()
                # ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
                cleaned_sentence = re.sub(r'[^\w\sê°€-í£]', ' ', cleaned_sentence)
                cleaned_sentence = re.sub(r'\s+', ' ', cleaned_sentence)
                
                if len(cleaned_sentence) > 10:  # ìµœì†Œ ê¸¸ì´ í™•ì¸
                    sentences.append(cleaned_sentence)
                
                current_sentence = ""
        
        # ë§ˆì§€ë§‰ ë¬¸ì¥ ì²˜ë¦¬
        if current_sentence.strip():
            cleaned_sentence = re.sub(r'[^\w\sê°€-í£]', ' ', current_sentence.strip())
            cleaned_sentence = re.sub(r'\s+', ' ', cleaned_sentence)
            if len(cleaned_sentence) > 10:
                sentences.append(cleaned_sentence)
        
        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(sentences):,}ê°œ ë¬¸ì¥")
        return sentences
    
    def extract_keywords_with_frequency(self, sentences: List[str]) -> Dict[str, int]:
        """í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„"""
        print("ğŸ” í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„ ì¤‘...")
        
        all_words = []
        
        for sentence in sentences:
            if self.analyzer:
                # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„
                try:
                    words = self.analyzer.morphs(sentence)
                    # ëª…ì‚¬ë§Œ ì¶”ì¶œ
                    nouns = [word for word in words if len(word) > 1 and word.isalpha()]
                    all_words.extend(nouns)
                except:
                    # í˜•íƒœì†Œ ë¶„ì„ ì‹¤íŒ¨ì‹œ ë‹¨ìˆœ ë¶„í• 
                    words = sentence.split()
                    all_words.extend([word for word in words if len(word) > 1])
            else:
                # í˜•íƒœì†Œ ë¶„ì„ê¸° ì—†ì´ ë‹¨ìˆœ ë¶„í• 
                words = sentence.split()
                all_words.extend([word for word in words if len(word) > 1])
        
        # ë¶ˆìš©ì–´ ì œê±°
        stop_words = {'ê²ƒ', 'ë“±', 'ë°', 'ë˜í•œ', 'ë”°ë¼ì„œ', 'ê·¸ëŸ¬ë‚˜', 'í•˜ì§€ë§Œ', 'ê·¸ë¦¬ê³ ', 'ë˜ëŠ”', 'ë”°ë¼', 'ì´ì—', 'ì´ë¥¼', 'ì´ë¡œ', 'ì´ì™€', 'ì´ì˜', 'ì´ë¥¼', 'ì´ì—', 'ì´ë¡œ', 'ì´ì™€', 'ì´ì˜'}
        filtered_words = [word for word in all_words if word not in stop_words]
        
        # ë¹ˆë„ ê³„ì‚°
        word_freq = Counter(filtered_words)
        
        print(f"âœ… í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„ ì™„ë£Œ: {len(word_freq):,}ê°œ ê³ ìœ  ë‹¨ì–´")
        return dict(word_freq)
    
    def analyze_regional_topics(self, sentences: List[str]) -> Dict[str, Any]:
        """ì§€ì—­ë³„ í† í”½ ë¶„ì„"""
        print("ğŸ—ºï¸ ì§€ì—­ë³„ í† í”½ ë¶„ì„ ì¤‘...")
        
        regional_data = {}
        
        for region_name, region_keywords in self.regional_keywords.items():
            print(f"  ğŸ“ {region_name} ë¶„ì„ ì¤‘...")
            
            # í•´ë‹¹ ì§€ì—­ ì–¸ê¸‰ ë¬¸ì¥ë“¤ ì°¾ê¸°
            region_sentences = []
            for sentence in sentences:
                if any(keyword in sentence for keyword in region_keywords):
                    region_sentences.append(sentence)
            
            if not region_sentences:
                continue
            
            # í† í”½ë³„ ì ìˆ˜ ê³„ì‚°
            topic_scores = {}
            dominant_topics = []
            
            for topic_name, topic_info in self.comprehensive_topic_categories.items():
                score = 0
                for sentence in region_sentences:
                    for keyword in topic_info['keywords']:
                        if keyword in sentence:
                            score += 1
                
                if score > 0:
                    topic_scores[topic_name] = score
            
            # ìƒìœ„ í† í”½ ì„ ì • (ì ìˆ˜ ê¸°ì¤€)
            if topic_scores:
                sorted_topics = sorted(topic_scores.items(), key=lambda x: x[1], reverse=True)
                dominant_topics = [topic for topic, score in sorted_topics[:5]]
            
            # ì •ì±… ê³µì•½ ì¶”ì¶œ
            promises = self._extract_promises(region_sentences)
            
            regional_data[region_name] = {
                'region_name': region_name,
                'level': self._get_region_level(region_name),
                'mention_count': len(region_sentences),
                'dominant_topics': dominant_topics,
                'topic_scores': topic_scores,
                'interpretation': self._generate_interpretation(region_name, dominant_topics, topic_scores),
                'promises': promises[:10],  # ìƒìœ„ 10ê°œ ê³µì•½
                'confidence_score': self._calculate_confidence_score(topic_scores),
                'sample_sentences': region_sentences[:5]  # ìƒ˜í”Œ ë¬¸ì¥ 5ê°œ
            }
        
        print(f"âœ… ì§€ì—­ë³„ í† í”½ ë¶„ì„ ì™„ë£Œ: {len(regional_data)}ê°œ ì§€ì—­")
        return regional_data
    
    def _extract_promises(self, sentences: List[str]) -> List[str]:
        """ì •ì±… ê³µì•½ ì¶”ì¶œ"""
        promises = []
        
        # ê³µì•½ ê´€ë ¨ í‚¤ì›Œë“œ íŒ¨í„´
        promise_patterns = [
            r'ì„\s*ìœ„í•œ\s*[ê°€-í£]+',
            r'ë¥¼\s*ìœ„í•œ\s*[ê°€-í£]+',
            r'ì„\s*í†µí•œ\s*[ê°€-í£]+',
            r'ë¥¼\s*í†µí•œ\s*[ê°€-í£]+',
            r'ì„\s*ìœ„í•´\s*[ê°€-í£]+',
            r'ë¥¼\s*ìœ„í•´\s*[ê°€-í£]+',
            r'ê±´ì„¤',
            r'ì¡°ì„±',
            r'êµ¬ì¶•',
            r'ê°œë°œ',
            r'í™•ì¶©',
            r'ì§€ì›',
            r'ê°•í™”',
            r'ê°œì„ '
        ]
        
        for sentence in sentences:
            for pattern in promise_patterns:
                matches = re.findall(pattern, sentence)
                promises.extend(matches)
        
        # ì¤‘ë³µ ì œê±° ë° ê¸¸ì´ ì œí•œ
        unique_promises = list(set(promises))
        filtered_promises = [p for p in unique_promises if 3 <= len(p) <= 50]
        
        return filtered_promises
    
    def _get_region_level(self, region_name: str) -> str:
        """ì§€ì—­ ë ˆë²¨ íŒë‹¨"""
        if region_name in ["ì„œìš¸", "ë¶€ì‚°", "ëŒ€êµ¬", "ì¸ì²œ", "ê´‘ì£¼", "ëŒ€ì „", "ìš¸ì‚°", "ì„¸ì¢…"]:
            return "ê´‘ì—­ì‹œë„"
        elif region_name in ["ê²½ê¸°", "ê°•ì›", "ì¶©ë¶", "ì¶©ë‚¨", "ì „ë¶", "ì „ë‚¨", "ê²½ë¶", "ê²½ë‚¨", "ì œì£¼"]:
            return "ë„"
        else:
            return "ì‹œêµ°êµ¬"
    
    def _generate_interpretation(self, region_name: str, dominant_topics: List[str], topic_scores: Dict[str, int]) -> str:
        """ì§€ì—­ë³„ í•´ì„ ìƒì„±"""
        if not dominant_topics:
            return f"{region_name}ì— ëŒ€í•œ ì •ì±…ì  ë…¼ì˜ê°€ ì§„í–‰ë˜ê³  ìˆìœ¼ë‚˜ íŠ¹ì • ë¶„ì•¼ì— ì§‘ì¤‘ë˜ì§€ëŠ” ì•ŠìŒ"
        
        top_topic = dominant_topics[0]
        top_score = topic_scores.get(top_topic, 0)
        
        if top_score >= 10:
            return f"{region_name}ì€ {top_topic} ë¶„ì•¼ì—ì„œ ê°•í•œ ê´€ì‹¬ì„ ë³´ì´ë©°, ì´ëŠ” í•´ë‹¹ ì§€ì—­ì˜ ì£¼ìš” ì •ì±… ì´ìŠˆì„"
        elif top_score >= 5:
            return f"{region_name}ì€ {top_topic} ë¶„ì•¼ì— ìƒë‹¹í•œ ê´€ì‹¬ì„ ë³´ì´ë©°, ê´€ë ¨ ì •ì±… ë…¼ì˜ê°€ í™œë°œí•¨"
        else:
            return f"{region_name}ì€ {top_topic} ë¶„ì•¼ì— ê´€ì‹¬ì„ ë³´ì´ë©°, ê´€ë ¨ ì •ì±… ê°œë°œì´ í•„ìš”í•¨"
    
    def _calculate_confidence_score(self, topic_scores: Dict[str, int]) -> int:
        """ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚° (0-10)"""
        if not topic_scores:
            return 0
        
        max_score = max(topic_scores.values())
        total_score = sum(topic_scores.values())
        
        if total_score >= 20:
            return min(10, max_score // 2)
        elif total_score >= 10:
            return min(8, max_score // 2)
        elif total_score >= 5:
            return min(6, max_score)
        else:
            return min(4, max_score)
    
    def run_comprehensive_analysis(self) -> Dict[str, Any]:
        """ì¢…í•© ë¶„ì„ ì‹¤í–‰"""
        print("ğŸš€ ì •ì±…ì„ ê±°ë¬¸í™” ë¬¸ì„œ ì¢…í•© ë¶„ì„ ì‹œì‘!")
        print("=" * 60)
        
        # 1. PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
        full_text = self.extract_text_from_pdf()
        
        # 2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        sentences = self.preprocess_text(full_text)
        
        # 3. í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„
        keyword_frequency = self.extract_keywords_with_frequency(sentences)
        
        # 4. ì§€ì—­ë³„ í† í”½ ë¶„ì„
        regional_topics = self.analyze_regional_topics(sentences)
        
        # 5. ì „ì²´ í† í”½ í†µê³„
        overall_topic_stats = self._calculate_overall_topic_stats(regional_topics)
        
        # 6. ê²°ê³¼ í†µí•©
        comprehensive_results = {
            "document_info": self.analysis_results['document_info'],
            "analysis_metadata": {
                "analysis_date": datetime.now().isoformat(),
                "total_sentences": len(sentences),
                "total_keywords": len(keyword_frequency),
                "total_regions": len(regional_topics),
                "total_topics": len(self.comprehensive_topic_categories),
                "analyzer_used": KOREAN_ANALYZER or "none"
            },
            "comprehensive_topic_categories": self.comprehensive_topic_categories,
            "keyword_frequency_analysis": dict(list(keyword_frequency.items())[:100]),  # ìƒìœ„ 100ê°œ
            "regional_topics": regional_topics,
            "overall_topic_statistics": overall_topic_stats,
            "top_promises_by_region": self._extract_top_promises_by_region(regional_topics)
        }
        
        print("=" * 60)
        print("ğŸ‰ ì¢…í•© ë¶„ì„ ì™„ë£Œ!")
        print(f"ğŸ“Š ë¶„ì„ ê²°ê³¼ ìš”ì•½:")
        print(f"  â€¢ ì´ ì§€ì—­: {len(regional_topics)}ê°œ")
        print(f"  â€¢ ì´ í† í”½: {len(self.comprehensive_topic_categories)}ê°œ")
        print(f"  â€¢ ì´ ë¬¸ì¥: {len(sentences):,}ê°œ")
        print(f"  â€¢ ì´ í‚¤ì›Œë“œ: {len(keyword_frequency):,}ê°œ")
        
        return comprehensive_results
    
    def _calculate_overall_topic_stats(self, regional_topics: Dict[str, Any]) -> Dict[str, Any]:
        """ì „ì²´ í† í”½ í†µê³„ ê³„ì‚°"""
        topic_counts = defaultdict(int)
        topic_total_scores = defaultdict(int)
        
        for region_data in regional_topics.values():
            for topic, score in region_data['topic_scores'].items():
                topic_counts[topic] += 1
                topic_total_scores[topic] += score
        
        # ìƒìœ„ í† í”½ ì„ ì •
        sorted_topics = sorted(topic_total_scores.items(), key=lambda x: x[1], reverse=True)
        
        return {
            "most_discussed_topics": [topic for topic, score in sorted_topics[:10]],
            "topic_frequency": dict(topic_counts),
            "topic_total_scores": dict(topic_total_scores),
            "average_topics_per_region": sum(topic_counts.values()) / len(regional_topics) if regional_topics else 0
        }
    
    def _extract_top_promises_by_region(self, regional_topics: Dict[str, Any]) -> Dict[str, List[str]]:
        """ì§€ì—­ë³„ ì£¼ìš” ê³µì•½ ì¶”ì¶œ"""
        top_promises = {}
        
        for region_name, region_data in regional_topics.items():
            promises = region_data.get('promises', [])
            if promises:
                top_promises[region_name] = promises[:5]  # ìƒìœ„ 5ê°œ
        
        return top_promises
    
    def save_results(self, results: Dict[str, Any], output_dir: str = None) -> str:
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        if output_dir is None:
            output_dir = os.path.join(os.path.dirname(__file__), "comprehensive_policy_analysis")
        
        os.makedirs(output_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        json_file = os.path.join(output_dir, f"comprehensive_policy_analysis_{timestamp}.json")
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # í”„ë¡ íŠ¸ì—”ë“œìš© ìš”ì•½ íŒŒì¼ ì €ì¥
        frontend_data = {
            "last_updated": results["analysis_metadata"]["analysis_date"],
            "total_regions": results["analysis_metadata"]["total_regions"],
            "comprehensive_topic_categories": results["comprehensive_topic_categories"],
            "regional_data": results["regional_topics"]
        }
        
        frontend_file = os.path.join(output_dir, f"comprehensive_regional_topics_frontend_{timestamp}.json")
        with open(frontend_file, 'w', encoding='utf-8') as f:
            json.dump(frontend_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
        print(f"  ğŸ“„ ì „ì²´ ê²°ê³¼: {json_file}")
        print(f"  ğŸ–¥ï¸ í”„ë¡ íŠ¸ì—”ë“œìš©: {frontend_file}")
        
        return json_file

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    pdf_file_path = "/Users/hopidaay/Desktop/231215_ì •ì±…ì„ ê±°ë¬¸í™”_í™•ì‚°ì„_ìœ„í•œ_ì–¸ë¡ ê¸°ì‚¬_ë¹…ë°ì´í„°_ë¶„ì„.pdf"
    
    if not os.path.exists(pdf_file_path):
        print(f"âŒ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_file_path}")
        return
    
    analyzer = ComprehensivePolicyTopicAnalyzer(pdf_file_path)
    results = analyzer.run_comprehensive_analysis()
    analyzer.save_results(results)
    
    print("\nğŸŠ ì •ì±…ì„ ê±°ë¬¸í™” ë¬¸ì„œ ì¢…í•© ë¶„ì„ ì™„ë£Œ!")
    print("ğŸ“ˆ ë” ë§ì€ í† í”½ê³¼ ì„¸ë°€í•œ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main()
