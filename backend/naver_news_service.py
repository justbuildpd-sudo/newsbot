#!/usr/bin/env python3
"""
ë„¤ì´ë²„ ë‰´ìŠ¤ API ì„œë¹„ìŠ¤
ì •ì¹˜ì¸ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ë¶„ì„
"""

import requests
import json
import time
from datetime import datetime, timedelta
from urllib.parse import quote
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class NaverNewsService:
    def __init__(self):
        # ë„¤ì´ë²„ ê°œë°œì API ì •ë³´
        self.client_id = "kXwlSsFmb055ku9rWyx1"
        self.client_secret = "JZqw_LTiq_"
        self.base_url = "https://openapi.naver.com/v1/search/news.json"
        
        # ìš”ì²­ í—¤ë”
        self.headers = {
            "X-Naver-Client-Id": self.client_id,
            "X-Naver-Client-Secret": self.client_secret,
            "User-Agent": "NewsBot/1.0"
        }
        
        # API í˜¸ì¶œ ì œí•œ (í•˜ë£¨ 25,000íšŒ)
        self.request_delay = 0.1  # 100ms ê°„ê²©
        self.daily_limit = 25000
        self.request_count = 0
        
        # ì •ì¹˜ì¸ ëª©ë¡ ë¡œë“œ
        self.load_politicians()
        
    def load_politicians(self):
        """22ëŒ€ êµ­íšŒì˜ì› ëª©ë¡ ë¡œë“œ"""
        try:
            with open('22nd_assembly_members_300.json', 'r', encoding='utf-8') as f:
                members = json.load(f)
            self.politicians = [member.get('naas_nm', '') for member in members if member.get('naas_nm')]
            logger.info(f"ì •ì¹˜ì¸ ëª©ë¡ ë¡œë“œ ì™„ë£Œ: {len(self.politicians)}ëª…")
        except FileNotFoundError:
            try:
                with open('../22nd_assembly_members_300.json', 'r', encoding='utf-8') as f:
                    members = json.load(f)
                self.politicians = [member.get('naas_nm', '') for member in members if member.get('naas_nm')]
                logger.info(f"ì •ì¹˜ì¸ ëª©ë¡ ë¡œë“œ ì™„ë£Œ: {len(self.politicians)}ëª…")
            except FileNotFoundError:
                self.politicians = ["ì´ì¬ëª…", "í•œë™í›ˆ", "ì¡°êµ­", "ì •ì²­ë˜", "ê¹€ê¸°í˜„"]
                logger.warning("ì •ì¹˜ì¸ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ ëª©ë¡ ì‚¬ìš©")
    
    def search_news(self, query, display=10, start=1, sort="sim"):
        """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰"""
        if self.request_count >= self.daily_limit:
            logger.warning("ì¼ì¼ API í˜¸ì¶œ í•œë„ ì´ˆê³¼")
            return None
            
        try:
            # URL ì¸ì½”ë”©
            encoded_query = quote(query)
            
            params = {
                "query": encoded_query,
                "display": display,  # ìµœëŒ€ 100
                "start": start,      # ì‹œì‘ ìœ„ì¹˜
                "sort": sort         # sim(ì •í™•ë„), date(ë‚ ì§œ)
            }
            
            response = requests.get(
                self.base_url, 
                headers=self.headers, 
                params=params,
                timeout=10
            )
            
            self.request_count += 1
            
            if response.status_code == 200:
                data = response.json()
                logger.info(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì„±ê³µ: '{query}' - {len(data.get('items', []))}ê±´")
                return data
            else:
                logger.error(f"API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code} - {response.text}")
                return None
                
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return None
        
        finally:
            time.sleep(self.request_delay)
    
    def search_politician_news(self, politician_name, days=30, max_results=20):
        """íŠ¹ì • ì •ì¹˜ì¸ ê´€ë ¨ ë‰´ìŠ¤ ê²€ìƒ‰"""
        try:
            # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± (ì •ì¹˜ ë‰´ìŠ¤ì— íŠ¹í™”)
            queries = [
                f'{politician_name} ì˜ì›',
                f'{politician_name} êµ­íšŒì˜ì›',
                f'{politician_name} êµ­íšŒ',
                f'{politician_name} ì •ë‹¹',
                f'{politician_name} ë²•ì•ˆ'
            ]
            
            all_news = []
            seen_titles = set()
            
            for query in queries:
                if len(all_news) >= max_results:
                    break
                    
                news_data = self.search_news(query, display=min(20, max_results), sort="sim")
                if not news_data:
                    continue
                
                for item in news_data.get('items', []):
                    # ì¤‘ë³µ ì œê±°
                    title = self.clean_html(item.get('title', ''))
                    if title in seen_titles:
                        logger.debug(f"ì¤‘ë³µ ì œëª© ê±´ë„ˆë›°ê¸°: {title}")
                        continue
                    
                    # ì •ì¹˜ ê´€ë ¨ ë‰´ìŠ¤ í•„í„°ë§
                    description = item.get('description', '')
                    if not self.is_political_news(title, description):
                        logger.debug(f"ì •ì¹˜ ê´€ë ¨ ì—†ìŒ: {title}")
                        continue
                    
                    seen_titles.add(title)
                    
                    # ë‚ ì§œ í•„í„°ë§ (ìµœê·¼ Nì¼) - ë” ê´€ëŒ€í•˜ê²Œ
                    pub_date = self.parse_date(item.get('pubDate', ''))
                    if pub_date:
                        # timezone-aware datetimeìœ¼ë¡œ ë³€í™˜
                        now = datetime.now(pub_date.tzinfo) if pub_date.tzinfo else datetime.now()
                        days_diff = (now - pub_date).days
                        logger.debug(f"ë‚ ì§œ ì²´í¬: {title} - {days_diff}ì¼ ì „")
                        # 30ì¼ ì´ë‚´ ë‰´ìŠ¤ë§Œ ìˆ˜ì§‘ (ê¸°ë³¸ê°’ë³´ë‹¤ ê´€ëŒ€)
                        if days_diff > days:
                            logger.debug(f"ë‚ ì§œ ì´ˆê³¼ë¡œ ì œì™¸: {days_diff}ì¼ > {days}ì¼")
                            continue
                    else:
                        logger.debug(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {item.get('pubDate', '')}")
                        # ë‚ ì§œë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ì–´ë„ ìˆ˜ì§‘ (ìµœì‹  ë‰´ìŠ¤ì¼ ê°€ëŠ¥ì„±)
                    
                    # ë‰´ìŠ¤ ë°ì´í„° ì •ë¦¬
                    news_item = {
                        'title': title,
                        'description': self.clean_html(description),
                        'link': item.get('link', ''),
                        'pub_date': pub_date.isoformat() if pub_date else item.get('pubDate', ''),
                        'politician': politician_name,
                        'sentiment': self.analyze_sentiment(title + ' ' + description),
                        'collected_at': datetime.now().isoformat()
                    }
                    
                    all_news.append(news_item)
                    logger.info(f"ë‰´ìŠ¤ ìˆ˜ì§‘: {title}")
                    
                    if len(all_news) >= max_results:
                        break
            
            logger.info(f"{politician_name} ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘: {len(all_news)}ê±´")
            return all_news
            
        except Exception as e:
            logger.error(f"{politician_name} ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def collect_all_politicians_news(self, days=7, max_per_politician=10):
        """ëª¨ë“  ì •ì¹˜ì¸ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        all_news = {}
        total_collected = 0
        
        logger.info(f"ì „ì²´ ì •ì¹˜ì¸ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘: {len(self.politicians)}ëª…")
        
        for i, politician in enumerate(self.politicians):
            if self.request_count >= self.daily_limit * 0.9:  # 90% ì‚¬ìš© ì‹œ ì¤‘ë‹¨
                logger.warning("API í˜¸ì¶œ í•œë„ ê·¼ì ‘ìœ¼ë¡œ ìˆ˜ì§‘ ì¤‘ë‹¨")
                break
            
            logger.info(f"ì§„í–‰ë¥ : {i+1}/{len(self.politicians)} - {politician}")
            
            news = self.search_politician_news(politician, days, max_per_politician)
            if news:
                all_news[politician] = news
                total_collected += len(news)
            
            # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
            time.sleep(0.2)
        
        logger.info(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: ì´ {total_collected}ê±´")
        return all_news
    
    def is_political_news(self, title, description):
        """ì •ì¹˜ ê´€ë ¨ ë‰´ìŠ¤ì¸ì§€ íŒë‹¨ (ê°œì„ ëœ ë²„ì „)"""
        political_keywords = [
            'ì˜ì›', 'êµ­íšŒ', 'ì •ì¹˜', 'ì •ë¶€', 'êµ­ì •', 'ë²•ì•ˆ', 'ë°œì˜', 'ìœ„ì›íšŒ',
            'ì •ë‹¹', 'ì„ ê±°', 'ê³µì•½', 'ì •ì±…', 'êµ­ì •ê°ì‚¬', 'êµ­ì •ì¡°ì‚¬', 'ë³¸íšŒì˜',
            'ë¯¼ì£¼ë‹¹', 'êµ­ë¯¼ì˜í˜', 'ì¡°êµ­í˜ì‹ ë‹¹', 'ê°œí˜ì‹ ë‹¹', 'ì§„ë³´ë‹¹', 'ê¸°ë³¸ì†Œë“ë‹¹',
            'ë°œì–¸', 'ë¹„íŒ', 'ì§€ì§€', 'ë°˜ëŒ€', 'ì°¬ì„±', 'ì˜ì •', 'êµ­íšŒì˜ì›', 'ëŒ€í†µë ¹',
            'ì´ë¦¬', 'ì¥ê´€', 'ì²­ì™€ëŒ€', 'êµ­ë¬´íšŒì˜', 'ë‚´ê°', 'ì—¬ë‹¹', 'ì•¼ë‹¹', 'ì›ë‚´ëŒ€í‘œ'
        ]
        
        # ì œì™¸í•  í‚¤ì›Œë“œ (ì˜í™”, ë“œë¼ë§ˆ, ì—°ì˜ˆ ë“±) - ë” ì—„ê²©í•˜ê²Œ
        exclude_keywords = [
            'ì˜í™”', 'ë“œë¼ë§ˆ', 'ì˜ˆëŠ¥', 'ì—°ì˜ˆ', 'ë°°ìš°', 'ê°€ìˆ˜', 'ì•„ì´ëŒ',
            'ë°©ì†¡', 'TV', 'ì‹œì²­ë¥ ', 'ì¶œì—°', 'ìºìŠ¤íŒ…', 'ì´¬ì˜', 'ê³µì—°', 'ì½˜ì„œíŠ¸'
        ]
        
        text = title + ' ' + description
        
        # ê°•í•œ ì œì™¸ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì •ì¹˜ ë‰´ìŠ¤ê°€ ì•„ë‹˜
        strong_exclude = ['ì˜í™”', 'ë“œë¼ë§ˆ', 'ì˜ˆëŠ¥', 'ë°°ìš°', 'ê°€ìˆ˜']
        if any(keyword in text for keyword in strong_exclude):
            return False
        
        # ì •ì¹˜ í‚¤ì›Œë“œê°€ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ì •ì¹˜ ë‰´ìŠ¤ë¡œ íŒë‹¨
        return any(keyword in text for keyword in political_keywords)
    
    def analyze_sentiment(self, text):
        """ê°„ë‹¨í•œ ê°ì • ë¶„ì„ (í‚¤ì›Œë“œ ê¸°ë°˜)"""
        positive_words = [
            'ì„±ê³µ', 'ë°œì „', 'ê°œì„ ', 'ê¸ì •', 'ì¢‹ì€', 'í›Œë¥­', 'ì„±ê³¼', 'ë°œí‘œ', 
            'ì§€ì§€', 'ì°¬ì„±', 'í˜‘ë ¥', 'í•©ì˜', 'í†µê³¼', 'ìŠ¹ì¸', 'ì¶”ì§„'
        ]
        
        negative_words = [
            'ì‹¤íŒ¨', 'ë¬¸ì œ', 'ë¹„íŒ', 'ë°˜ëŒ€', 'ê°ˆë“±', 'ë…¼ë€', 'ìš°ë ¤', 'ìœ„ê¸°',
            'ë¶€ì •', 'ê±°ë¶€', 'ë°˜ë°œ', 'ì¶©ëŒ', 'íê¸°', 'ì¤‘ë‹¨', 'ì·¨ì†Œ'
        ]
        
        positive_count = sum(1 for word in positive_words if word in text)
        negative_count = sum(1 for word in negative_words if word in text)
        
        if positive_count > negative_count:
            return 'positive'
        elif negative_count > positive_count:
            return 'negative'
        else:
            return 'neutral'
    
    def clean_html(self, text):
        """HTML íƒœê·¸ ì œê±°"""
        import re
        # HTML íƒœê·¸ ì œê±°
        clean = re.sub('<.*?>', '', text)
        # HTML ì—”í‹°í‹° ë³€í™˜
        clean = clean.replace('&lt;', '<').replace('&gt;', '>')
        clean = clean.replace('&amp;', '&').replace('&quot;', '"')
        clean = clean.replace('&#39;', "'").replace('&nbsp;', ' ')
        return clean.strip()
    
    def parse_date(self, date_string):
        """ë‚ ì§œ ë¬¸ìì—´ íŒŒì‹±"""
        try:
            # RFC 2822 í˜•ì‹: "Mon, 16 Sep 2024 10:30:00 +0900"
            from email.utils import parsedate_to_datetime
            return parsedate_to_datetime(date_string)
        except:
            try:
                # ISO í˜•ì‹ ì‹œë„
                return datetime.fromisoformat(date_string.replace('Z', '+00:00'))
            except:
                return None
    
    def save_news_data(self, news_data, filename=None):
        """ë‰´ìŠ¤ ë°ì´í„° ì €ì¥"""
        if filename is None:
            filename = f"news_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(news_data, f, ensure_ascii=False, indent=2)
            
            total_news = sum(len(news) for news in news_data.values())
            logger.info(f"ë‰´ìŠ¤ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filename} ({total_news}ê±´)")
            return filename
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ë°ì´í„° ì €ì¥ ì˜¤ë¥˜: {e}")
            return None
    
    def get_trending_politicians(self, news_data, limit=10):
        """ë‰´ìŠ¤ ì–¸ê¸‰ëŸ‰ ê¸°ì¤€ íŠ¸ë Œë”© ì •ì¹˜ì¸"""
        trending = []
        
        for politician, news_list in news_data.items():
            if news_list:
                sentiment_counts = {'positive': 0, 'negative': 0, 'neutral': 0}
                for news in news_list:
                    sentiment_counts[news.get('sentiment', 'neutral')] += 1
                
                trending.append({
                    'politician': politician,
                    'news_count': len(news_list),
                    'positive': sentiment_counts['positive'],
                    'negative': sentiment_counts['negative'],
                    'neutral': sentiment_counts['neutral'],
                    'sentiment_ratio': sentiment_counts['positive'] / len(news_list) if news_list else 0
                })
        
        # ë‰´ìŠ¤ ìˆ˜ ê¸°ì¤€ ì •ë ¬
        trending.sort(key=lambda x: x['news_count'], reverse=True)
        return trending[:limit]
    
    def get_api_status(self):
        """API ì‚¬ìš© í˜„í™©"""
        return {
            'request_count': self.request_count,
            'daily_limit': self.daily_limit,
            'remaining': self.daily_limit - self.request_count,
            'usage_percentage': (self.request_count / self.daily_limit) * 100
        }

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    service = NaverNewsService()
    
    print("ğŸŒ ë„¤ì´ë²„ ë‰´ìŠ¤ API ì„œë¹„ìŠ¤ ì‹œì‘")
    print(f"ğŸ“Š ì •ì¹˜ì¸ ìˆ˜: {len(service.politicians)}ëª…")
    
    # ì£¼ìš” ì •ì¹˜ì¸ ë‰´ìŠ¤ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸
    test_politicians = ["ì´ì¬ëª…", "í•œë™í›ˆ", "ì¡°êµ­"]
    
    for politician in test_politicians:
        print(f"\nğŸ” {politician} ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
        news = service.search_politician_news(politician, days=3, max_results=5)
        
        if news:
            print(f"âœ… {len(news)}ê±´ ìˆ˜ì§‘")
            for i, item in enumerate(news[:3]):
                print(f"  {i+1}. {item['title']} ({item['sentiment']})")
        else:
            print("âŒ ë‰´ìŠ¤ ì—†ìŒ")
    
    print(f"\nğŸ“Š API ì‚¬ìš©ëŸ‰: {service.get_api_status()}")

if __name__ == "__main__":
    main()
