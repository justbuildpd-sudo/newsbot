#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í™•ì¥ ê°€ëŠ¥í•œ ëŒ€ê·œëª¨ ì‹œìŠ¤í…œ
25,000ëª… ê·œëª¨ê¹Œì§€ í™•ì¥ ê°€ëŠ¥í•œ ì •ì¹˜ì¸ ë°ì´í„°ë² ì´ìŠ¤ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤.
"""

import json
import logging
from typing import Dict, List, Optional
from datetime import datetime
import os
import gc
import hashlib
from collections import defaultdict

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ScalableMassiveSystem:
    """í™•ì¥ ê°€ëŠ¥í•œ ëŒ€ê·œëª¨ ì‹œìŠ¤í…œ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.data_sources = {}
        self.integrated_politicians = []
        self.partitioned_data = {}  # íŒŒí‹°ì…˜ë³„ ë°ì´í„°
        self.search_index = {}
        self.system_config = {
            'max_batch_size': 2000,
            'partition_size': 5000,
            'memory_cleanup_interval': 1000,
            'max_search_results': 100
        }
    
    def register_data_source(self, source_name: str, file_path: str, category: str, priority: int):
        """ë°ì´í„° ì†ŒìŠ¤ë¥¼ ë“±ë¡í•©ë‹ˆë‹¤."""
        self.data_sources[source_name] = {
            'file_path': file_path,
            'category': category,
            'priority': priority,
            'loaded': False,
            'count': 0
        }
        logger.info(f"ğŸ“‹ ë°ì´í„° ì†ŒìŠ¤ ë“±ë¡: {source_name} ({category}, ìš°ì„ ìˆœìœ„: {priority})")
    
    def load_data_source(self, source_name: str) -> List[Dict]:
        """ê°œë³„ ë°ì´í„° ì†ŒìŠ¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        source_info = self.data_sources.get(source_name)
        if not source_info:
            logger.error(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ë°ì´í„° ì†ŒìŠ¤: {source_name}")
            return []
        
        try:
            file_path = source_info['file_path']
            
            if not os.path.exists(file_path):
                logger.warning(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {file_path}")
                return []
            
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ë‹¤ì–‘í•œ ë°ì´í„° í˜•ì‹ ì§€ì›
            candidates = []
            if isinstance(data, list):
                candidates = data
            elif isinstance(data, dict):
                if 'candidates' in data:
                    candidates = data['candidates']
                elif 'members' in data:
                    candidates = data['members']
                else:
                    # ì²« ë²ˆì§¸ ë¦¬ìŠ¤íŠ¸ íƒ€ì… ê°’ ì°¾ê¸°
                    for value in data.values():
                        if isinstance(value, list):
                            candidates = value
                            break
            
            source_info['loaded'] = True
            source_info['count'] = len(candidates)
            
            logger.info(f"âœ… {source_name} ë¡œë“œ ì™„ë£Œ: {len(candidates):,}ëª…")
            return candidates
            
        except Exception as e:
            logger.error(f"âŒ {source_name} ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def normalize_politician_minimal(self, politician: Dict, category: str, priority: int) -> Dict:
        """ì •ì¹˜ì¸ ë°ì´í„°ë¥¼ ìµœì†Œ í˜•íƒœë¡œ í‘œì¤€í™”í•©ë‹ˆë‹¤."""
        name = politician.get('name', 'unknown').strip()
        district = (politician.get('district') or 'unknown').strip()
        
        # í•´ì‹œ ê¸°ë°˜ ê³ ìœ  ID (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
        unique_string = f"{category}_{name}_{district}"
        politician_id = hashlib.md5(unique_string.encode()).hexdigest()[:12]
        
        # ìµœì†Œ í•„ìˆ˜ ì •ë³´ë§Œ í¬í•¨
        normalized = {
            'id': politician_id,
            'name': name,
            'category': category,
            'party': (politician.get('party') or '').strip() or None,
            'district': district,
            'is_elected': politician.get('is_elected', False),
            'search_priority': priority,
            'political_level': self._get_political_level(category, politician),
            'vote_count': politician.get('vote_count'),
            'age': politician.get('age'),
            'gender': politician.get('sexDistinction') or politician.get('gender')
        }
        
        return normalized
    
    def _get_political_level(self, category: str, politician: Dict) -> str:
        """ì •ì¹˜ ìˆ˜ì¤€ì„ ê²°ì •í•©ë‹ˆë‹¤."""
        levels = {
            'current_member': 'êµ­íšŒì˜ì›',
            'election_candidate': 'êµ­íšŒì˜ì› ì¶œë§ˆì',
            'education_candidate': 'êµìœ¡ê°',
            'metro_council_candidate': 'ì‹œÂ·ë„ì˜ì›',
            'local_politician': 'ì§€ë°©ì˜ì›',
            'basic_council_candidate': 'ê¸°ì´ˆì˜ì›'
        }
        return levels.get(category, 'ì •ì¹˜ì¸')
    
    def partition_data(self) -> Dict:
        """ë°ì´í„°ë¥¼ íŒŒí‹°ì…˜ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
        logger.info("ğŸ”„ ë°ì´í„° íŒŒí‹°ì…”ë‹ ì‹œì‘...")
        
        partitions = defaultdict(list)
        partition_size = self.system_config['partition_size']
        
        # ì¹´í…Œê³ ë¦¬ë³„ íŒŒí‹°ì…”ë‹
        for politician in self.integrated_politicians:
            category = politician['category']
            partition_key = f"{category}_{len(partitions[category]) // partition_size}"
            partitions[partition_key].append(politician)
        
        self.partitioned_data = dict(partitions)
        
        logger.info(f"âœ… ë°ì´í„° íŒŒí‹°ì…”ë‹ ì™„ë£Œ: {len(self.partitioned_data)}ê°œ íŒŒí‹°ì…˜")
        return self.partitioned_data
    
    def build_scalable_search_index(self) -> Dict:
        """í™•ì¥ ê°€ëŠ¥í•œ ê²€ìƒ‰ ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤."""
        logger.info("ğŸ”„ í™•ì¥ ê°€ëŠ¥í•œ ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶•...")
        
        search_index = {
            'by_name': defaultdict(list),
            'by_party': defaultdict(list),
            'by_category': defaultdict(list),
            'by_political_level': defaultdict(list),
            'priority_index': defaultdict(list)  # ìš°ì„ ìˆœìœ„ë³„ ì¸ë±ìŠ¤
        }
        
        processed_count = 0
        batch_size = self.system_config['memory_cleanup_interval']
        
        for politician in self.integrated_politicians:
            name = politician['name']
            party = politician['party']
            category = politician['category']
            political_level = politician['political_level']
            priority = politician['search_priority']
            
            # ì´ë¦„ë³„ ì¸ë±ìŠ¤ (ìš°ì„ ìˆœìœ„ ìˆœ ì •ë ¬)
            if name:
                search_index['by_name'][name.lower()].append(politician)
                # ìƒìœ„ 5ê°œë§Œ ìœ ì§€ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
                search_index['by_name'][name.lower()].sort(
                    key=lambda x: x['search_priority'], reverse=True
                )
                search_index['by_name'][name.lower()] = search_index['by_name'][name.lower()][:5]
            
            # ì •ë‹¹ë³„ ì¸ë±ìŠ¤
            if party:
                search_index['by_party'][party].append(politician)
            
            # ì¹´í…Œê³ ë¦¬ë³„ ì¸ë±ìŠ¤
            search_index['by_category'][category].append(politician)
            
            # ì •ì¹˜ ìˆ˜ì¤€ë³„ ì¸ë±ìŠ¤
            search_index['by_political_level'][political_level].append(politician)
            
            # ìš°ì„ ìˆœìœ„ë³„ ì¸ë±ìŠ¤
            priority_group = priority // 10 * 10  # 10ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”
            search_index['priority_index'][priority_group].append(politician)
            
            processed_count += 1
            
            # ë©”ëª¨ë¦¬ ê´€ë¦¬
            if processed_count % batch_size == 0:
                gc.collect()
                logger.info(f"   ê²€ìƒ‰ ì¸ë±ìŠ¤ ì²˜ë¦¬: {processed_count:,}ëª… ì™„ë£Œ")
        
        # defaultdictë¥¼ ì¼ë°˜ dictë¡œ ë³€í™˜
        final_index = {
            'by_name': dict(search_index['by_name']),
            'by_party': dict(search_index['by_party']),
            'by_category': dict(search_index['by_category']),
            'by_political_level': dict(search_index['by_political_level']),
            'priority_index': dict(search_index['priority_index'])
        }
        
        logger.info("âœ… í™•ì¥ ê°€ëŠ¥í•œ ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ")
        return final_index
    
    def run_scalable_integration(self) -> Dict:
        """í™•ì¥ ê°€ëŠ¥í•œ í†µí•© ì‘ì—…ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        logger.info("ğŸš€ í™•ì¥ ê°€ëŠ¥í•œ ëŒ€ê·œëª¨ í†µí•© ì‹œìŠ¤í…œ ì‹œì‘")
        
        # 1. ë°ì´í„° ì†ŒìŠ¤ ë“±ë¡
        self.register_data_source("í˜„ì§_êµ­íšŒì˜ì›", "enhanced_298_members_with_contact.json", "current_member", 100)
        self.register_data_source("22ëŒ€_ì¶œë§ˆì", "politician_classification_results.json", "election_candidate", 80)
        self.register_data_source("êµìœ¡ê°", "education_superintendent_election.json", "education_candidate", 75)
        self.register_data_source("ì‹œë„ì˜ì›", "metro_council_election_full.json", "metro_council_candidate", 65)
        self.register_data_source("ì§€ë°©ì˜ì›", "20th_local_election_full.json", "local_politician", 60)
        self.register_data_source("ê¸°ì´ˆì˜ì›", "basic_council_election_full.json", "basic_council_candidate", 50)
        
        # 2. ëª¨ë“  ë°ì´í„° ì†ŒìŠ¤ ë¡œë“œ ë° í†µí•©
        total_processed = 0
        
        for source_name, source_info in self.data_sources.items():
            logger.info(f"ğŸ”„ {source_name} ì²˜ë¦¬ ì¤‘...")
            
            candidates = self.load_data_source(source_name)
            
            if source_name == "22ëŒ€_ì¶œë§ˆì":
                # íŠ¹ë³„ ì²˜ë¦¬: classification_resultsì—ì„œ candidates ì¶”ì¶œ
                winners = candidates.get('current_assembly_members', []) if isinstance(candidates, dict) else []
                losers = candidates.get('former_politicians', []) if isinstance(candidates, dict) else []
                candidates = winners + losers
            
            # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í™•ë³´
            if len(candidates) == 0:
                continue
                
            batch_size = min(self.system_config['max_batch_size'], len(candidates))
            if batch_size == 0:
                batch_size = 1
            
            for i in range(0, len(candidates), batch_size):
                batch = candidates[i:i + batch_size]
                
                for politician in batch:
                    normalized = self.normalize_politician_minimal(
                        politician, 
                        source_info['category'], 
                        source_info['priority']
                    )
                    self.integrated_politicians.append(normalized)
                
                total_processed += len(batch)
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                if total_processed % self.system_config['memory_cleanup_interval'] == 0:
                    gc.collect()
                    logger.info(f"   ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ: {total_processed:,}ëª… ì²˜ë¦¬")
            
            logger.info(f"âœ… {source_name} ì²˜ë¦¬ ì™„ë£Œ: {len(candidates):,}ëª…")
        
        # 3. ì¤‘ë³µ ì œê±° (íš¨ìœ¨ì  ì•Œê³ ë¦¬ì¦˜)
        logger.info("ğŸ”„ ëŒ€ê·œëª¨ ì¤‘ë³µ ì œê±°...")
        unique_dict = {}
        duplicate_count = 0
        
        for politician in self.integrated_politicians:
            name = politician['name']
            district = politician['district']
            key = f"{name}_{district}"
            
            if key not in unique_dict:
                unique_dict[key] = politician
            else:
                # ìš°ì„ ìˆœìœ„ ë¹„êµ
                if politician['search_priority'] > unique_dict[key]['search_priority']:
                    unique_dict[key] = politician
                duplicate_count += 1
        
        self.integrated_politicians = list(unique_dict.values())
        logger.info(f"âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: {duplicate_count:,}ëª… ì œê±°, {len(self.integrated_politicians):,}ëª… ìµœì¢…")
        
        # 4. ë°ì´í„° íŒŒí‹°ì…”ë‹
        self.partition_data()
        
        # 5. í™•ì¥ ê°€ëŠ¥í•œ ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶•
        self.search_index = self.build_scalable_search_index()
        
        # 6. í†µê³„ ìƒì„±
        statistics = {
            'total_politicians': len(self.integrated_politicians),
            'duplicates_removed': duplicate_count,
            'partitions_count': len(self.partitioned_data),
            'categories': {},
            'parties': len(self.search_index['by_party']),
            'political_levels': len(self.search_index['by_political_level']),
            'integration_timestamp': datetime.now().isoformat()
        }
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        for politician in self.integrated_politicians:
            category = politician['category']
            if category not in statistics['categories']:
                statistics['categories'][category] = 0
            statistics['categories'][category] += 1
        
        results = {
            'statistics': statistics,
            'partitioned_data': self.partitioned_data,
            'search_index': self.search_index,
            'system_config': self.system_config
        }
        
        logger.info("âœ… í™•ì¥ ê°€ëŠ¥í•œ ëŒ€ê·œëª¨ í†µí•© ì™„ë£Œ")
        return results
    
    def generate_scalability_report(self, results: Dict) -> str:
        """í™•ì¥ì„± ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        stats = results['statistics']
        
        report = f"""
ğŸš€ NewsBot.kr í™•ì¥ ê°€ëŠ¥í•œ ëŒ€ê·œëª¨ ì‹œìŠ¤í…œ ê²°ê³¼
{'='*120}

ğŸ“Š í˜„ì¬ í†µí•© í†µê³„:
- ğŸ¯ ì „ì²´ ì •ì¹˜ì¸: {stats['total_politicians']:,}ëª…
- ğŸ”„ ì¤‘ë³µ ì œê±°: {stats['duplicates_removed']:,}ëª…
- ğŸ“¦ íŒŒí‹°ì…˜ ìˆ˜: {stats['partitions_count']:,}ê°œ
- ğŸ­ ì •ë‹¹ ìˆ˜: {stats['parties']:,}ê°œ
- ğŸ“Š ì •ì¹˜ ìˆ˜ì¤€: {stats['political_levels']:,}ê°œ

ğŸ“‹ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:
"""
        
        for category, count in stats['categories'].items():
            percentage = (count / stats['total_politicians']) * 100
            report += f"   - {category}: {count:,}ëª… ({percentage:.1f}%)\n"
        
        report += f"""
ğŸ”§ ì‹œìŠ¤í…œ í™•ì¥ì„± ë¶„ì„:
- í˜„ì¬ ì²˜ë¦¬ ëŠ¥ë ¥: {stats['total_politicians']:,}ëª… âœ…
- ëª©í‘œ í™•ì¥ ê·œëª¨: 25,000ëª…
- í™•ì¥ ì—¬ìœ ë„: {25000 - stats['total_politicians']:,}ëª… ({((25000 - stats['total_politicians']) / stats['total_politicians'] * 100):.1f}% ì¶”ê°€ ê°€ëŠ¥)

ğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™”:
- íŒŒí‹°ì…˜ í¬ê¸°: {self.system_config['partition_size']:,}ëª…/íŒŒí‹°ì…˜
- ë°°ì¹˜ ì²˜ë¦¬: {self.system_config['max_batch_size']:,}ëª…/ë°°ì¹˜
- ë©”ëª¨ë¦¬ ì •ë¦¬: {self.system_config['memory_cleanup_interval']:,}ëª…ë§ˆë‹¤

ğŸ” ê²€ìƒ‰ ì‹œìŠ¤í…œ ìµœì í™”:
- ì´ë¦„ë³„ ê²€ìƒ‰: ìƒìœ„ 5ê°œ ê²°ê³¼ë§Œ ìœ ì§€
- ìµœëŒ€ ê²€ìƒ‰ ê²°ê³¼: {self.system_config['max_search_results']:,}ê°œ
- ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì •ë ¬

ğŸš€ ì¶”ê°€ í™•ì¥ ì¤€ë¹„:
- âœ… 25,000ëª… ê·œëª¨ê¹Œì§€ í™•ì¥ ê°€ëŠ¥
- âœ… íŒŒí‹°ì…˜ ê¸°ë°˜ ë©”ëª¨ë¦¬ ê´€ë¦¬
- âœ… ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ ìµœì í™”
- âœ… ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì¤‘ë³µ ì œê±°

ğŸ’¡ ë‹¤ìŒ ì„ ê±° DB í†µí•© ì‹œ:
1. register_data_source()ë¡œ ìƒˆ ë°ì´í„° ë“±ë¡
2. ìë™ ë°°ì¹˜ ì²˜ë¦¬ ë° ë©”ëª¨ë¦¬ ìµœì í™”
3. ê¸°ì¡´ ë°ì´í„°ì™€ ìë™ í†µí•©
4. ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì¤‘ë³µ ì œê±°

"""
        
        return report
    
    def save_scalable_results(self, results: Dict):
        """í™•ì¥ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            # ë©”íƒ€ë°ì´í„°ë§Œ ì €ì¥ (í†µê³„, ì„¤ì •)
            metadata = {
                'statistics': results['statistics'],
                'system_config': results['system_config'],
                'timestamp': datetime.now().isoformat()
            }
            
            with open("newsbot_scalable_metadata.json", 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            # íŒŒí‹°ì…˜ë³„ ì €ì¥ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
            for partition_name, partition_data in results['partitioned_data'].items():
                filename = f"newsbot_partition_{partition_name}.json"
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(partition_data, f, ensure_ascii=False, indent=1)
                
                file_size = os.path.getsize(filename) / (1024 * 1024)
                logger.info(f"âœ… íŒŒí‹°ì…˜ ì €ì¥: {partition_name} - {len(partition_data):,}ëª… ({file_size:.1f}MB)")
            
            # ê²€ìƒ‰ ì¸ë±ìŠ¤ ì €ì¥ (ì••ì¶•)
            with open("newsbot_scalable_search_index.json", 'w', encoding='utf-8') as f:
                json.dump(results['search_index'], f, ensure_ascii=False, indent=1)
            
            logger.info("âœ… í™•ì¥ ê°€ëŠ¥í•œ ê²°ê³¼ íŒŒì¼ ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("=== NewsBot.kr í™•ì¥ ê°€ëŠ¥í•œ ëŒ€ê·œëª¨ ì‹œìŠ¤í…œ ì‹œì‘ ===")
    
    system = ScalableMassiveSystem()
    
    # í™•ì¥ ê°€ëŠ¥í•œ í†µí•© ì‹¤í–‰
    results = system.run_scalable_integration()
    
    if not results:
        logger.error("âŒ í™•ì¥ ê°€ëŠ¥í•œ ì‹œìŠ¤í…œ ì‹¤í–‰ ì‹¤íŒ¨")
        return False
    
    # ê²°ê³¼ ì €ì¥
    system.save_scalable_results(results)
    
    # ë¦¬í¬íŠ¸ ìƒì„± ë° ì¶œë ¥
    report = system.generate_scalability_report(results)
    print(report)
    
    # ë¦¬í¬íŠ¸ íŒŒì¼ ì €ì¥
    with open("newsbot_scalability_report.txt", "w", encoding="utf-8") as f:
        f.write(report)
    
    logger.info("âœ… í™•ì¥ ê°€ëŠ¥í•œ ëŒ€ê·œëª¨ ì‹œìŠ¤í…œ ì™„ë£Œ")
    return True

if __name__ == "__main__":
    success = main()
    if success:
        logger.info("âœ… í™•ì¥ ê°€ëŠ¥í•œ ì‹œìŠ¤í…œ ì‹¤í–‰ ì™„ë£Œ")
    else:
        logger.error("âŒ í™•ì¥ ê°€ëŠ¥í•œ ì‹œìŠ¤í…œ ì‹¤í–‰ ì‹¤íŒ¨")
